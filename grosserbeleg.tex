% !TeX spellcheck = de_DE
\documentclass[a4paper,hyperref,german,beleg,final,twoside]{cgvpub}

\usepackage{listings}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[chapter]{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\newenvironment{abstractpage}
{\cleardoublepage\vspace*{\fill}\thispagestyle{empty}}
{\vfill\cleardoublepage}

\renewenvironment{abstract}{
\vspace*{\fill}
\begin{center}%
    \bfseries\abstractname
\end{center}}%
{\vfill}

%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% female	weibliche Titelbezeichnung bei Diplom
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig
\author{Tobias Hänel}
\title{Multimodale Registrierung sowie Fusion von präoperativen MRT-Aufnahmen mit intraoperativer 2D-Bildgebung}
\matno{3951236}
\betreuer{Dr. rer. nat. Nico Hoffmann}
\date{22. Februar 2018}
\bibfiles{literatur}

\problem{\textbf{Motivation}\\
Bei der Resektion von Hirntumorgewebe ist für den Chirurgen die Abgrenzung von Tumorgewebe zu funktionstragendem Normalgewebe essentiell. Aus diesem Grund wird in der Neurochirurgie die zeitaufgelöste Thermografie wie auch Optical Imaging (OI) als neuartige Ansätze zur Identifikation eloquenter, funktionstragender Areale  eingesetzt. Zur Verbesserung der Genauigkeit der statistischen Datenanalyseverfahren sowie zur anatomischen Validierung sollen nun diese intraoperativen 2D Bilddaten mittels Bildregistrierung und projektiver Abbildung mit der korrespondierenden Stelle des zugehörigen präoperativen 3D MRT-Datensatzes fusioniert werden.

\textbf{Zielstellung}\\
In dieser Arbeit sollen Vorarbeiten zur Registrierung, Fusionierung und Segmentierung  von  präoperativen MRT-Datensätzen mit intraoperativen thermografischen Aufnahmen generalisiert und die Genauigkeit der Bildfusion verbessert  werden. Hierzu wird ausgenutzt, dass in navigierten neurochirurgischen Eingriffen die Lage und Orientierung von kalibrierten Objekten bzgl. eines a  priori registrierten Patientendatensatzes (MRT) kontinuierlich erfasst werden kann. Die zu entwickelnde interaktionsarme Bestimmung  der  extrinsischen  Kameraparameter  der  Wärmebildkamera  bzw. der OI-Kamera bzgl. des kalibrierten Objektes ermöglicht nun die Transformation des Kamerakoordinatensystems in das Patientenkoordinatensystem. Im Anschluss erfolgt die 2D-3D Bildfusion per projektiver Abbildung des 2D-Bildes auf die entsprechende Oberfläche des 3D-
Datensatzes. Mit einem vorhandenem physikalischen Phantom soll die Registrierungsgenauigkeit untersucht- sowie mögliche Einflussgrößen analysiert werden.

\textbf{Schwerpunkte}
\begin{enumerate}
    \item Literaturrecherche zu generischen Ansätzen der multimodalen Bildregistrierung und -fusion
    \item Interaktionsarme Kalibrierung der Kamerasysteme zur Referenzeinheit 
    \item Implementierung der Bildregistrierung sowie -fusion in 3D Slicer
    \item Quantitative Untersuchung von Einflussfaktoren auf die Registrierungs- und Fusionsgenauigkeit sowie Geschwindigkeit des Frameworks
    \item Optional: Erweiterung der Bildfusion um Informationen aus einer gegebenen 3D-Stereorekonstruktion der Oberfläche
\end{enumerate}
}

\sisetup{
    list-final-separator = { und },
    list-pair-separator  = { und },
    range-phrase         = { bis },
    separate-uncertainty = true
}
\def\lnot#1{\overline{#1}}
\def\fm#1{{\boldmath$#1$}}

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Eingabewerte:}}
\renewcommand{\algorithmicensure}{\textbf{Ausgabewerte:}}

\begin{document}

 % --------------------------------------------------------------------------- %   
    
\begin{abstract}
    Zur Abbildung der Struktur und der Funktionalität des zentralen Nervensystems werden bildgebende Verfahren des Neuroimagings eingesetzt. Mithilfe der multimodalen Bildfusion können mehrere dieser Bilder mit jeweils unterschiedlicher Modalitäten in ein informativeres Gesamtbild kombiniert werden. Die meisten medizinischen Anwendungen fusionieren ähnliche Bildmodalitäten, wie Magnetresonanztomographie (MRT), Computertomografie oder Ultraschall. Die Fusion von dreidimensionalen MRT- und zweidimensionalen Thermografiebilder, ist dabei noch weitestgehend unerforscht. In dieser Arbeit wird eine bildfusionsbasierter Visualisierungsansatz vorgestellt, der eine kombinierte Darstellung des Gehirns aus intraoperativen Thermografiebildern und einem präoperativen MRT-Bild ermöglicht. Dafür wird eine kalibrationsbasierte Registrierung eingesetzt, die ein perspektivisches Kameraabbildungsmodell verwendet. Mit einem selbst enwickelten thermografischen Kalibrationsmuster und mit einem Neuronavigationssystem werden präoperativ die Kameraparameter und eine Transformation von der Kamera zu einem statisch angebrachten Referenzadapter bestimmt. Die Thermografiedaten werden intraoperativ auf eine Oberfläche projiziert, die aus den MRT-Daten extrahiert wurde. Die finale Visualisierung entsteht durch ein Bildfusionsschema, welches in Echtzeit eine MRT-Volumengrafik und die projizierte Thermografiebilder fusioniert. Die Registrierung und die Oberflächenextraktion wurden hinsichtlich ihrer Genauigkeit evaluiert und die Qualität der erzeugten Visualisierung wurde bewertet. Die Evaluation ergab einen mittleren Registrierungsfehler von \SI{2,69}{mm} und einen mittleren Oberflächenrekonstruktionsfehler von \SI{4,00}{mm}. Die größten Fehlerquellen bildeten die extrinsischen Kameraparameter, die benutzte Gehirnsegmentierung für die Oberflächenrekonstruktion und das verwendete Neuronavigationssystem.
\end{abstract}

\selectlanguage{english}

\begin{abstract}
    Neuroimaging techniques are used to image the structure and function of the central nervous system. Multi-modal image fusion can be used to combine images from different modalities into a single image, that is more informative than any of the input images. Most medical applications fuse images from similar modalities like magnetic resonance imaging, computed tomography or ultrasound. Medical image fusion of three-dimensional MRI images and two-dimensional thermal images is mostly unexplored. This minor thesis presents an approach, that creates a fused visualisation of the brain from intraoperative thermal images and a preoperative MRI scan. A calibration based registration is used to determine a perspective camera model. A transformation from the camera to a reference adapter and the camera model parameters are determined presurgically with a thermal calibration pattern and a neuronavigation system. Thermografic data is intraoperatively projected onto a suface, that is reconstructed from the MRI scan. The final visualisation is formed with an image fusion scheme, that combines an MRI volume rendering with the projected thermal image in real time. The accuracy of the registration and the surface extraction was evalutated and the quality of the combined visualisation was assessed. The measurements showed an average registration error of \SI{2,69}{mm} and an average surface reconstruction error of \SI{4,00}{mm}. The major error source were the extrinsic camera parameters, the chosen brain segmentation for the extraction  of the brain surface and the used neuronavigation system.
\end{abstract}

\selectlanguage{ngerman}

% --------------------------------------------------------------------------- %
\chapter{Einführung}
Neurochirurgische Eingriffe werden durchgeführt, um Erkrankungen, Fehlbildungen und Schädigungen des zentralen Nervensystems zu behandeln. So muss z.B. ein bösartiger Hirntumor chirurgisch entfernt werden, um Komplikationen zu vermeiden, die bei dem Verbleib des Tumors entstünden. Das Ziel der intraoperativen Therapie ist die Entfernung von soviel Tumormasse wie möglich. Gleichzeitig ist es wichtig, dass das funktionstragende Gewebe intakt bleibt. Zur Erfüllung beider Operationsziele, müssen daher genaue Informationen über die Lokalisation und die Abgrenzung zwischen eloquenten Gehirnarealen und dem Tumorgewebe vorliegen. Zu diesem Zweck können bildgebenden Verfahren genutzt werden, um das Wissen über die Anatomie und die dynamischen Vorgänge des zentralen Nervensystems nichtinvasiv zu erfassen. Diese diagnostischen Verfahren, die der Abbildung des zentralen Nervensystems dienen, werden unter dem Überbegriff Neuroimaging zusammengefasst.

Das Einsatzgebiet von Neuroimagingverfahren lässt sich in zwei Hauptbereiche untergliedern. Zur Erfassung von anatomischen und strukturellen Merkmalen, können z.B. die Magnetresonanztomografie (MRT), die Computertomografie (CT), die Ultraschall- oder die Röntgenbildgebung verwendet werden. Für die Bestimmung von dynamischem Verhalten, wie der Gehirnfunktion, werden z.B. Verfahren wie die Elektroenzephalografie (EEG), die funktionale Magnetresonanztomografie (fMRT), die Positronen-Emissions-Tomographie (PET), die Einzelphotonen-Emissionscomputertomographie (SPECT), die Thermografie oder die optische Bildgebung genutzt. Manche Verfahren, wie beispielsweise MRT oder CT, werden aufgrund der räumlichen und zeitlichen Einschränkungen die sie mit sich bringen nur selten intraoperativ und gleichzeitig mit weiteren Bildgebungsverfahren eingesetzt. Stattdessen werden die Bilddaten einzeln und präoperativ aufgenommen. Um ein umfassendes Abbild des zentralen Nervensystems zu erschaffen, müssen die Informationen mehrerer Bildgebungsverfahren, von potentiell unterschiedlichen Zeitpunkten, kombiniert werden.

Zu diesem Zweck soll im Rahmen dieser Arbeit ein Framework zur fusionierte Visualisierung von präoperativen MRT-Daten und intraoperativen Thermografiedaten geschaffen werden. Im Gegensatz zu eigenständigen Visualisierungen von MRT- und Thermografiedaten, die bereits an vielen Stellen eingesetzt werden, ist eine Kombination beider Bildgebungsverfahren in einer gemeinsamen Visualisierung noch nicht üblich. Mit der Analyse- und Visualisierungssoftware 3D Slicer lassen sich bereits Thermografiedaten in Pseudofarbe und MRT-Daten als eine Volumengrafik darstellen. Für diese Software soll eine Erweiterung implementiert werden, die eine solche fusionierte Visualisierung umsetzt. Eine große Herausforderung stellt sich darin, die räumliche Relation zwischen den MRT- und Thermografiedaten zu ermitteln (der Prozess der Registrierung). Das Problem ist dabei, dass die Thermografie funktionale, dynamische Prozesse abbildet und dass die Magnetresonanztomografie anatomische Merkmale beschreibt.

Für die Registrierung der beiden Datensätze wird in dieser Arbeit ein Ansatz zu präoperativen Bestimmung der Abbildungseigenschaften der Thermografiekamera vorgestellt. Im selben Kontext wurde ein Ansatz zur prä- und intraoperativen Bestimmung der Kamerapose erarbeitet, der auf der Verwendung eines Trackingsystems und auf der Anbringung eines lokalisierbaren Referenzadapters basiert. Für die präoperativen Parameterbestimmung wurde dafür ein geeignetes Kalibrationsmuster gefertigt, welches sichtbare Merkmale in Thermografieaufnahmen besitzt. Neben der Bildregistrierung wurde eine Visualisierung entwickelt, welche die Thermografiedaten perspektivisch und in Pseudofarbe auf eine Oberfläche projiziert, die aus den MRT-Daten rekonstruiert wird. Für eine kombinierte Darstellung der Thermografiedaten und der MRT-Daten wurde eine weitere Visualisierung entwickelt, welche die projizierten Thermografiebilder mit arbiträr generierten MRT-Bildern fusioniert. Zuletzt wurde die Registrierungsgenauigkeit, die Visualisierungsqualität und die Performance des vorgestellte Frameworks evaluiert.

% --------------------------------------------------------------------------- %
\chapter{Verwandte Arbeiten}

Zur Erstellung einer fusionierten Darstellung von MRT- und Thermodaten müssen mehrere Teilschritte durchgeführt werden. Die Umsetzung dieser Schritte lässt sich in drei Hauptaufgaben unterteilen, die jeweils Gegenstand aktiver Forschung sind. Die erste Aufgabe, die Bildregistrierung, hat das Ziel die Koordinaten der MRT- und Thermografiedaten zueinander in Relation zu stellen. Ist eine räumliche Übereinstimmung bestimmt wurden, stellt sich als nächstes die Aufgabe der wissenschaftlichen Visualisierung. Sie behandelt Methoden mit denen Bilder erzeugt werden können, die alle relevanten Informationen aus einem Datensatz sinnvoll darstellen. Die Bilderzeugung muss dabei gewährleisten, dass die Daten unverfälscht wiedergegeben werden, dass die Bilder mit angemessenem Aufwand erzeugt werden und dass sie für die Fähigkeiten der menschliche Wahrnehmung geeignet sind. Diese Arbeit beschränkt sich dabei auf die wissenschaftliche Visualisierung der Thermografiedaten. Es wird angenommen, dass eine Visualisierung der MRT-Daten gegeben ist. Zuletzt müssen die so entstandenen Bilder in ein einziges, informativeres Bild überführt werden, das ist die finale Aufgabe der Bildfusion.

In einer Vorarbeit von Hoffmann \textit{et al.} wurde bereits ein Framework zur fusionierten Darstellung von MRT- und Thermografiedaten vorgestellt \cite{Hoffmann2017}. Für die Bildregistrierung wurde manuell, präoperativ eine Transformation von einem statisch befestigten Referenzadapter zu der Thermografiekamera ermittelt. Anschließend wurde der Referenzadapter, in einem Koordinatensystem mit bekannter Transformation zu den MRT-Daten, intraoperativ getrackt, um die Thermografiebildebene auf die MRT-Daten zu registrieren. Die Thermografiedaten wurden per Parallelprojektion auf einer Isofläche der MRT-Daten in Pseudofarbe visualisiert. Die graphische Repräsentationen von den projizierten Thermografiedaten und von der MRT-Isofläche wurde per \textit{Alpha Blending} in ein einziges Bild überführt.

Weil eine manuelle Registrierung der Daten viel Potential für Fehler bietet, soll in dieser Arbeit die Bildregistrierung so gestaltet werden, dass die dazu nötige präoperative Parameterbestimmung automatisch durchführbar ist. Die zur Visualisierung eingesetzte Parallelprojektion ist nur eine vergleichsweise ungenaue Approximation für das Abbildungsverhalten der Thermografiekamera. Daher soll die anschließende Visualisierung der Thermografiedaten so erfolgen, dass die Projektion der Bilddaten mehr dem tatsächlichen Verhalten der Thermografiekamera ähnelt (z.B. mithilfe einer perspektivischen Projektion). Außerdem sollen die Bilder mit einem aktuellen Bildfusionsverfahren so kombiniert werden, dass die wesentlichen, wahrnehmbaren Merkmalen aus beiden Bildern erkenntlich sind. Das in der Vorarbeit verwendete \textit{Alpha Blending} ähnelt unter der Verwendung einer konstanten Transparenz einer einfachen gewichteten Mittelwertbildung, die diese Bedingung nicht immer konsequent umsetzt.

    \section{Bildregistrierung}

Das Ziel der Bildregistrierung ist es, eine Übereinstimmung zwischen zwei oder mehreren Bilder zu erzeugen. Dazu muss eine Transformation bestimmt werden, die ein Bild so transformiert, dass es in einem gemeinsamen Raum mit dem anderen Bild liegt. Die Bilder zeigen dabei im Idealfall Ausschnitte des selben Bildgegenstand in unterschiedlichen Bildräumen. Das Bild welches im Zielraum liegt und auf das die anderen Eingangsbilder registriert werden sollen, wird als Referenzbild bezeichnet. Alle Bilder welche die Bildregistrierung zu diesem Zweck transformiert, werden als Objektbilder bezeichnet. Die gesuchte Transformation bildet also die Punkte der Objektbilder auf zugehörige Punkte im Referenzbild ab. Die Bildpunkte die von der Beobachtung des selben Ortes auf dem Bildgegenstand stammen, sollen nach der Anwendung der Transformation in allen Bildern an der gleichen Stelle im Zielraum liegen. Die Bilddaten können sich dabei im Aufnahmeort, in dem Aufnahmezeitpunkt, in dem verwendeten Bildsensor, in der dargestellten Bildmodalität, in der Dimensionalität der Bildräume und/oder im Zustand des aufgenommen Bildgegenstands variieren. Der Unterbereich der medizinische Bildregistrierung, der den Bildgegenstand auf die Anatomie einer zu behandelnden Person beschränkt, ist dabei besonders relevant für die Zielstellung dieser Arbeit.

Einen allgemeinen Überblick über die unterschiedlichen Arten der medizinischen Bildregistrierung liefert eine Analyse von Maintz und Viergever \cite{Maintz1998}, die bereits 1998 veröffentlicht wurde. Sie stellen darin eine Taxonomie, zur Einordnung von unterschiedlichen Registrierungsmethoden für medizinische Bilddaten, vor. Die Klassifikation erfolgt anhand von folgenden Kriterien:

Die \textbf{Dimensionalität der Bilder} beschreibt die Kombination der Dimensionalität der Objektbilder und der Dimensionalität des Referenzbildes, die darüber entscheidet ob eine 2D-2D-, eine 2D-3D- oder eine 3D-3D-Registrierung nötig ist. Darin inbegriffen ist auch der zeitlich Aspekt, der beschreibt ob die Objektbilder von gleichem Zeitpunkt wie das Referenzbild stammen oder ob die Objektbilder eine Bildreihe aus Aufnahmen von unterschiedlichen Zeitpunkten formen.

Die \textbf{Registrierungsgrundlage} unterteilt in extrinsische, intrinsische oder kalibrationsbasierte Bildregistrierungsprozesse. Eine Extrinsische Bildregistrierung basiert auf der Einführung von fremden Objekten in den Bildraum. Eine Intrinsische Bildregistrierung verwendet hingegen nur Informationen über den Bildgegenstand, der in den zu registrierenden Bildern bereits enthalten ist. Eine kalibrationsbasierte Bildregistrierung ist komplett unabhängig von den Bilddaten. Stattdessen werden alle Informationen über die verwendeten Koordinatensystem aggregiert, um eine Transformation direkt zu berechnen.

Die \textbf{Klasse der verwendeten Transformation} stellt fest welche Eigenschaften der Objektbilder die Transformation erhält. Je weniger Eigenschaften erhalten werden, desto mehr Transformationen fallen in die zugehörige Klasse (Teilmengenrelation). Die übergreifenste Klasse bilden generische Verformungstransformationen die nur Nachbarschaftsrelationen erhalten. Projektive Transformationen erhalten zusätzlich die Kollinearität von Punkten, sodass gerade Linien gerade bleiben. Affintransformationen halten außerdem die Parallelität von Linien aufrecht. Die einfachste Art von Transformationen bilden die Festkörpertransformationen, die gewährleisten, dass Winkel und Distanzen unverändert bleiben.

Die \textbf{Domäne der Transformation} unterscheidet, ob die Transformation global und einheitlich auf das gesamte Bild angewandt wird oder ob unterschiedliche Transformationen lokal auf verschiedene Bildteile angewandt werden. 

Die \textbf{Notwendige Interaktion} beschreibt, ob die Parameter interaktiv, semi-automatisch oder automatisch bestimmt werden. Im interaktiven Fall werden die Parameter komplett manuell bestimmt, die Registrierungssoftware bietet dabei nur eine Hilfestellung. Im semi-automatischen Fall wird ein Teil der Parameter manuell bestimmt, so könnte z.B. die initiale Transformation vorgegeben werden. Im automatischen Fall werden alle Parameter direkt von der Software bestimmt.

Mit der Art der \textbf{Parameterbestimmung} wird unterteilt, ob die Parameter direkt berechnet werden oder ob die Parameter, z.B. anhand der Optimierung einer Zielfunktion, gesucht werden.

Anhand der \textbf{Involvierten Modalitäten} wird zwischen monomodaler Registrierung, multimodaler Registrierung, Modalität-zu-Model-Registrierung und Modalität-zu-Patient-Registrierung unterschieden. Basieren alle Bilder auf der gleichen Modalität ist von monomodaler Registrierung die Rede. Eine multimodale Registrierung bringt stattdessen Bilder in Übereinstimmung, die auf unterschiedlichen Modalitäten basieren. Lässt sich das Referenzbild als ein mathematisches Modell beschreiben, auf das aufgenommene Objektbilder einer Modalität registriert werden, liegt eine Modalität-zu-Model-Registrierung vor. Bei der Modalität-zu-Patient-Registrierung ist das Referenzbild eine abstrakte Information über die Lage der zu behandelnden Person.

Anhand der \textbf{Involvierten Subjekte} wird die Bildregistrierung in eine Intrasubjektregistrierung, eine Intersubjektregistrierung oder eine atlasbasierte Registrierung unterteilt. Bei der Intrasubjektregistrierung stammt das Referenzbild und die Objektbilder von der selben zu behandelnden Person. Stellen die Objektbilder hingegen einer andere Person dar, als die des Referenzbildes, wird das als Intersubjektregistrierung bezeichnet. Bei der atlasbasierten Registrierung ist das Referenzbild keine echte Aufnahme des Bildgegenstands, sondern stattdessen ein synthetisierter Prototyp des Bildgegenstands.

Die \textbf{Involvierten Objekte} beschreiben den medizinisch relevanten Bildgegenstand, der in den Objektbildern und dem Referenzbild dargestellt ist. Mainz und Viergever listen hierfür den Kopf, Thorax, Abdomen, Pelvis und Perineum, die Gliedmaßen und die Wirbelsäule als mögliche Objekte auf.

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{images/maintz1998_medical_image_registration_taxonomy}
    \caption{\label{taxonomy_medical_image_registration_fgr}Taxonomie der medizinischen Bildregistristrierung nach Maintz und Viergever \cite{Maintz1998}}
\end{figure}

Diese Taxonomie wurde 2016 in einem Review von Viergever \textit{et. al} \cite{Viergever2016} bezüglich ihrer Aktualität überprüft. Nach dem Fazit des Reviews funktioniert die Taxonomie noch weitgehend. Weitere Kriterien (wie z.B. die Unterscheidung in paar- oder gruppenbasierte Registrierung) wären zwar vorstellbar, aber sie wurden nicht näher spezifiziert. Weiterhin wurde angemerkt, dass für einige Kategorien eine genauere Unterteilung nötig wäre.

Die Aufgabenstellung dieser Arbeit und deren Anwendungsbereich, schränkt die Registrierungsart nach dem von Maintz und Viergever vorgestellten Schema ein. Weil MRT- und Thermografiedaten registriert werden sollen, wird eine multimodale Registrierung benötigt. Dabei muss es sich um eine Intrasubjekt-Registrierung handeln, da für die beschriebenen Anwendungsfälle die Daten immer von einer Person stammen. Die vorliegende Daten beschränken sich außerdem auf das Gehirn, das involvierte Objekt ist daher die Kopfregion. Die MRT-Daten sind dreidimensional und die Thermografiedaten sind zweidimensional, dementsprechend muss eine 2D-3D-Registrierung vorgenommen werden. Da mehrere Thermografiedaten von aufeinanderfolgenden Zeitpunkten registriert werden sollen, bilden die Objektbilder eine zeitlich variable Bildreihe. Zur genauen Unterscheidung von 2D-3D-Registrierungsalgorithmen ist es relevant, ob die 2D-Daten einen Schnitt durch das aufgenommen 3D-Volumen repräsentieren oder ob die 2D-Daten durch eine projektive Abbildung des aufgenommen 3D-Volumens entstanden sind. Für die Thermografiedaten ist letzteres der Fall, weil die temperaturabhängige Infrarotstrahlung durch ein Linsensystem auf ein Sensorarray fällt. Der Beobachtungsraum der Thermografiedaten besteht aus den Projektionen der dreidimensionalen Oberflächenpunkte, die im Sichtfeld der Kamera liegen. Die selben Oberflächenpunkte werden auch von den MRT-Daten beschrieben.

Markelj \textit{et al.} veröffentlichen 2012 einen Bericht über aktuelle 2D-3D-Bildregistrierungsmethoden für bildgestützte, medizinische Eingriffe \cite{Markelj2012}. Darin beschäftigten sie sich überwiegend mit der Registrierung von präoperativen MRT-/CT-Bildern auf intraoperative Röntgenbilder. In der Arbeit ist das Problem der Registrierung so formuliert, dass beide Bilder in ein gemeinsames Koordinatensystem überführt werden sollen. Die Transformation von dem 2D-Bildraum in das gemeinsame Koordinatensystem wird dabei als bekannt angenommen, gesucht ist daher eine Transformation vom 3D-Bildraum in das gemeinsame Koordinatensystem. Für die 2D-3D-Bild-zu-Bild-Registrierung ist es das Ziel, die gesuchte Transformation optimal bezüglich eines Übereinstimmungskriteriums zu wählen. Dafür müssen entweder die Projektionen der transformierten 3D-Bilder auf die Domäne des 2D-Bilds mit den 2D-Bilddaten übereinstimmen oder die transformierten 3D-Daten müssen mit den Rückprojektionen oder Rekonstruktionen der 2D-Daten auf die Domäne des 3D-Bilds übereinstimmen. Mit dieser Zieldefinition werden drei Strategien vorgestellt, mit denen die Daten auf eine gemeinsame Dimensionalität abgebildet werden können. Bei der Projekionsstrategie werden 3D-Daten mit einer zugehöriger Projektionsabbildung in 2D-Daten umgewandelt. Das Gegenteil bildet die Rückprojektionsstrategie, bei der die 2D-Bilder mit einer Rückprojektionsabbildung in 3D-Daten umgewandelt werden. Bei der Rekonstruktionsstrategie werden mehrere 2D-Bilder mit einer Rekonstruktionsfunktion in ein einziges 3D-Modell umgewandelt. Die Projektionsstrategie reduziert das Problem somit auf eine 2D-2D-Bildregistrierung, während die Rückprojektions- und Rekonstruktionsstrategie zu einer 3D-3D-Bildregistrierung führen. In dem Bericht wurden verschiedene 2D-3D-Bildregistrierungsmethoden vorgestellt, die anhand dem von Maintz und Viergever \cite{Maintz1998} vorgestellten Kriterium der Bildregistrierungsgrundlage klassifiziert wurden.

Ein Beispiel aus der Klasse der extrinsische 2D-3D-Bildregistrierungsalgorithmus liefert Aubry \textit{et al.}, deren Registrierungsansatz basiert auf der Einbringung von Goldmarkern, um die Bewegung von Prostatakrebs in CT-Bildern zu ermitteln \cite{Aubry2004}. Dazu wurde die Rekonstruktionsstrategie angewandt, um die 3D-Positionen der Marker aus mehreren Portal Imaging-Aufnahmen zu ermitteln. Die so ermittelten 3D-Positionen wurden auf die 3D-Positionen der Marker in präoperativen CT-Bildern registriert. Aufgrund der geringen Größen der Punktmengen lässt sich die gesuchte Transformation dabei effizient bestimmen.


Nach Markelj \textit{et al.} lässt sich die Klasse der intrinsische Algorithmen für die 2D-3D-Bildregistrierung in merkmalsbasierte, intensitätsbasierte, und gradientenbasierte Verfahren unterteilen. Die merkmalbasierten Algorithmen versuchen eine Transformation zu finden, welche die Distanz zwischen den 3D-Merkmalen, die aus dem präoperativen 3D-Bildern oder aus einem Model extrahiert wurden, und ihren korrespondieren 2D-Merkmalen minimiert. Die Distanz wird dabei nach der Dimensionalitätsangleichung in der gemeinsamen Dimension berechnet. Ein Merkmal kann z.B. ein alleinstehender Punkt, eine Ecke oder eine Punktmenge, die einer bestimmten Form besitzt, sein. Die Registrierungsgenauigkeit hängt dabei zusätzlich von der Genauigkeit der Merkmalsdetektion ab. Da die Anzahl der Merkmale i.d.R. relativ gering ist, kann die Registrierung vergleichsweise schnell berechnet werden.

Zheng \textit{et al.} stellten ein merkmalsbasiertes 2D-3D-Bild-Model-Registrierungsschema zur Rekonstruktion von Knochenoberflächen vor, welches eine iterative Kombination der Projektions- und Rekonstruktionsstrategie verwendet  \cite{Zheng2006}. Die Konturen eines Models werden, gemäß der Projektionsstrategie, auf 2D-Fluoroskopiebilder projiziert um Korrespondenzen mit Ecken-Merkmalen zu bestimmen. Dann werden, gemäß der Rekonstruktionsstrategie, aus den 2D-Merkmalspunkten neue 3D-Merkmalspunkte generiert. Der neue 3D-Merkmalspunkt zu einem 2D-Merkmalspunkt, ist der Punkt entlang des Projektionsstrahls durch den 2D-Merkmalspunkt, der den minimalen Abstand zu dem alten korrespondierenden 3D-Konturpunkt besitzt. Die so neu entstandene 3D-Punktwolke wird auf die alte 3D-Punktwolke registriert, indem eine Verformungstransformation gesucht wird, welche die Abstände zwischen den 3D-3D-Punktkorrespondenzen minimiert.

Intensitätsbasierte 2D-3D-Bildregistrierungsalgorithmen verwenden direkt die vorhandenen Intensitätswerte an den Voxel-/Pixelpositionen. Pixel/Voxel die dabei nach der Anwendung der Transformation an derselben Position liegen, werden als korrespondierend betrachtet. Eine Transformation wird so optimiert, dass die Summe eines Ähnlichkeitsmaßes für alle korrespondierenden Pixel/Voxel maximal wird. Mögliche Ähnlichkeitsmaße sind z.B. Mutual Information oder die Kreuzkorrelation. Die Zielfunktion hat i.d.R. viele lokale Maxima, sodass die initiale Transformation nahe an der gesuchten sein sollte, damit auch das globale Maximum gefunden wird. Aufgrund der hohen Informationsmenge die jedes mal verarbeitet werden muss, sind darauf basierende Registrierungsalgorithmen vergleichsweise langsam.

Birkfellner \textit{et al.} beschreiben einen intensitätsbasierten 2D-3D-Bildregistrierungsalgorithmus, der zur Registrierung von CT- und Röntgenbilder verwendet werden kann \cite{Birkfellner2003}. Aus den 3D-CT-Bildern werden mit einer initialen Festkörpertransformation per \textit{Ray Casting} simulierte Röntgen-Bilder generiert (Projektionsstrategie). Diese Bilder werden als digital rekonstruierte Röntgenaufnahme (DRR) bezeichnet. Die initiale Transformation wird schrittweise verbessert, indem wiederholt ein DRR-Bild erzeugt wird, für das eine optimale Transformation bezüglich der Zielfunktion ermittelt wird. Das Ähnlichkeitsmaß der Zielfunktion basiert auf einem 2D-Differenzbild und es ergibt sich durch das Anwenden einer Faltungsmatrix, welche die lokale Umgebung jedes Pixels im Differenzbild vergleicht.

Gradientenbasierte Verfahren nutzen den Zusammenhang, dass ein Projektionsstrahl der eine projizierte, zweidimensionale Kanten schneidet, auch gleichzeitig den zugehörigen Punkt auf der dreidimensionale Oberflächen tangential schneiden muss. Daher muss ein Projektionsstrahl, der durch eine Kante im 2D-Bild verläuft, auch durch ein lokales Maximum im Gradientenvektorfeld verlaufen, wenn die Bilder sich überlagern. Dazu müssen allerdings nur die Gradienten betrachtet werden, die eine bestimmte Länge überschreiten. Dadurch reduziert sich die Informationsmenge die im Vergleich mit intensitätsbasierten Verfahren verarbeitet werden muss und die Registrierung kann schneller erfolgen.

Wein \textit{et al.} schlagen einen solchen gradientenbasierten Algorithmus vor \cite{Wein2005}. Die Idee der Optimierung ist ähnlich wie in Arbeit von Birkfellner \textit{et al.} \cite{Birkfellner2003}. Statt der Intensitäten, werden allerdings die projizierte Gradienten mit den Bildgradienten verglichen. Deswegen wird zur Bestimmung der Transformation ein Zielfunktion anhand eines Gradientenkorrelationsmaßes optimiert. Der Wert des Ähnlichkeitsmaß ist dann maximal, wenn Gradienten in die gleiche Richtung zeigen. Zur Berechnung der Zielfunktion schlagen sie zwei Methoden vor. Die erste ist die Erzeugung eines DRR-Gradientenbilds mittels \textit{Ray Casting} durch das Gradientenvolumen, wobei kurze Gradienten anhand eines Octree ignoriert werden. Die zweite Variante traversiert jeden Voxel mithilfe von \textit{Splatting} nur einmal und sie berechnet so das Gradientenkorrelationsmaß direkt.

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{images/markelj2012_3D-2D_registration.png}
    \caption{\label{3D-2D-registration_fgr}Geometrische Konfigurationen der intrinsischen 2D-3D-Registrierungsmethoden gemäß der Registrierungsgrundlage und der Dimensionalitätssangleichungsstrategie \cite{Markelj2012}}
\end{figure}

Alle möglichen Kombinationen der intrinsischen 2D-3D-Registrierungsmethoden sind in der Abbildung \ref{3D-2D-registration_fgr} aufgeführt. Die Kombinationen ergeben sich dabei anhand der Registrierungsgrundlage und anhand der Strategie zur Dimensionalitätsangleichung.

Die letzte Klasse der Bildregistrierungsgrundlagen bilden kalibrationsbasierte Algorithmen. Sie Basieren auf sorgfältig kalibrierten Kameras und der Bestimmung der relativen Positionen und Orientierungen der Kamera zum Operationstisch / gemeinsamen Koordinatensystem. Dazu werden verschiedene bewegliche Elemente getrackt, um die Transformationen zwischen den verwendeten Koordinatensystemen zu ermitteln. Da alle nötigen Informationen bekannt sind, lässt sich die gesuchte Transformation somit direkt berechnen.

Iwai und Sato verwenden ein kalibrationsbasiertes 2D-3D-Bildregistrierungsverfahren um Thermografiebilder mithilfe eines Beamers und einer RGB-Kamera auf die bemessene Szene zu projizieren \cite{Iwai2010}. Die Transformationen von den Bildschirmkoordinatensystemen in das Szenenkoordinatensystem werden mithilfe des Direkten Linearen Transformations-Algorithmus (DLT) bestimmt. Dazu wird ein Referenzwürfel so in den Sichtbereich der RGB-Kamera, in den Sichtbereich der Thermografiekamera und in den Projektionsbereich des Beamers gestellt, dass mehrere seiner Seiten in allen Bereichen sichtbar sind. Der Referenzwürfel hat räumlich bekannte Merkmale, welche in der Modalität der RGB-Kamera und in der Modalität der Thermografiekamera sichtbar sind. Anhand des Referenzwürfels wird gleichzeitig das Szenenkoordinatensystem definiert. Die Merkmalspunkte werden in den Wärme- und RGB-Bildern detektiert und mit dem DLT-Algorithmus wird die Transformation von dem Szenenkoordinatensystem zu den Bildschirmkoordinatensystemen der Kameras ermittelt. Der Beamer projiziert ein Muster, das auf dem Gray Code basiert, auf den Referenzwürfel, welches von der kalibrierten RGB-Kamera erkannt wird. Somit lassen sich Korrespondenzen zwischen den Bildkoordinaten des Beamers und Referenzpunkten in der Szene ermitteln, mit denen wiederum die Transformation von Szenenkoordinatensystem zum Bildschirmkoordinatensystems des Beamers bestimmt wird. Die Transformationen zwischen den einzelnen Bildschirmkoordinatensystem lassen sich dann berechnen, um ein Thermografie-Bild so zu transformieren, dass es vom Beamer auf die bemessen Stellen projiziert werden kann.

Ayaz \textit{et al.} stellen eine Methode vor, mit der Bilder der funktionalen Nahinfrarotspektroskopie (fNIR) auf einen MRT-basierten Gehirnoberflächenprototypen registriert werden können \cite{Ayaz2006}. Zur Aufnahme der fNIR-Bilder wurde das Sensorarray, welches aus insgesamt 16 Sensoren besteht, an eine feste Position auf der Stirn der zu behandelnden Person angebracht. Auf der horizontalen Achse wurde das Sensorarray so positioniert, dass die zentrale y-Bildachse parallel zur longitudinalen Symmetrieachse des Kopfes (zwischen den Augen) verläuft. Auf der vertikalen Achse wird das Sensorarray gemäß dem internationalen 10/20-System zur Positionierung von Elektroden angebracht. Für den MRT-Prototypbild liegen die korrespondieren Punkte in diesem System vor, somit lässt sich die vertikale Verschiebung des Bildes bestimmen. Dann wird aus dem MRT-Bild in der transversalen Ebene, auf der Höhe der bekannten Punkte, eine Gehirnkontur extrahiert. Anschließen wird anhand der geometrische Informationen über das Sensorarray, die aufgrund der gewählten Positionierung bekannt ist und anhand der Projektionen der Gehirnkontur, die horizontale Verschiebung des fNIR-Bilds berechnet.

Sanches \textit{et al.} haben in ihrer Arbeit beschrieben, wie aus Thermografie- und MRT/CT-Bildern ein 3D-Modell der Kopfregion erzeugt werden kann, das die Temperaturinformationen an allen Oberflächenpunkten enthält \cite{Sanches2013}. Die Thermografiedaten wurden dazu aus vier festen Winkeln aufgenommen (\SIlist{0;90;180;270}{\degree}). Gemäß dieser Winkel werden aus den MRT-Bildern per Projektion 2D-Tiefenbilder gerendert. Auf den Tiefenbildern und auf den Thermografiebildern werden manuell korrespondierende Punkte der Kopfkontur ausgewählt. Anhand dieser Korrespondenzen wird iterativ eine optimale Affintransformtion gesucht. Mit der bestimmten Transformation und den Tiefenbildern, werden dann die Temperaturwerte Modellpunkten zugeordnet.

Für eine intraoperative Bildfusion muss die Registrierung in Echtzeit berechenbar sein, da die Kameraposition während der Operation verändert werden kann. Deswegen sind Registrierungsalgorithmen, die eine Verformungstransformationen bestimmen, dafür weniger geeignet. Die nötige Berechnung würde mit hoher Wahrscheinlichkeit, aufgrund der hohen Anzahl der zu optimierender Variablen, zu lang dauern. Dementsprechend kommen Festkörpertransformationen, Affintransformationen oder Perspektivische Transformationen infrage. Extrinsische Registrierungsverfahren können nicht verwendet werden, da die Visualisierung ohne Eingriffe an der zu operierenden Person erfolgen soll. Für eine merkmalsbasierte Bildregistrierung müssen verlässlich Merkmale vorhandener, anatomischer Merkmale gefunden werden, die zu jedem Zeitpunkt, für jeden Eingriff, gleichzeitig in MRT- und Thermographie-Daten vorhanden sind. Dieses Problem ist aufgrund der Modalitätsunterschiede ein offenes Problem in der Forschung. Für eine intensitätsbasierte Registrierung muss eine Ähnlichkeitsmaß definiert werden, welches sinnvoll eine Ähnlichkeit zwischen Temperaturwerten und MRT-Kontrastwerten beschreibt. Da diese Werte nur zum Teil voneinander abhängen und potentiell komplementäre Informationen vorliegen, ist das im Rahmen dieser Arbeit schwer vorstellbar. Eine gradientenbasierte Registrierung wäre möglich, wenn ein MRT-Kontrastwertänderung immer mit einer gleich ausgerichteten Temperaturänderung verbunden wäre. Da der Temperaturverlauf und der Kontrastwertverlauf aber ebenso potentiell komplementär verläuft, ist das vermutlich genau so schwer realisierbar wie eine intensitätsbasierte Registrierung. Eine kalibrationsbasierte Registrierung ist möglich, wenn alle beteiligten Koordinatensysteme und deren Relation zueinander zur Laufzeit bestimmbar sind. In der Vorarbeit von Hoffmann \textit{et al.} \cite{Hoffmann2017} wurde bereits gezeigt, dass dies unter Verwendung eines Neuronavigationssystems, mithilfe des Trackings von Referenzadaptern und mit einer manuellen Registrierung der Kamera auf einen Referenzadapter möglich ist. Das selbe System wird in der Klinik für Neurochirurgie am Universitätsklinikum Dresden eingesetzt und steht auch für diese Arbeit zur Verfügung. Neuronavigationssysteme verwenden i.d.R. eigene Registrierungsverfahren, mit dem ein MRT-Koordinatensystem in ein eigenes Szenenkoordinatensystem transformiert wird, in dem auch alle Positionen und Orientierungen der getrackten Adapter und Instrumente definiert sind. Daher bestimmen diese Systeme, ähnlich wie in der Arbeit von Iwai und Sato \cite{Iwai2010}, eine Transformation vom Kamerakoordinatensystem in das Szenenkoordinatensystem.
    
    \section{Visualisierung von projektiven Daten}

Das Ziel der wissenschaftlichen Visualisierung von Thermografiedaten ist es, ein Bild zu erzeugen, welches die Temperaturwerte aus dem zweidimensionalem Beobachtungsraum unverfälscht darstellt. Die Temperaturen sollen dabei leicht erfassbar sein und die Bilderzeugung muss in angemessener Zeit durchgeführt werden können. Da der zweidimensionale Merkmalsraum der Thermografiedaten aus Skalaren besteht, ist eine Pseudofarbdarstellung mit einer einfache Abbildung von Temperaturewerten auf Farbwerte oftmals hinreichend. Damit die Temperatur eindeutig bestimmbar ist, sollte die Abbildungsfunktion injektiv sein. Bei einer kombinierten Darstellung von MRT- und Thermografiedaten muss an jedem Pixel der dargestellte Ort aus dem gemeinsamen Beobachtungsraum übereinstimmen. Da die Thermografiedaten für den vorliegenden Anwendungsfall von der Gehirnoberfläche stammen, müssen sie auch auf diesen zugehörigen Oberflächenpunkten dargestellt werden. Ist die Oberfläche bekannt, müssen mithilfe der Transformation zwischen den Beobachtungsräumen und einer Kameraabbildung die Korrespondenzen zwischen den Thermografiebildpunkten und den bemessenen Oberflächenpunkten bestimmt werden. Dabei gibt es zwei unterschiedliche Verfahren, die einen direkt Einfluss auf die Bilderstellung haben. Eine Option ist es, ausgehend vom dreidimensionalen Beobachtungsraum die Oberflächenpunkten mit der Kameraabbildung der Thermografiekamera auf die Bildebene zu projizieren. Dieser Schritt muss potentiell für alle Oberflächenpunkte durchgeführt werden. Der andere Ansatz geht vom Beobachtungsraum der Thermografiedaten aus und verfolgt die Projektionsstrahlen vom Fokuspunkt der Thermografiekamera durch deren virtuelle Bildebene, um die Schnittpunkte mit der Oberfläche zu bestimmen. Dieser Schritt muss für alle Bildpunkte der Thermografiedaten durchgeführt werden. Falls so ein \textit{Ray Casting}-Ansatz in Kombination mit einer Volumendarstellung der Oberfläche eingesetzt wird, dauert ein einzelner Schritt potentiell länger, da der Projektionsstrahl durch das Oberflächenvolumen iterativ verfolgt werden muss.

Segal \textit{et al.} beschrieben zuerst einen Renderingansatz, um Bilder, Lichtpunkte und Schatten perspektivisch auf Geometrie zu projizieren \cite{Segal1992}. Das Resultat sieht dabei so aus, wie wenn ein Projektor oder ein Beamer die Bilder über die Geometrie geworfen hätte. Dazu wird das zu projizierende Bild als eine Textur verwendet. Die passenden Texturkoordinaten werden dynamisch berechnet. Dazu werden die homogene Koordinaten der Geometriepunkte mithilfe einer Projektionsmatrix vom Weltkoordinatensystem in das Texturkoordinatensystem des Projektors überführt. Geometriepunkte die nicht vom Projektor gesehen werden, erhalten dabei auch Texturkoordinaten und entsprechende Farben. Außerdem wurden ein Ansatz für die Erzeugung von Schatten vorgestellt. Dafür wird zunächst ein Tiefenpuffer aus Sicht des Projektors erzeugt. Beim Rendern aus der eigentlichen Ansicht, werden die Werte im Tiefenbuffer mit den berechneten Tiefen im Projektorkoordinatensystem verglichen. Sind die Werte gleich, ist der Bildpunkt vom Projektor sichtbar und die Farbe der Geometrie bleibt unbeeinflusst. Ist das nicht der Fall liegt der Punkt im Schatten und deswegen wird die Farbe dementsprechend abgedunkelt.

Dey \textit{et al.} verwenden eine ähnlichen Algorithmus, um registrierte Endoskopiebilder auf eine Gehirnoberfläche abzubilden \cite{Dey2002}. Die Prinzipien des Algorithmus sind gleich wie bei Segal \textit{et al.} \cite{Segal1992}, wobei der Algorithmus als Pseudocode für einen Vertex-Shader formuliert wird. Weiterhin schlagen sie ebenfalls die Erzeugung eines Tiefenpuffers und den Tiefenvergleich vor, um nur direkt sichtbare Gehirnoberflächenelemente mit den Endoskopiebildern zu texturieren. Beide Ansätze setzen ein Polygonnetz als Oberflächenrepräsentation voraus. Oberflächenpunkte können allerdings auch während des Renderings direkt aus den volumetrischen Daten extrahiert werden.

Einen Möglicher Renderingansatz der im Bildraum arbeitet, um eine Isofläche aus Gitterdaten darzustellen, wird von Choi \textit{et al.} beschrieben \cite{Choi2000}. Der erste sichtbare Oberflächenpunkt wird dabei pro Pixel mittels \textit{Ray Casting} durch das Volumen bestimmt. Der Projektionsstrahl der vom Kamerazentrum durch die virtuelle Bildebene verläuft, tastet dabei in regelmäßigen Schritten die Volumendaten ab, um den Schnittpunkt mit der Isofläche zu bestimmen. Da Zugriffe auf die Volumendaten i.d.R. ineffizient sind, wird bei dem initialen Rendern aus dem Tiefenpuffer ein Begrenzungsvolumen bestimmt. Ändern sich die Ansicht oder der Isowert nicht, können die Tiefenwerte verwendet werden, um einen besseren Startwert für die Strahlengleichung zu verwenden. Außerdem wird ein Octree-ähnliche Datenstruktur erzeugt, welche die minimalen und maximalen Intensitätswerte ihrer Knoten speichert. Unter Verwendung dieser Datenstruktur lässt sich das Abtasten von Strahlenpunkten, die eine niedrigere Intensität als den vorgegebenen Isowert besitzen, reduzieren.

Der vorgeschlagener Ansatz von Segal \textit{et al.} \cite{Segal1992} ist prinzipiell verwendbar, wobei die vom virtuellen Projektor nicht sichtbaren Objektstellen auch eingefärbt werden. Deswegen ist der abgewandelte Ansatz von Dey \textit{et al.} \cite{Dey2002} noch besser geeignet, da die nicht sichtbaren Objektstellen, ähnlich wie bei der Schattenberechnung ignoriert werden. Außerdem ist der Algorithmus bereits als Pseudocode für einen Vertex-Shader vorhanden, was eine GPU-basierte Implementierung erleichtert. Ray Casting-Ansätze wie z.B. der von Choi \textit{et al.} \cite{Choi2000} verlangen zwar keine präoperative Erzeugung eines Polygonnetzes, sie sind aber in der intraoperativen Berechnung aufwendiger. Außerdem ist der Ansatz von Choi \textit{et al.} dafür ausgelegt, dass das Bild direkt aus der verwendeten Kameraansicht gerendert wird. Da aber die Korrespondenzen zwischen Bild- und Oberflächenpunkten aus der Sicht der Thermografiekamera ermittelt werden müssen, lässt sich der vorgestellte Ansatz nicht unmodifiziert für das Rendering von beliebigen Kameraansichten verwenden.
  
    \section{Bildfusion}

Das Ziel der Bildfusion ist es, mehrere Eingabebilder in ein informativeres, vollständigeres Gesamtbild zu kombinieren, welches alle relevanten Informationen der Eingabebilder besitzt. Die Bilder zeigen einen Ausschnitt des selben Bildgegenstands in einem gemeinsame Koordinatensystem. Die Eingabebilder können sich im Aufnahmezeitpunkt, im verwendeten Bildsensor und in der dargestellten Bildmodalität unterscheiden. Ein optimales Ergebnis ist abhängig von der Anwendungsart und es ist zunächst nicht formal definiert. Ein mögliches Ziel wäre z.B. die Erhöhung der Sicherheit der Informationen über den gezeigten Bildgegenstand. Das Ziel kann auch darin bestehen, die relevanten Informationen aus potentiell widersprüchlichen Daten zu selektieren. Eventuell stellt ein fusionierte Bild Informationen dar, die sich nur aus der Betrachtung aller Daten ergeben. Ein weiteres Ziel ist es, mehrere unabhängige Eigenschaften in ein Bild kombinieren. Der Unterbereich der medizinische Bildfusion, der den Bildgegenstand auf die Anatomie einer zu behandelnden Person beschränkt, ist dabei, genau wie bei der Bildregistrierung, besonders relevant für die Zielstellung dieser Arbeit.

Heizmann stellte im Rahmen eines Vortrages im 45. Heidelberger Bildverarbeitungsforum mögliche Anwendungsfälle von Bildfusionsalgorithmen vor \cite{Heizmann2011}. Zu diesem Zweck klassifiziert er die Bildfusionsalgorithmen nach der Art der Eingabebildinformation, nach der Ebene der Fusion, nach der Art der Zwischenrepräsentation und nach der Art der Parameteroptimierung. Die Information der Eingabebilder liegt entweder in redundanter, teilweise unzuverlässiger Form, in potentiell komplementärer Form, über mehrere Bilder verteilt oder in zueinander orthogonaler Form vor. Die Ebene der Fusion bestimmt, wieviel die Daten vorverarbeitet werden, bevor sie fusioniert werden. Die Fusion kann direkt auf den Pixelwerten, auf den daraus detektierten Merkmalen oder auf dem wiederum darauf abgeleiteten Entscheidungen geschehen. Die Fusion unterscheidet sich außerdem in der Art der verwendeten Zwischenrepräsentation. Ein allgemeines Bildfusionsschema sieht vor, dass die Eingabebilder (auf der entsprechenden Fusionsebene) in eine beliebige Zwischenrepräsentation transformiert werden, dass anschließend die Parameter der Zwischenrepräsentation zusammengeführt werden und dass zuletzt die fusionierte Zwischenrepräsentation wieder in ein Bild rücktransformiert wird. Weiterhin unterscheidet Heizmann in der Art wie die Parameter optimal zusammengeführt werden.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{images/du2016_image_fusion_examples}
    \caption{\label{du2016_image_fusion_examples_fgr}Du \textit{et al.} vergleichen verschiedene medizinische multimodale Bildfusionsalgorithmen. Das Ergebnis einer Bildfusion von einem MRT-Bild (a) und einem PET-Bild (b) ist für unterschiedliche Algorithmen dargestellt (c-h) \cite{Du2016}.}
\end{figure}

Du \textit{et al.} geben eine Übersicht über die multimodale Fusion von medizinischen Bildern \cite{Du2016}. Dabei wurden zuerst fünf Dekompositions- und Rekonstruktionsschemata betrachtet, die jeweils unterschiedliche Zwischenrepräsentation erzeugen. Das erste Schema bildet die Umwandlung von und in spezielle Farbräume. So lässt sich mit z.B. mithilfe der Umwandlung in den HSV-Farbraum eine Bildfusion gestalten, welche die Farben eines Eingangsbildes erhält und welche gleichzeitig die relevanten Helligkeitswerte aus beiden Eingangsbilder übernimmt. Nach der Konvertierung der Eingabebilder in den HSV-Farbraum, wird der Farbtonkanal und der Sättigungskanal aus einem Bild übernommen und nur die Helligkeitsinformation aus beiden Bildern werden mithilfe von Fusionsregeln kombiniert. Zum Schluss muss das fusionierte HSV-Farbraum wieder in den Quellfarbraum umgewandelt werden. Eine weiteres Dekompositions-Rekonstruktions-Schema bieten Bildpyramiden. Ausgehend von Eingabebilder werden die Ebenen der Bildpyramide von unten nach oben generiert, wobei die höhere Ebenen auf der jeweils niedrigeren Ebene basieren und immer kleiner werden. Für diese einzelnen Ebenen können unterschiedliche Fusionsregeln definiert werden, die an die Charakteristika der jeweiligen Ebene angepasst sind. Die fusionierte Ebenen werden dann von oben nach unten in das fusionierte Bild zusammengeführt. Mithilfe von Waveletbasierten Transformationen lässt sich das Bildsignal der Eingabebilder in einen Spektralkoeffizientendarstellung überführen. Diese Koeffizienten beschreiben jeweils unterschiedliche Aspekte der Eingangsbilder (wie z.B. Bildfrequenzen), für die wiederum geeignete Fusionsregeln gewählt werden können. Die fusionierten Spektralkoeffizienten werden, mit einer Rücktransformation wieder in ein Bild umgewandelt. Eine vergleichsweise neue Zwischenrepräsentation bietet die Sparse Representation, die einzelne Bildausschnitte als eine Linearkombination von Einträgen eines übervollständigen Verzeichnisses beschreibt. Einzelne Bildausschnitte werden dazu in eine Vektorform umgewandelt. Der Vektor beschreibt die Linearkoeffizienten mit denen sich der Bildauschnitt aus den jeweiligen Einträgen des Verzeichnisses geniert. Da das Verzeichnis übervoll ist, wird der Vektor so gewählt, dass er möglichst dünn besetzt ist. Die Vektordarstellungen werden dann mit vergleichsweise einfachen Fusionsregeln kombiniert. Die einzelnen Bildausschnitte werden als Linearkombination von den fusionierten Linearkoeffizienten und den dazu gehörigen Verzeichniseinträgen errechnet und in das fusionierte Bild zusammengeführt. Die letzte Klasse bilden die Methoden zur Dekomposition- und Rekonstruktion, die auf salienten Merkmale basieren. Sie basieren auf der Grundlage, dass Helligkeitsunterschiede und Texturinformationen wesentlich salienter für die menschliche Wahrnehmung sind, als die eigentlichen Helligkeitsintensitäten. Diese Methoden haben die Anwendung von verschiedenen Weichzeichnungsfiltern gemeinsam, um mehrere bandpassgefilterte Bilder zu erzeugen, die jeweils die Helligkeitsvariationen eines bestimmten Frequenzbereichs erfassen. Diese Bilder werden mit passenden Fusionsregeln fusioniert und nach dem inversen Konstruktionsschema in das finale fusionierte Bild überführt. Mögliche Ergebnisse von verschiedenen Bildfusionsalgorithmen sind in der Abbildung \ref{du2016_image_fusion_examples_fgr} dargestellt.

Neben der Wahl der Zwischenrepräsentation ist die Wahl der Fusionsregeln von großer Relevanz. In der Arbeit von Lu \textit{et al.} werden Fusionsregeln nach mehreren Eigenschaften unterschieden. Das erste Kriterium ist die Art der Aktivitäts-/Salienzbestimmung, die beschreibt wie die einzeln Koeffizienten der Zwischenrepräsentation gemäß ihrer Salienz oder Aktivität bewertet werden. Die Salienz eines Koeffizienten kann entweder nur von dem Koeffizienten selbst abhängen, sie kann von den Koeffizienten in einem lokalen Fenster mit fester Größe abhängen oder sie kann von einer beliebigen Region an Koeffizienten abhängen. Fusionsregeln unterscheiden sich außerdem in der Koeffizientengruppierung, die beschreibt welche Gruppierung von fusionierten Koeffizienten mit der gleichen Strategie ermittelt werden (z.B. von allen Zwischenrepräsentationslevels oder nach einzelnen Zwischenrepräsentationslevels). Ein weiteres wichtiges Kriterium zur Klassifikation von Fusionsregeln ist die Koeffizientenkombination, die beschreibt welche Funktion verwendet wird, um die korrespondierenden Koeffizienten auf den fusionierten Koeffizienten abzubilden. Mögliche Funktionen sind z.B. die Wahl des Koeffizienten mit der maximalen Salienz, die einfache Mittelung der Koeffizienten oder eine Salienz-gewichtete Mittelung der Koeffizienten. Das letzte Klassifikationsmerkmal von Fusionsregeln, bildet die eingesetzte Konsistenzverifkation, die beschreibt ob und wie dafür sorge getragen wird, dass benachbarte Koeffizienten ähnlich fusioniert werden. In der Übersicht von Lu \textit{et al.} wurden außerdem vier Beispielvertreter für Fusionsregeln genannt. So kann Fuzzylogik genutzt werden, um Regeln auf der Entscheidungs-/Klassifikationsebene aufzustellen. Statistische Methoden wie z.B. PCA können verwendet werden um versteckte, saliente Strukturen zu extrahieren. Die Simulation von Effekten der menschliche Wahrnehmung anhand von Filterketten, kann genutzt werden, um die Salienz von Merkmalen zu bestimmen. Auf die Eingabebilder werden dabei direkt verschiedene Filter angewandt, die der menschlichen Wahrnehmung von optischen Reizen nachempfunden sind. Die Ausgaben der Filter sind dabei so verkettet, dass dabei direkt ein Fusionsergebnis entsteht. Das letzte Beispiel für eine Fusionsregel bildet die Optimierung des Fusionsergebnisses anhand einer Zielfunktion, die auf einer Evaluationsmetrik wie z.B dem Signal-Rausch-Verhältnis beruht.

Daneshvar und Ghassemian setzen die Bildfusion von MRT- und PET-Bildern mit dem HSV-Farbraum und mit einem Fusionsmodell der menschlichen Wahrnehmung um \cite{Daneshvar2010}. Das PET-Bild wird vom RGB-Farbraum in den HSV-Farbraum transformiert und der daraus resultierende Helligkeitskanal des PET-Bilds wird mit dem monochromatischem MRT-Bild fusioniert. Der fusionierte Helligkeitskanal wird mit den Farbton- und Sättigungskanal des PET-Bilds kombiniert und zurück in den RGB-Farbraum transformiert. Die Helligkeitsfusion ist dabei von der Funktionsweise der Netzhaut inspiriert. Das Fusionsresultat ergibt sich durch eine Verkettung von Filteroperationen, welche jeweils die Funktion bestimmter Zellen der Netzhautschichten modellieren.


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/xu2014_input}
        \caption{Eingabebild}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/xu2014_base}
        \caption{Grobes Bild}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/xu2014_detail}
        \caption{Detailbild}
    \end{subfigure}
    \caption{\label{xu2014_represenation_fgr}Xu \textit{et al.} erzeugen in ihrem Bildfusionsansatz grobe Varianten (b) aus jedem Eingabebild (a). Mithilfe eines Subtraktionsschemas wird aus jedem groben Bild und dem zugehörigen Eingabebild ein Detailbild (c) erzeugt, welches die hochfrequenten Bilddetails enthält. Die Detailbilder und das grobe Bilder der beiden Eingabebilder werden unterschiedlich kombiniert und anschließend als fusioniertes Bild rekonstruiert \cite{Xu2014}.}
\end{figure}

Xu stellt einen Bildfusionsalgorithmus vor, der auf einer Minimum-Maximum-Hüllenoperation und der Bewertung salienter Merkmale beruht \cite{Xu2014}. Für die Konstruktion der Zwischenrepräsentation, werden zunächst die lokalen Maxima und Minima für jedes Eingabebild bestimmt. Aus diesen lokalen Extremwertpunkten werden mit der Delaunay-Triangulierung und einem Interpolationsschema Minima- und Maximabilder erstellt. Die Mittelung dieser Bilder erzeugt eine gröbere Variante des Eingabebilds. Je größer die Fenstergröße gewählt wird, mit der die lokalen Extremwerte bestimmt werden, desto geringer ist die maximale Frequenz der Helligkeitsvariationen, die in der groben Bildvariante noch erhalten bleiben. Dieser Fakt wird genutzt, um mit einem Subtraktionsschema und der Variation der Fenstergröße ein grobes Bild und mehrere Detailbilder zu erzeugen. Ein Teil der Dekomposition wird in der Abbildung \ref{xu2014_represenation_fgr} beispielhaft dargestellt. Diese Bilder werden jeweils einzeln fusioniert. Die eingesetzten Fusionsregeln unterscheiden sich in der Salienzbestimmung für das grobe Bild und für die Detailbilder. Das Salienzmaß für das grobe Bild ist die lokale Energie. Das Salienzmaß der Detailbilder entspricht dem lokalen Kontrast zwischen dem entsprechenden Detailbild und dem groben Bild. Ist die Salienz an einem Pixel in dem zu fusionierenden Bild maximal im Vergleich mit allen Eingangsbildern, wird die Intensität an dem Pixel komplett in das fusionierte Bild übernommen. Gibt es mehrere gleiche maximale Salienzen, werden die Pixelintensitäten gemittelt. Gemäß dem vorher angewandten Subtraktionsschema, ergibt sich das fusionierte Bild durch die Aufsummierung des fusionierten groben Bilds mit allen fusionierten Detailbildern. 

Ein effizienter Bildfusionsalgorithmus, der auf geführter Weichzeichnung basiert, wurde von Li \textit{et al.} vorgestellt \cite{Li2013}. Die Zwischenrepräsentation besteht pro Eingabebild aus einem Grundbild und aus einem Detailbild. Das Grundbild ergibt sich dadurch, dass ein lokaler Mittelwertfilter auf das Eingabebild angewandt wird. Das Detailbild ergibt sich aus der Subtraktion des Grundbilds von dem Eingabebild. Die eingesetzten Fusionsregeln unterscheiden sich in der Konsistenzverifikation für das Grundbild und für das Detailbild. Direkt aus den Eingangsbildern werden Salienzkarten mit einer Laplacefilterung und mit einer Mittelwertsfilterung berechnet. Da die Koeffizientenkombination mit einem gewichteten Mittelwert berechnet werden sollen, müssen Gewichtskarten für die Detail- und Grundbilder erstellt werden. Aus den Salienzkarten werden zunächst initiale Gewichtskarten anhand der maximalen lokalen Salienz generiert. Diese Gewichtskarten unterscheiden sich noch nicht für das Detail- und das Grundbild, da sie nur von der Salienz der Eingabebilder abhängen. Die Gewichtskarten werden durch Anwendung einer geführter Weichzeichnungsfilterung mit den Eingabebilder so umgeformt, dass die Fusion konsistent ist. Der Weichzeichnungsgrad unterscheidet sich dabei für das Grund- und das Detailbild, sodass jeweils passende Gewichtskarten entstehen. Um das finale Fusionsergebnis zu erhalten, wird das fusionierte Grundbild auf das fusionierte Detailbild addiert.

Gan \textit{et al.} setzen eine abgewandelte Variante des Bildfusionsalgorithmus von Li \textit{et al.} \cite{Li2013} ein, um Infrarotbilder mit optischen Bildern zu fusionieren \cite{Gan2015}. Dafür wurde eine andere Zwischenrepräsentation und ein anderes Salienzmaß verwendet, um die Fusionsqualität zu verbessern. Die Zwischenrepräsentation beruht dabei auf einem Subtraktionsschema, mit dem ein Grundbild und mehrere Detailbilder erzeugt werden. Ähnlich wie in der Arbeit von Xu \cite{Xu2014}, werden dafür zunächst unterschiedlich grobe Bilder erzeugt. Dazu wird eine kantenerhaltenden Weichzeichnungsoptimierung mit variierendem Weichzeichnungsgrad eingesetzt. Die Salienzwerte werden mit der aufwendiger berechenbaren Phasenkongruenz bestimmt, die mehr sichtbar unterscheidbare Merkmale erfasst als die Laplacefilterung. Die Fusion verläuft ansonsten genau so wie sie von Li \textit{et al.} vorgestellt wurde.

Ma \textit{et al.} stellen eine Alternative für die Fusion von Infrarotbildern und optischen Bildern vor, die auf einer modalitätsspezifischen Optimierung basiert \cite{Ma2016}. Eine Extraktion von salienten Merkmale ist dabei nicht nötig. Laut Ma \textit{et al.} charakterisiert sich die Information der Infrarotbilder durch die Pixelintensitäten und die damit verbundene thermische Energie. Die Texturinformation der optischen Bildern charakterisiert sich hingegen vor allem durch Gradienten, wobei starke Gradienten (Kanten) die Details der Szene darstellen. Deswegen wird eine Zielfunkion definiert, die anhand der Intensitätsunterschiede zwischen dem Infrarotbild und dem fusionierten Bild und anhand der Gradientenunterschiede zwischen dem optischen Bild und dem fusionierten Bild definiert wird. Die Zielfunktion wird so umgestellt, dass sich das Problem mit Algorithmen für die totale Abweichungsminimierung lösen lässt.

Der Bildfusionsalgorithmus der in dieser Arbeit eingesetzt werden soll, muss verschiedene Voraussetzungen erfüllen. Da aufeinander folgende Bildreihen fusioniert werden sollen, muss die verwendete Bildfusion invariant gegenüber Skalierungen, Rotationen und Verschiebungen sein. Außerdem muss die Bildfusion sich so schnell berechnen lassen, dass die resultierenden Bildwiederholraten für die interaktive Nutzung geeignet sind. Zusätzlich sollen alle Fusionsergebnisse frei von Artefakten sein. Pyramidenbasierte Fusionsansätze haben das Problem, dass sie nicht invariant gegenüber Skalierungen sind. Einzelne Koeffizienten beziehen sich, aufgrund der Skalierungsschritte, immer auf größere Pixelblöcke und nicht auf einzelne Pixel. Deswegen befinden sich im fusionierten Bild potentiell Blockartefakte. Waveletbasierte Verfahren sind zwar invariant gegenüber Skalierungen, aber nur ein Teil dieser Verfahren ist invariant gegenüber Rotationen. Diese Verfahren setzen meist nicht separierbare Filter ein, deren Berechnungskomplexität quadratisch von der Größe der verwendeten Faltungsmatrix abhängt. Eine intraoperative Berechnung ist somit nicht effizient genug. Die Bildfusion mit Sparse Representation ist nicht invariant gegenüber Skalierungen, da die Größe der verwendeten Bildausschnitte im übervollen Verzeichnis meist nicht variabel ist. Sind außerdem keine Bilder zum Erlernen des Verzeichnisses vorhanden, müssen die Verzeichnisse statisch erstellt werden, was eine verminderte Fusionsqualität zur Folge hat. Die statistischen Bildfusionsmethoden sind zwar einfach zu implementieren, aber sie liefern selten gute Ergebnisse, da die Koeffizienten oft global gleich kombiniert werden. Außerdem wird keine Zwischenrepräsentation eingesetzt, sodass unterschiedliche Bildcharakteristika, wie z.B. hoch- und niedrigfrequente Bildanteile, nicht unterschiedlich behandelt werden können. Einige der Bildfusionsalgorithmen die auf salienten Merkmalen basieren, erfüllen alle Invarianzkriterien. Außerdem gibt es für viele dieser Algorithmen effiziente Implementierungen. Das liegt daran, dass diese Algorithmen auf Filteroperationen beruhen, die nur linear oder gar nicht von der Größe der verwendeten Faltungsmatrix abhängen. Auswertungen anhand von Bildfusionsmetriken deuten darauf hin, dass die Bildfusionsalgorithmen die auf Sparse Representation oder salienten Merkmalen basieren, oft die visuell besten Ergebnisse erzielen. Unter den vorgestellten Bildfusionsalgorithmen sticht der Ansatz von Li \textit{et al.} \cite{Li2013} hervor. Er ist sehr effizient implementierbar und er scheint, nach den verschiedenen Evaluationsergebnissen, vergleichsweise gute Ergebnisse zu erzielen. Das von Daneshvar und Ghassemian vorgestellte Bildfusionsschema \cite{Daneshvar2010}, lässt sich einsetzen um eine farberhaltende Fusion zu realisieren.

% --------------------------------------------------------------------------- %    
\chapter{Methodische Grundlagen}

Für die Erzeugung der kombinierten Visualisierung von präoperativen MRT- und intraoperativen Thermografiedaten müssen mehrere präoperative und intraoperative Verarbeitungsschritte durchgeführt werden. Am Beginn dieser Pipeline werden präoperativ verschiedene Informationen über die Thermografiekamera und einen daran befestigten Referenzadapter bestimmt. Dafür werden Thermografiedaten eines Kalibrationsmusters und Trackinginformation des Neuronavigationssystems genutzt. Intraoperativ werden die Thermografiedaten auf die Anatomie der zu behandelnden Person registriert. Dazu werden die präoperativ bestimmten Kamerainformationen und intraoperative Trackinginformationen genutzt. Die Relation von den MRT-Daten zu der Anatomie ist bekannt. Aus den Thermografiedaten wird dann in dem Koordinatensystem der Anatomie eine dreidimensionale projektive Visualisierung erzeugt. In dieser Visualisierung werden die Temperaturen auf der bemessenen Gehirnoberfläche dargestellt. Die Gehirnoberfläche wird präoperativ aus den MRT-Daten extrahiert wird. Die MRT-Daten werden selbst als Volumengrafik dargestellt. Die so erzeugten MRT- und Thermografiebilder werden bei jeder Veränderung fusioniert und angezeigt. Der ganze Prozess ist in Abbildung \ref{visualisation_pipeline_fgr} schematisch dargestellt.

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{images/visualisation_pipeline}
    \caption{\label{visualisation_pipeline_fgr}Pipeline für die Erstellung der fusionierten MRT- und Thermografievisualisierung}
\end{figure}

Es wird ein Bildregstrierungsansatz vorgestellt, um die Thermografiedaten auf die zu behandelnde Person zu registrieren. Der Ansatz basiert dabei auf zwei präoperativen Schritten, einer Kamerakalibrierung und einer Kameralagebestimmung. Zur Bestimmung der intraoperativen Pose der Kamera, wird prä- und intraoperatives ein Referenzadapter getrackt, der statisch an der Kamera befestigt ist. Das erlaubt eine effiziente Bildregistrierung durchzuführen, die bei jeder Änderung der Kamerapose aktualisiert wird. Dazu wird eine projektive Transformation zwischen dem Thermografiebildraum und dem Anatomieobjektraum bestimmt. Diese Transformation kann eine perfekte Übereinstimmung erzeugen, wenn sich das Gehirn der zu behandelnden Person wie ein Festkörper verhält. Das ist allerdings nur bedingt der Fall, da sich das Gehirn vor und während der Operation verformt. Letteboer \textit{et al.} untersuchten dieses als Brain Shift bezeichnete Phänomen mithilfe von 3D-Ultraschall-Bildgebung \cite{Letteboer2005}. In dieser Arbeit wurde beobachtet, das der Unterschied zwischen dem prä- und intraoperativen Gehirnzustand quantifizierbar groß ist. Der durchschnittliche Fehler betrug dabei parallel zur Gravitation \SI{3,0}{mm} und senkrecht zur Gravitation betrug er \SI{3,9}{mm}. Der Unterschied wird noch leicht größer, wenn die Schädeldecke und die Dura (die äußerste Hirnhaut) während der Operation geöffnet wird. In der Arbeit lag der durschnittliche Fehler nach der Öffnung parallel zur Graviation bei \SI{0,2}{mm} und senkrecht zur Gravitation lag er bei \SI{1,4}{mm}. Der vorgestellte Bildregistrierungsansatz kann diesen Fehler aus Effizienzgründen nicht ausgleichen. Soll diese Fehlerquelle dennoch kompensiert werden, könnte der vorgestellte Ansatz zur Initialisierung einer verformungsbasierten Bildregistrierung verwendet werden. Wird das fehlende Festkörperverhalten vernachlässigt, hängt die Genauigkeit des vorgestellten Ansatzes überwiegend von extern kontrollierbaren Parametern ab. Der Inhalt der MRT- und Thermografiedaten ist dabei für Bildregistrierung komplett irrelevant. Dadurch lässt sich der Ansatz für beliebige Eingriffe verwenden, solange der durch Brain Shift erzeugte Fehler nicht zu groß wird.

Neben der Bildregistrierung wird ein projektiver Visualisierungsansatz für die Thermografiedaten vorgestellt. Das dafür notwendige Renderingprinzip ähnelt stark dem shaderbasierten Ansatz, der von Dey \textit{et al.} \cite{Dey2002} beschrieben wurde. Diese Visualisierung benötigt außerdem eine Oberflächengeometrie, auf der die Thermografiedaten projektiv dargestellt werden können. Zu diesem Zweck wird aus den MRT-Daten die komplette Gehirnoberfläche rekonstruiert. Diese Oberflächengeometrie enthält dabei auch Flächenelemente die nicht von der Thermographiekamera aufgenommen werden. Der vorgestellte Visualisierungsansatz färbt dabei nur die Teile der Oberfläche mit einer Pseudofarbdarstellung der Thermografiedaten ein, die von der Thermografiekamera sichtbar sind. Alle Elemente der Oberfläche für die keine Thermografiedaten vorliegen werden verworfen.


Zuletzt wird ein Bildfusionsansatz vorgestellt, der die Helligkeitsinformationen der MRT-Bilder mit denen der Thermografiebilder kombiniert und der die Farbinformation komplett aus den Thermografiebildern übernimmt. Die Trennung von Farb- und Helligkeitsinformation erfolgt dabei anhand einer Transformation vom dem RGB-Farbraum in den HSV-Farbraum. Dabei kommt der von Li \textit{et al.} \cite{Li2013} vorgestellte Bildfusionsalgorithmus zum Einsatz. Der Algorithmus ist weitest gehend invariant gegenüber Skalierungen, Rotationen und Verschiebungen, wenn die Fenstergrößen der angewandten Filter passend zur Auflösung des Bildgegenstands gewählt werden. Die Fusionsqualität ist im Vergleich mit der angepassten Variante von Gan \textit{et al.} \cite{Gan2015} zwar etwas schlechter, aber der benötigten Rechenaufwand ist dafür weitaus geringer. Für den von Gan \textit{et al.} vorgestellten Ansatz muss mehrfach ein von Farman \textit{et al.} vorgestellter, nichtlinearer kanten-erhaltender Weichzeichnungsfilter \cite{Farbman2008} angewandt werden. Die einmalige Anwendung diese Filters benötigte mit einem \SI{2.2}{GHz} Intel Core 2 Duo \SI{3,5}{s} Rechendauer pro Megapixel. Der von Li \textit{et al.} vorgestellten Algorithmus benötigte für die komplette Berechnung des finalen Ergebnisses mit einer ähnlichen CPU nur \SI{0,04}{s} pro \num{0,25} Megapixel.

    \section{Magnetresonanztomographie}

Vor einem neurochirurgischen Eingriff wird von der Kopfregion der zu behandelnden Person ein dreidimensionales Bild aufgenommen. Dafür wird die Magnetresonanztomografie als bildgebendes Verfahren eingesetzt. Die nachfolgende Erklärung orientiert sich am Lehrbuch \textit{Medizinische Bildgebung}, das von Dössel und Buzug herausgegeben wurde \cite{Doessel2014}. Die Bestimmung der gemessenen Merkmalswerte basiert dabei auf dem Kernspinresonanzeffekt. Die Wasserstoffatomkerne, die sich in unterschiedlichen Verteilungen in dem zu untersuchenden Gewebe befinden, haben einen Eigendrehimpuls/Kernspin. Sie lassen sich deswegen als magnetische Dipole betrachten. Im Normalfall sind die Orientierungen der Drehimpulse gleichmäßig in alle Richtungen verteilt, sodass sich die einzelnen Magnetfelder der Dipole auslöschen. Die Stärke der gesamten Magnetisierung beträgt daher im Normalfall 0. Wird ein starkes statisches Magnetfeld im Bereich der Wasserstoffkerne angelegt, richtet sich die Mehrheit der Drehimpulse in Richtung des Magnetfelds, in einem niedrig energetischen Zustand aus. Eine Minderheit der Drehimpulse richtet sich parallel und entgegen der Richtung des Magnetfeld, in einem hohen energetischen Zustand aus.  Da sich so mehr Dipole in die positive Richtung des Magnetfelds ausrichten, entsteht beim Anlegen des statischen Magnetfelds insgesamt eine Longitudinalmagnetisierung in Richtung des Magnetfelds. Wegen der Drehimpulserhaltung formen die Drehimpulse eine Präzessionsbewegung mit der Lamorfrequenz, die abhängig von der angelegten Feldstärke ist. Weil die Longitudinalmagnetisierung nicht messbar ist, wird zusätzlich zu dem statischen Magnetfeld ein hochfrequentes Wechselfeld angelegt, welches mit der Lamorfrequenz resoniert. Dadurch werden die Drehimpulse phasensynchron und einige der Drehimpulse mit niedrigem energetischen Zustand wechseln in einen höheren energetischen Zustand. Die so entstanden Balance zwischen den Drehimpulsen im hohen und niedrig energetischen Zustand lässt die Longitudinalmagnetisierung verschwinden. Die Phasensynchronität erzeugt stattdessen eine Transversalmagnetsierung, welche sich senkrecht zur statischen Magnetfeldrichtung dreht und welche eine messbare Wechselspannung in einer umliegenden Spule induziert. Die Amplitude der induzierten Wechselspannung ist dabei proportional zur Stärke der Transversalmagnetsierung. Wird das hochfrequente Wechselfeld abgeschalten, wechseln die Drehimpulse in den vorherigen Zustand zurück. Die Spin-Gitter-Relaxionszeit $T_1$ beschreibt dabei die Zeit, die ab dem Abschalten benötigt wird, bis die Drehimpulse wieder überwiegend in den niedrig energetischen Zustand gewechselt sind. Genauer definiert ist es die Zeitdauer, in der die Longitudinalmagnetisierung um ca. 63\% wiederhergestellt ist. Die Spin-Spin-Relaxionszeit $T_2$ beschreibt die Zeit, die ab dem Abschalten benötigt wird, bis die Drehimpulse nicht mehr phasensynchron sind. Sie ist als die Zeitdauer definiert, in der die Transversalmagnetisierung um ca. 63\% verschwunden ist. Diese $T_1$- und $T_2$-Zeiten unterscheiden sich je nach Gewebetyp. Wenn die Zeitabstände variiert werden, in denen das Wechselfeld an- und ausgeschaltet wird, lassen sich entweder die $T_1$- oder die $T_2$-Zeiten aus der induzierten Wechselspannung ermitteln. Ein Magnetresonanztomograph beinhalten einen primären Magnet zur Erzeugung des statischen Wechselfelds, einen Gradientenmagneten zur genauen dreidimensionalen Einordnung der abgefassten induzierten Spannung und mehrere Radiofrequenzspulen zum Erzeugen des Wechselfelds und zum Abnehmen der induzierten Wechselspannung. Mit diesen Komponenten lassen sich aus den Werten der $T_1$- oder $T_2$-Zeiten 2D-Schnittbilder erstellen. Dabei werden mehrere Schnittbilder entlang einer Achse aufgenommen und zu einem 3D-Gitter zusammengefasst.

Im Bereichs des Neuroimagings ist ein Anwendungszweck der Magnetresonanztomographie die Untersuchung von Schnittebenen durch das Volumen. Werden die $T_1$-/$T_2$-Werte linear mit Grauwerten gleichgesetzt, ergibt sich eine einfache Viusalisierungsmethode für solche Schnittbilder. In einem $T_1$-gewichteten Bild ist so z.B. die Gehirn-Rückenmarks-Flüssigkeit dunkel und fetthaltiges Gewebe, wie z.B. das Knochenmark oder die weiße Substanz, ist hell. In einem $T_2$-gewichteten Bild ist die Gehirn-Rückenmarks-Flüssigkeit hingegen hell und das fetthaltiges Gewebe ist dunkler als in einem analogen $T_1$-gewichteten Bild. Weil die charakteristische $T_1$- und $T_2$-Werte für verschiedene Materialien bekannt sind, lassen sich Rückschlüsse auf die Beschaffenheit der dargestellten Orte ziehen. Wenn die Schnittebene interaktiv veränderbar sind, kann der Bildgegenstand explorativ auf anatomische Anomalien untersucht werden. Die Magnetresonanztomografie kann auch für die funktionelle Bildgebung (als fMRT) eingesetzt werden. Die $T_2$-Relaxionszeit steht nämlich in einem indirekten Zusammenhang mit der Konzentration von deoxygeniertem Hämoglobin. Dieser Zusammenhang lässt sich ausnutzen, um die Gehirnaktivität zu bestimmen, da eine erhöht Menge von aktivierte Nervenzellen einen zeitlich versetzten Zufluss von oxygeniertem Blut bewirkt (die Hämodynamische Reaktion). Die Magnetresonanztomografie kann auch verwendet werden, um indirekte strukturelle Merkmale zu extrahieren. Ein Anwendungsbeispiel dafür ist die Diffusions-Tensor-Bildgebung, welche die Diffusionsbewegung von Wassermolekülen im Körpergewebe misst. Dabei wird pro Voxel ein Tensor bestimmt, der das dreidimensionale Diffusionsverhalten beschreibt. Aus diesen Tensordaten lässt sich, z.B. mithilfe der Traktografie, der Verlauf von größeren Nervenfaserbündeln rekonstruieren und darstellen.

    \section{Thermografie}

Während einem neurochirurgischen Eingriff wird von der freigelegten Gehirnoberfläche der zu behandelnden Person ein zweidimensionales Bild aufgenommen. Dafür wird die Thermografie als bildgebendes Verfahren eingesetzt. Die Bestimmung der gemessenen Merkmalswerte basiert dabei auf der Emission von Wärmestrahlung. Alle Körper, die eine Temperatur besitzen, welche ungleich \SI{0}{K} ist, emittieren ein elektromagnetisches Strahlung aus einem charakteristischen Spektrum. Je größer die Temperatur, desto kürzer wird dabei die mittlere Wellenlänge der emittierten Strahlung. Zur Bestimmung der Temperatur, muss die Intensität dieser emittierten Strahlung gemessen werden. Die Temperaturmessung erfolgt dabei in verschiedenen Teilbereichen der Infrarotstrahlung, um den Effekt der atmosphärische Gegenstrahlung zu unterdrücken. Die Art der Infrarotstrahlungsmessung wird dabei anhand des verwendeten Spektrums klassifiziert. Den Bereich mit den kürzesten Wellenlängen bildet das nahe Infrarot (NIR), in dem elektromagnetische Strahlung mit Wellenlänge von \SIrange{0,75}{1,4}{\micro\meter} gemessen wird. Darauf folgt der kurzwellige Infrarotbereich (SWIR) mit Wellenlängen von \SIrange{1,4}{3}{\micro\meter} , der mittlere (MWIR, IIR) mit Wellenlängen von \SIrange{3}{8}{\micro\meter}, der langwellige Infrarotbereich (LWIR) mit Wellenlängen von \SIrange{8}{15}{\micro\meter}  und der ferne Infrarotbereich (FIR) mit Wellenlänge von \SIrange{15}{1000}{\micro\meter}. Diese Bereiche sind als Teil des kompletten elektromagnetischen Strahlenspektrums in der Abbildung \ref{thermography_infrared_spectrum_fgr} dargestellt. Zur Bildgebung werden Kameras eingesetzt, die in ihrere Funktionsweise optischen Kameras ähneln. Durch Objektive aus einem speziellen Material, wie z.B. einkristallinem Germanium für den FIR-Bereich, wird die einfallende elektromagnetische Strahlung fokussiert und auf ein Sensorarray gelenkt. Der Aufbau der Sensoren variiert je nach dem gewünschten Messbereich. Es werden z.B. tief gekühlte Fotohalbleiter, Mikrobolometer oder pyroelektrischen Sensoren eingesetzt. Damit aus den Sensorwerten Temperaturen errechnet werden können, muss eine Thermografiekamera i.d.R. kalibriert werden.

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{images/spectrum}
    \caption{\label{thermography_infrared_spectrum_fgr}Die für die Thermografie relevanten Bereiche des elektromagnetischen Infrarotspektrums. Quelle: Wikimedia Commons - Elektromagnetisches Spektrum \cite{WMCSpektrum2014}}
\end{figure}

Im Bereich des Neuroimagings ist ein Anwendungsbeispiel der Thermografie, die funktionale Bildgebung der neuronalen Aktivität von verschiedenen Gehirnarealen. Es gibt dabei mehrere messbare thermische Reaktionen auf die neuronale Aktivität. So wird z.B. bei einer erhöhten Aktivität durch die Aktionspotentiale mehr thermischen Energie an das umgebende Gewebe freigesetzt. Der daraus resultierende Temperaturunterschied kann im Vergleich mit einer Grundtemperatur, welche aus mehreren Thermografieaufnahmen bestimmbar ist, gemessen werden. Wie beim fMRI lässt sich auch die hämodynamischen Reaktion ausnutzen, um die neuronale Aktivität zu bestimmen.  Das Blut, welches bei einer erhöhten Aktivität vom Körperinneren über die Arterien an die Gehirnoberfläche transportiert wird, ist wärmer als das Gewebe an der Gehirnoberfläche, welches der umgebenden Luft ausgesetzt ist. Die Reaktionen erfolgen dabei unterschiedlich schnell. Die intrinsische Erwärmung erfolgt deutlich schneller, als die Erwärmung, die durch die hämodynamische Reaktion hervorgerufen wird. Die Bildgebung der Durchblutung lässt sich für weitere Zwecke einsetzen. Nach dem gleichen Prinzip können ischämische Areale, bei denen die Durchblutung gestört ist, abgegrenzt werden. Und mit der zusätzlichen Einbringung von thermischen Tracern lassen sich Aneurysmen, Hirntumore und vaskulären Malformationen anhand ihres dynamischen Verhaltens untersuchen. Die Forschung zum Einsatz für Thermografie im Bereich des Neuroimagings, ist aber noch weniger fortgeschritten, als die Forschung an entsprechenden Einsatzgebieten der Magnetresonanztomografie.

    \section{Registrierung der MRT- und Thermografiedaten}

Die Thermografiedaten sollen mit dem Prinzip der Rückprojektion auf der Oberfläche dargestellt werden, die von der Thermografiekamera aufgenommen wird. Dafür ist eine Zuordnung zwischen den 2D-Punkten im Thermografiebildkoordinatensystem und den 3D-Oberflächenpunkten im Anatomiekoordinatensystem notwendig. Das Anatomiekoordinatensystem orientiert sich an den Lagebezeichnung für den menschlichen Körper und es ist eindeutig durch die MRT-Metadaten (Auflösung, Dimensionen, ...) definiert. Diese Zuordnung muss den Abbildungseigenschaften der Thermografiekamera folgen. Unter der (stark vereinfachten) Betrachtung der Strahlenoptik und unter der Annahme einer unendlich kleinen Blendenöffnung entspricht diese Zuordnung einer nicht injektiven Abbildung vom anatomischen Objektraum auf den Thermografiebildraum. Dabei wird jeder beobachtbare Oberflächenpunkt, über das Linsensystem und den hypothetischen Blendenöffnungspunkt der Thermografiekamera, genau in einem Bildpunkt dargestellt. Ein aufgenommener Bildpunkt zeigt einen beliebig entfernten Oberflächenpunkt, welcher auf dem zugehörigen Sichtstrahl durch den  Blendenöffnungspunkt liegt. Die Bildregistrierung beschreibt in diesem Zusammenhang den Prozess, ein Abbildungsmodell für einen Szenenzustand mit einer fixen Kamerapose, mit einem festen Kamerazustand und mit einem konkreten Anatomiezustand zu bestimmen. Dazu müssen die Projektionseigenschaften der Bildgebung modelliert werden, welche bei der Thermografiekamera durch das verwendeten Objektiv und durch die interne Lage des Sensorenarrays charakterisiert werden. Außerdem muss die Lage und die Orientierung der Kamera bezüglich der Anatomie / den MRT-Daten bestimmt werden. Wird dieses Abbildungsmodells auf einen Anatomiepunkt aus den MRT-Daten angewandt, ergibt sich ein zugehöriger Punkt in den Thermografiedaten.

Damit die Parameter des Abbildungsmodells leicht bestimmbar sind, wird die gesuchte Abbildung als eine ideale Lochkamera modelliert. Die nachfolgende Erklärung dieses Kameraabbildungsmodells orientiert sich an dem Buch \textit{Multiple View Geometry in Computer Vision}, welches von Hartley und Zisserman herausgegeben wurde \cite{Hartley2004}. Das Model beschreibt eine perspektivische Transformation, die einen dreidimensionalen Objektpunkt $\underline{P}$ auf einen zweidimensionalen Bildpunkt $\underline{p}$ abbildet:
\begin{align}
    \widetilde{p}\,(\underline{P}; R, \vec{t}, \underline{c}, f, m_x, m_y) &= C_I^K T_K^A \widetilde{P} = C_I^K \begin{pmatrix}
        R & \vec{t} \\
        0 & 1
    \end{pmatrix} \widetilde{P} \nonumber \\
    \Leftrightarrow
     \begin{pmatrix}
        \widetilde{p}_x \\
        \widetilde{p}_y \\
        \widetilde{p}_z
    \end{pmatrix} &=
    \begin{pmatrix}
        \alpha_x & 0 & \underline{c}_x & 0\\
        0 & \alpha_y & \underline{c}_y & 0\\
        0 & 0 & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
        r_{11} & r_{12} & r_{13} & \vec{t}_x \\
        r_{21} & r_{22} & r_{23} & \vec{t}_y \\
        r_{31} & r_{32} & r_{33} & \vec{t}_z \\
        0      & 0      & 0      & 1
    \end{pmatrix}
    \begin{pmatrix}
        \underline{P}_x \\
        \underline{P}_y \\
        \underline{P}_z \\
        1
     \end{pmatrix} \nonumber \\
    \text{wobei } \alpha_x &= f * m_ x \nonumber \\
    \alpha_y &= f * m_y \label{projection_without_distortion_eq}
\end{align}

$\widetilde{P}$ und $\widetilde{p}$ sind hierbei die homogenen Koordinatendarstellungen des Objektpunkts $\underline{P}$ und des Bildpunkts $\underline{p}$. Zunächst wird in der Abbildungsgleichung \ref{projection_without_distortion_eq} die Transformationsmatrix $T_K^A$ auf den Objektpunkt $\widetilde{P}$ angewandt, um die homogenen Objektraumkoordinaten in das Kamerakoordinatensystem zu überführen. Diese Transformation wird anhand einer Rotationsmatrix $R$ und anhand eines Translationsvektor $\vec{t}$ definiert, welche die Lage und die Orientierung der Kamera relativ zum Objektraum beschreiben. Die Kameramatrix $C_I^K$ projiziert darauf die homogene Kamerakoordinaten auf die projektive Bildebene. Die Kameramatrix wird dabei durch den Kamerahauptpunkt $\underline{c} = \left(\underline{c}_x, \underline{c}_y\right)^T$ und durch die achsenspezifischen Brennweiten $\alpha_x, \alpha_y$, die in Pixeln angegeben werden, definiert. Die achsenspezifischen Brennweiten stehen mit der horizontalen Pixelauflösung $m_x$ und der vertikalen Pixelauflösung $m_y$ in Relation zu der tatsächlichen Brennweite $f$, welche in der Längeneinheit des Objektraums angegeben wird. $f$, $\underline{c}$, $m_x$ und $m_y$ sind die intrinsischen Kameraparameter, die nur von dem verwendeten Kamerasystem (Sensor, Linsensystem und Objektiv) abhängig sind. $R$ und $\vec{t}$ beschreiben die extrinsische Kameraparameter, die nur von der Position und Orientierung der Kamera in der Szene abhängig sind. Das Lochkameramodell ist in der Abbildung \ref{pinhole_camera_model_fgr} dargestellt.

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{images/pinhole_camera_model}
    \caption{\label{pinhole_camera_model_fgr}Das verwendete Lochkameramodell bildet einen Objektpunkt $P$ per Zentralprojektion auf die Bildebene ab. Die Kameraposition $\mathcal{F}_c$ bildet dabei das Projektionszentrum. Quelle: OpenCV Dokumentation - \textit{Camera Calibration and 3D Reconstruction} \cite{OCVCalib3D2017} }
\end{figure}

Reale Linsen weisen Verzerrungen auf, die zusätzlich modelliert werden muss. Da sich diese Verzerrungseffekte i.d.R. nicht linear verhalten, wird aus der Kameraprojektionsmatrix eine Projektionsabbildung:
\begin{align}
    \underline{p} = R \underline{P} + \vec{t}        
\end{align}
\begin{align}
    \begin{pmatrix}
        x' \\
        y'
    \end{pmatrix} =
    \begin{pmatrix}
        x / z \\
        y / z
    \end{pmatrix}
\end{align}
\begin{align}
    \begin{pmatrix}
        x'' \\
        y''
    \end{pmatrix} &=
    \begin{pmatrix}
        x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2 (r^2 + 2 x'^2)\\
        y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2y'^2) + 2 p_2 x' y'
    \end{pmatrix} \nonumber \\
    \text{wobei } r^2 &= x'^2 + y'^2 \label{distortion_eq}
\end{align}
\begin{align}
    \begin{pmatrix}
        u \\
        v
    \end{pmatrix} =
    \begin{pmatrix}
        \alpha_x x'' + \underline{c}_x \\
        \alpha_y y'' + \underline{c}_y
    \end{pmatrix}
\end{align}
Die verzerrten Bildkoordinaten $\left(x'', y''\right)^T$ in der Gleichung \ref{distortion_eq} ergeben sich aus der Anwendung von einem radialen und von einem tangentialen Verzerrungsmodell. $k_1$ bis $k_6$ sind die Koeffizienten eines radialen Verzerrungsmodell. $p_1$ und $p_2$ sind die Parameter eines tangentialen Verzerrungsmodells. Die Stärke der jeweiligen Verzerrungseffekte hängt dabei von dem Abstand zur Bildebenenmitte ($\sqrt{r}$) ab. Beispiele für die Auswirkung einer radialen Verzerrung sind in der Abbildung \ref{radial_distortion_fgr} dargestellt. Mögliche Unschärfeeffekte werden von diesem Modell komplett vernachlässigt. Die ist allerdings kein Nachteil für die Echtzeitfähigkeit und die Erkennbarkeit der Temperaturwerte in einer Visualisierung. Außerdem sind die korrespondierenden Bildkoordinaten nicht wie in der Realität diskret, sondern sie sind kontinuierlich.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{images/radial_distortion_examples}
    \caption{\label{radial_distortion_fgr}Beispiele für radiale Verzerrungseffekte. Quelle: OpenCV Dokumentation - \textit{Camera Calibration and 3D Reconstruction} \cite{OCVCalib3D2017}}
\end{figure}

Die Bestimmung der intrinsischen Kameraparameter wird als Kamerakalibrierung bezeichnet. Zur Berechnung der Parameter werden dafür Korrespondenzen zwischen Punkten in einem arbiträrem Objektkoordinatensystem und den zugehörigen projizierten Punkten im Bildkoordinatensystem genutzt. Die dafür nötigen Punktpaare können mithilfe eines Kalibrationsmusters bestimmt werden. Dieses Muster sollte gut detektierbare Merkmale in der entsprechenden Bildmodalität besitzen und die Geometrie dieser Merkmalspunkte sollte bekannt sein. Von diesem Kalibrationsmuster können dann Bilder aufgenommen werden, in denen die Merkmalspunkte detektiert werden können. Die in den Bildern sichtbaren Objektpunkte können in einem eigens definierten Koordinatensystem anhand des Vorwissens über die Merkmalsgeometrie spezifiziert werden. Die Korrespondenzen lassen sich z.B. anhand von Grundwissen über die relative Ausrichtung des Muster und der Kamera bestimmen. Sind die Punktpaare bekannt, werden die intrinsischen Parameter anhand einer Zielfunktion gesucht. Zunächst werden die intrinsischen Parameter initialisiert. Der initiale Hauptpunkt $\underline{c}$ liegt in der Bildmitte, die initiale Brennweite $f$ wird mit einer Fluchtpunktschätzung berechnet und die initalen Verzerrungskoeffizienten werden auf 0 gesetzt. Mit den initialen intrinsischen Parametern werden die extrinsischen Kameraparameter bestimmt. Danach werden die intrinsischen Parameter iterativ anhand einer Zielfunktion optimiert. Dazu wird der Rückprojektionsfehler $e$ verwendet, der den euklidischen Abstand zwischen einem projizierten/geschätzten Bildpunkt $\hat{\underline{p}}$ und einem detektierten/realen Bildpunkt $\underline{p}$ beschreibt. Ein geschätzter Bildpunkt $\hat{\underline{p}}$ wird anhand seines Objektpunkt $\underline{P}$, mit der Kameraabbildung und den aktuellen intrinsischen Parameter berechnet.
Die Zielfunktion ist als die Summe der quadratischen Rückprojektionsfehler definiert. Damit ergibt sich folgendes Optimierungsproblem:
\begin{align}
    \min_{f, \underline{c}, k_1 \dots k_6, p_1, p_2} \sum_{i=1}^{n} \|\hat{\underline{p}}\,(\underline{P}; R, \vec{t}, \underline{c}, f, m_x, m_y, k_1 \dots k_6, p_1, p_2) - \underline{p}_i\|_2^2 \label{reprojection_error_minimisation_eq}
\end{align}
Die Parameter werden dabei mit dem Levenberg-Marquardt Optimierungsalgorithmus gesucht. Damit die Optimierung dabei nicht im einem lokalen Optimum konvergiert, müssen möglichst viele, unterschiedlich verteilte Korrespondenzen ermittelt werden. Deswegen werden meist mehrere Aufnahmen des Kalibrationsmuster aus unterschiedlichen Posen angefertigt. Das quadratisches Mittel des Rückprojektionsfehlers $e_{rms} $ bildet dabei ein ausreißersensitives Gütemaß, das beschreibt, wie gut ein final bestimmtes Abbildungsmodell die vorliegenden Daten erklärt:
\begin{align}
e_{rms} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \|\hat{p}\,(\underline{P}; R, \vec{t}, \underline{c}, f, m_x, m_y, k_1 \dots k_6, p_1, p_2) - \underline{p}_i\|_2}
\end{align}

Die Bestimmung der extrinsischen Parameter wird als Kameralagebestimmung bezeichnet. Zur Berechnung der Parameter werden ebenfalls Korrespondenzen zwischen Bild- und Objektpunkten genutzt. Allerdings müssen die Objektpunkte in einem Objektkoordinatensystem definiert sein, in dem  die Positionierung der Kamera bestimmt werden soll. Dieses Koordinatensystem muss dem Objektkoordinatensystem der Kamerakalibrierung bezüglich der verwendeten Einheit und bezüglich der zugrunde liegenden Händigkeit gleichen. Die Objektpunkte sind in dieser Arbeit in dem Anatomiekoordinatensystem definiert. Neben den Bild- und Objektpunktpaaren werden die intrinsischen Parameter benötigt. Die Parameter werden anhand der gleichen Zielfunktion wie in Gleichung \ref{reprojection_error_minimisation_eq} und mit dem selben Optimierungsalgorithmus gesucht. Zu Beginn werden die extrinsischen Parameter mit der Identitätstransformation initialisiert.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/coordinate_systems}
    \caption{\label{coordinate_systems_fgr}Übersicht aller verwendeter Koordinatensysteme und Transformationen}
\end{figure}

Da die Kamerapose intraoperativ variabel ist, wäre es sinnvoll das Ergebnis der Kameralagebestimmung mit leicht ermittelbaren Zusatzinformationen wiederverwenden zu können. Für diesen Zweck lässt sich ein Trackingsystem und ein darin trackbarer Referenzadapter nutzen. Das Trackingsystem muss dazu kontinuierlich eine Transformation ${T_A^R}$ von dem Referenzadapterkoordinatensystem in das Anatomiekoordinatensystem bestimmen. Das Referenzadapterkoordinatensystem, ist durch die Achsen und den Mittelpunkt des Referenzadapters definiert. Wenn der Referenzadapter statisch an der Kamera befestigt wird, gibt es eine konstante Transformation $T_K^R$ vom Referenzadapterkoordinatensystem in das Kamerakoordinatensystem. Diese Transformation lässt sich berechnen, wenn gleichzeitig die intrinsischen Kameraparameter und die Trackinginformationen für eine Kamerapose vorliegen:

\begin{align}
    T_K^R T_R^A &= T_K^A \nonumber \\
    T_K^R &= T_K^A \left(T_R^A\right)^{-1} \nonumber \\
    T_K^R &= T_K^A T_A^R \label{reference_transform_eq}
\end{align}

Die Kameratransformation vom Anatomiekoordinatensystem in das Kamerakoordinatensystem $T_K^A$, ist für die verwendete Pose durch die extrinsischen Parameter gegeben. Während der Operation lässt sich diese Kameratransformation für einen Zeitpunkt $t$ aus der getrackten Referenzadaptertransformation ${T_A^R}^{(t)}$ und der berechneten, konstanten Transformation $T_K^R$ berechnen:
\begin{align}
    {T_K^A}^{(t)} ({T_A^R}^{(t)}) = T_K^R \left({T_A^R}^{(t)}\right)^{-1} \label{camera_tracking_eq}
\end{align}
Dieser Transformation beschreibt die aktuellen extrinsischen Kameraparameter. In der Abbildung \ref{coordinate_systems_fgr} werden die Relationen zwischen allen verwendeten Koordinatensystemen und Transformationen zusammengefasst.
    
    \section{Visualisierung der Thermografiedaten}

Damit eine Thermografievisualisierung mit der MRT-Visualisierung kombiniert werden kann und damit sie fehlerfrei ausgewertet werden kann, müssen mehrere Anforderung erfüllt sein. Jeder Punkt auf der visualisierten Gehirnoberfläche soll die Temperatur darstellen, die aus dem Thermografiedaten mithilfe der Kameraabbildung bestimmbar ist. Außerdem sollen alle Punkte der Gehirnoberfläche, für die keine Temperaturwerte vorliegen, nicht dargestellt werden. Genau so sollen alle Temperaturwerte, die nicht von der Gehirnoberfläche stammen, nicht dargestellt werden. Die gemessenen Temperaturen sollen für den Anwendungszweck in Pseudofarbe dargestellt werden.

Die Grundidee der eingesetzten Visualisierung ist es, ein Dreiecksnetz der Gehirnoberfläche darzustellen, dass mit den rückprojizierten Thermografiedaten texturiert wird. Für die Visualisierung gilt das Dreiecksnetz als gegeben. Der Renderingansatz wird dabei mithilfe eines Vertexshaders und mithilfe eines Fragmentshaders formuliert. Die Knotenpositionen des Dreiecksnetzes werden dafür von dem Vertexshader interpoliert und an den Fragmentshader weitergegeben. Im Fragmentshader werden für jedes Fragment die zugehörige Texturkoordinaten der Thermografietextur mit der ermittelten Kameraabbildung errechnet. Dann wird der Temperaturwert aus der Textur ermittelt und mit einer vorher gewählten Farbkodierung in einen Farbwerte konvertiert. Ähnlich wie bei dem Ansatz von Dey \textit{et al.} \cite{Dey2002} werden dabei ungültige Fragmente verworfen. Das betrifft die Fragmente, deren projizierte Positionen außerhalb der Bildebene liegen und die Fragmente, die in der Thermografiekameraansicht von anderen Flächen verdeckt werden. Der Algorithmus \ref{thermo_vis_fragment_code} zeigt den verwendeten Fragmentshader als Pseudocode.

\begin{algorithm}
    \caption{Bestimmung der Fragmentfarbe}\label{thermo_vis_fragment_code}
    \begin{algorithmic}[1]
        \REQUIRE $\underline{P}, C, T_K^A, k_1 \dots k_6, p_1, p_2, I_t, I_z, z_{near}, z_{far}, \dddot{L}$
        \ENSURE $\dddot{c}$
        \STATE $u, v, z_{curr} \leftarrow \text{projectPoint}(\underline{P}, C, T_K^A, k_1 \dots k_6, p_1, p_2)$
        \IF {$u < 0$ \OR $u > 1$ \OR $v < 0$ \OR $v > 1$} \RETURN \ENDIF
        \STATE $b \leftarrow \text{sampleImage}(I_z, u, v)$
        \STATE $z_{buff} \leftarrow \text{calculateZFromBufferValue}(b, z_{near}, z_{far})$
        \IF {$z_{curr} > z_{buff} + \varepsilon$} \RETURN \ENDIF
        \STATE $t \leftarrow \text{sampleImage}(I_t, u, v)$
        \STATE $\dddot{c} \leftarrow \text{lookupPseudoColor}(t, \dddot{L})$
    \end{algorithmic}
\end{algorithm}

Bevor die Farbe der Fragmente bestimmt werden kann, wird in einem vorherigen Renderingschritt ein z-Buffer der Oberflächengeometrie aus der Sicht der Thermografiekamera gerendert. Zur Bestimmung der Fragmentfarbe wird  die Kameraprojektionsmatrix $C$, die Kameratransformationsmatrix $T_K^A$, die Verzerrungskoeffizienten $k_1 \dots k_6, p_1, p_2$ die Thermografiedaten $I_t$, der z-Buffer $I_z$, die Near-/Farclippingdistanz $z_{near}$/$z_{far}$ der Thermografiekameraansicht, eine Pseudofarbtabelle $\dddot{L}$ und die Fragmentposition $\underline{P}$ im Anatomiekoordinatensystem benötigt. Zuerst wird aus der Fragmentposition mithilfe der Kameraabildung die Texturkoordinaten $u$, $v$ im Texturkoordinatensystem und die z-Koordinate $z_{curr}$ der Fragmentposition im Kamerakoordinatensystem bestimmt. Liegen die Texturkoordinaten außerhalb der Temperaturtextur, liegen keine Temperaturwerte für das Fragment vor und es wird verworfen. Darauf wird der Wert des z-Buffers $b$ an den Texturkoordinaten bestimmt. Dieser Wert wird mithilfe der Near- und Farclippingdistanz in eine z-Koordinate $z_{buff}$ umgerechnet. Diese z-Koordinate gehört zu dem vordersten sichtbaren Fragment aus der Thermografiekameraansicht. Ist die z-Koordinate des Fragments im Kamerakoordinatensystem größer als die z-Koordinate aus dem z-Buffer, wird das Fragment aus der Sicht der Thermografiekamera von einem anderen Fragment verdeckt und es wird verworfen. In dies nicht der Fall, ist das Fragment von der Thermografiekamera aus sichtbar und es besitzt einen zugehörigen Temperaturwert. Die zugehörige Temperatur $t$ wird per Nearest neighbor Interpolation aus den Thermografiedaten mit den Texturkoordinaten ermittelt. Die zu der Temperatur zugehörige Fragmentfarbe $\dddot{c}$ wird in der Pseudofarbtabelle nachgeschlagen.

Die Wahl der Farbkodierung der Temperaturen beeinflusst dabei wie leicht und genau Temperaturwerte aus dem Bild abgelesen werden können. Für die Detektion von Temperaturanomalien ist es wichtig, dass Farbwerte für die zu detektierenden Temperaturunterschiede einen wahrnehmbaren Kontrast aufweisen. Eine häufig verwendete Farbabbildung ist die Regenbogenfarbkodierung, die den Wertebereich linear auf den Farbton abbildet. Borland und Taylor bewerten diese Farbkodierung allerdings als nachteilig \cite{Borland2007}, da z.B. keine intuitive Sortierung der Farbwerte möglich ist. Für die Darstellung von Thermografiedaten eignet sich z.B. eine Farbkodierung, welche die Erhitzung von Eisen modelliert. Mit steigender Temperatur geht die zugeordnete Farbe von schwarz, über rot, über gelb bis weiß. Die möglichen Farbwerte sind im Kontext von Temperaturen deutlich leichter zu interpretieren, als vergleichsweise zusammenhangslose Farben, wie die des Regenbogens. Weil die erzeugten Bilder der Thermografievisualisierung mit den Bildern der MRT-Visualisierung fusioniert werden, könnten farbkodierte Temperaturen verfälscht werden. Damit das nicht auftritt, kann der Bildfusionsalgorithmus angepasst werden und/oder die verwendeten Farbkodierungen könnte so gewählt werden, dass für alle möglichen Fusionsergebnisse die Temperaturzuordnung eindeutig bleibt. Verändert der Bildfusionsalgorithmus z.B. das Fusionsergebnis in Abhängigkeit vom dem projizierten Thermografiebild nur in der Luminanz, könnte eine Farbkodierung mit injektiver Farbtonabbildung die Ablesbarkeit der Temperatur gewährleisten.
    
    \section{Bildfusion der MRT- und Thermografievisualisierungen}

Die Hauptaufgabe die mithilfe der kombinierten Visualisierung von MRT- und Thermografiedaten gelöst werden soll, ist das Finden von Bereichen mit Temperaturunterschieden und die anschließende räumliche Abgrenzung/Einordnung dieser Bereiche. Der Fokus des fusionierten Bildes liegt dabei hauptsächlich darauf, die Abgrenzung von Bereichen mit unterschiedlichen Temperaturen zu ermöglichen. Dementsprechend darf das Fusionsergebnis die Temperaturzuordnung nicht beeinflussen. Zusätzlich werden räumliche Hinweisreize benötigt, damit die abgegrenzten Bereiche anatomisch eingeordnet werden können. Diese Hinweisreize stammen überwiegend aus der MRT-Visualisierung, aber zum Teil auch aus der Thermografie-Visualisierung. Sie spiegeln sich dabei überwiegend in der Helligkeitsvariationen und der damit verbunden Texturinformation wieder. Daher sollen die Helligkeitsvariationen aus beiden Bildern, an so vielen Bildstellen wie möglich im fusionierten Bild erhalten werden.

Zur Erfüllung dieser Anforderungen werden zwei voneinander unabhängigen visuelle Variablen genutzt, die Helligkeit und der Farbton. Wird eine geeignete Farbtransferfunktion für die Volumengrafikdarstellung der MRT-Visualisierung gewählt und wird eine geeigneten Farbkodierung für Thermografievisualisierung gewählt, lassen sich die zu vermittelnden Informationen (Temperaturen und räumliche Hinweisreize) voneinander unabhängig auf die visuellen Variablen abbilden. Weil die MRT-Visualisierung nur räumliche Hinweisreize enthält, ist eine Farbtransferfunktion, welche die $T_1$-/$T_2$-Werte ausschließlich auf Helligkeiten/Grautöne abbildet, geeignet. Die Thermografie-Visualisierung enthält beides, Temperaturen und räumliche Hinweisreize. Daher sollten mit der Temperatur die Helligkeit und der Farbton variiert werden. Bezüglich des Farbtons muss die Temperatur eindeutig zugeordnet werden. Sind diese Voraussetzungen erfüllt, kann ein Bildfusionsschema verwendet werden, welches beliebige Bildfusionsalgorithmen nutzen kann. Zur Trennung der Helligkeits- und Farbtoninformation wird der HSV-Farbraum genutzt, der eine Farbe mit einem Farbwinkels (\SIrange{0}{360}{\degree}), einem Farbsättigunganteil (\numrange{0}{1}) und mit eine Helligkeitsanteil (\numrange{0}{1}) beschreibt.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{images/hsv_fusion_scheme}
    \caption{\label{hsv_usage_fgr}Der HSV-Farbraum wird für die Fusion der MRT- und Thermografiebilder verwendet, damit der Farbton und die Sättigung der farbkodierten Temperaturwerte unverändert bleibt.}
\end{figure}

\begin{align}
    V &= \max(R,G,B) \label{rgb_to_h_eq}\\
    S &= \begin{cases}
        \frac{V-min(R,G,B)}{V}, & \text{wenn } V \neq 0 \label{rgb_to_s_eq} \\
        0,                      & \text{sonst}
    \end{cases} \\
    H &=  \begin{cases}
        \frac{\SI{60}{\degree} (G - B)}{(V - \min(R,G,B))},                     & \text{wenn } V = R \\
        \frac{\SI{120}{\degree} + \SI{60}{\degree} (B - R)}{(V - \min(R,G,B))}, & \text{wenn } V = G \\
        \frac{\SI{240}{\degree}+\SI{60}{\degree} (R - G)}{(V - \min(R,G,B))},   & \text{wenn } V = B \\
    \end{cases} \label{rgb_to_v_eq} \\
    \text{wobei }H &\in [\SI{0}{\degree},\SI{360}{\degree}] \text{ und } S, V \in [0,1] \nonumber
\end{align}

Die Bilder der projizierten Thermografie- und der MRT-Visualisierung werden, wie in den Gleichungen \ref{rgb_to_h_eq}, \ref{rgb_to_s_eq} und \ref{rgb_to_v_eq} beschrieben, vom RGB-Farbraum in den HSV-Farbraum konvertiert. Der Farbton- und der Salienzkanal der MRT-Visualisierung wird danach verworfen. Anschließend werden die Helligkeitskanäle beider Bilder fusioniert. Der fusionierte Helligkeitskanal wird mit dem Farbton- und dem Sättigungskanal der Thermografievisualsierung in ein HSV-Bild kombiniert. Das fusionierte Bild ergibt sich durch die Rückkonvertierung des HSV-Bilds.

\begin{align}
    h_\mathrm{i} &= \left \lfloor { \frac{H}{60^\circ} } \right \rfloor \nonumber \\
    f &= \left(\frac{H}{60^\circ} - h_\mathrm{i}\right) \nonumber \\
    p &= V \cdot (1 - S) \nonumber \\
    q &= V \cdot (1 - S \cdot f ) \nonumber \\
    t &= V \cdot \left( 1 - S \cdot (1 - f) \right) \nonumber \\
    \begin{pmatrix}
        R \\
        G \\
        B \\
    \end{pmatrix} &= \begin{cases}
        (V,t,p)^T, & \text{wenn } h_\mathrm{i} \in \{0,6\} \\
        (q,V,p)^T, & \text{wenn } h_\mathrm{i} = 1 \\
        (p,V,t)^T, & \text{wenn } h_\mathrm{i} = 2 \\
        (p,q,V)^T, & \text{wenn } h_\mathrm{i} = 3 \\
        (t,p,V)^T, & \text{wenn } h_\mathrm{i} = 4 \\
        (V,p,q)^T, & \text{wenn } h_\mathrm{i} = 5
    \end{cases} \label{hsv_to_rgb_eq}
\end{align}

Die Konvertierung vom HSV-Farbraum in den RGB-Farbraum erfolgt, unter der Berechnung von Hilfsvariablen, wie in Gleichung \ref{hsv_to_rgb_eq} beschrieben. Die Verwendung der HSV-Farbraumtransform ist in der Abbildung \ref{hsv_usage_fgr} dargestellt.

Zur Fusion der Helligkeitskanalbilder $I_1$ und $I_2$ kommt der Bildfusionalgorithmus von Li \textit{et al.} \cite{Li2013} zum Einsatz, da das Verhältnis des benötigten Rechenaufwands und der resultierenden Fusionsqualität für die Echtzeitfusion optimal erscheint. Pro Bild $I_n$ wird mit einem Mittelwertsfilter $Z$ eine zugehöriges Grundbild $B_n$ errechnet.
\begin{align}
    B_n = I_n * Z
\end{align}

Durch die Subtraktion des Grundbilds $B_n$ vom Orginalbild $I_n$ wird ein zusätzliches Detailbild $D_n$ erzeugt.
\begin{align}
    D_n = I_n - B_n
\end{align}

Für jedes der Eingangsbilder wird außerdem eine zugehörige Salienzkarte $S_n$ erstellt, die das Ausmaß an salienter Information für jedes Pixel beschreibt.
\begin{align}
    S_n = \left| I_n * L_n \right| * g_{r, \sigma}
\end{align}

Die Salienz wird mit einem Laplacefilter $L$ und einem Gaußfilter $g_{r, \sigma}$ berechnet. Aus allen Salienzkarten werden jeweils zugehörige initiale Gewichtskarten $P_n$ erstellt.
\begin{align}
    \left({P_n}\right)_{i,j} = \begin{cases}
        1, & \text{wenn $\left({S_n}\right)_{i,j} = max \left(\left({S_1}\right)_{i,j}, \left({S_2}\right)_{i,j}\right)$} \\
        0, & \text{sonst}
    \end{cases} 
\end{align}

Mithilfe eines geführten kanten-erhaltender Weichzeichnungsfilters $G_{r, \epsilon} (P, I)$, der ein Zielbild $P$ (hier die initiale Gewichtkarte) weichzeichnet und der die Kanten eines Führungsbildes $I$ (hier ein Eingabebild der Fusion) erhält, wird eine räumliche Konsistenz für die Fusionskoeffizienten gewährleistet.
\begin{align}
    W_n^B &= G_{r_1, \epsilon_1}(P_n, I_N) \\
    W_n^D &= G_{r_2, \epsilon_2}(P_n, I_N)
\end{align}

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{images/li2013_gff_image_fusion_scheme}
    \caption{Der von Li \textit{et al.} vorgestellte Bildfusionsalgorithmus wird zur Fusion der Helligkeitskanäle von den MRT- und Thermografiebildern genutzt \cite{Li2013}.}
\end{figure}

Dabei werden mit unterschiedlichen Filterparametern $r$ und $\epsilon$ zugehörige Gewichtskarten $W_n^B$, $W_n^D$ für die Grund- und die Detailbilder errechnet. Die Werte aller Filterparameter wurden von Li \textit{et al.} übernommen, da in dieser Arbeit nicht genügend Bildmaterial für eine etwaige Optimierung der Parameter vorlag. Um die Gewichte zu normalisierten werden die Gewichtskarten einer Klasse (Detail oder Grund) pro Pixel durch die Summe aller Gewichte an dem Pixel geteilt. Das fusioniertes Grundbild $\overline{B}$ und das fusioniertes Detailbild $\overline{D}$ wird pro Pixel mit einem gewichteten Mittelwert bestimmt.
\begin{align}
    \overline{B} &= W_1^B B_1 + W_2^B B_2 \\
    \overline{D} &= W_1^D D_1 + W_2^D D_2
\end{align}

Das Finale fusionierte Bild entsteht durch Addition des fusionierten Detail- und Grundbilds.
\begin{align}
    F = \overline{B} + \overline{D}
\end{align}

% --------------------------------------------------------------------------- %    
\chapter{Umsetzung und Implementierung}

In dieser Arbeit wurde ein Tool entwickelt, mit dem sich die intrinsischen Kameraparameter und die Transformation vom Referenzadapter zur Kamera bestimmen lässt. Außerdem wurde eine Erweiterung für 3D Slicer entwickelt, welche die intraoperative Visualisierung realisiert.  Für die Feststellung der dafür nötigen Daten, kommt neben der verwendeten Thermografiekamera, ein Neuronavigationssystem und ein Kalibrationsmuster zum Einsatz, welches speziell für die Wärmebildgebung geschaffen wurde.

Neben Kameraparametern, muss aus den MRT-Daten die Gehirnoberfläche extrahiert werden, die ausschnittsweise von der Thermografiekamera aufgenommen wird. Dazu wird eine vorhandene Erweiterung der Software 3D Slicer verwendet. Zunächst werden die MRT-Daten segmentiert, sodass klar wird welche Voxel das Gehirn darstellen. Dieser Prozess nennt sich \textit{Skull Stripping}. Die verwendete Erweiterung implementiert den von Iglesias \textit{et al.} vorgestellten Gehirnsegmentierungsalgorithmus ROBEX \cite{Iglesias2011}. Die Orte im MRT-Datensitz die nicht das Gehirn beschreiben, erhalten dabei den Wert 0. Der verwendete Ansatz zur Bestimmung der Gehirnoberfläche basiert darauf, dass aus dem segmentierten regelmäßigen Datengitter eine Isofläche in Form eines Dreicksnetzes extrahiert wird. Dazu wird der \textit{Marching Cubes} Algorithmus angewandt und der Isowert wird auf 1 gesetzt, um die Hülle der Segmentierung zu erhalten. Dieses Dreiecksnetz entspricht der weiterhin verwendeten Oberfläche.

Für die intraoperative Visualisierung wird ebenfalls 3D Slicer verwendet. Neben der Gehirnsegmentierungserweiterung kommen noch zwei weitere vorhandene Erweiterungen zum Einsatz, welche die Tracking- und Thermografiedaten mithilfe des OpenIGTLink-Protokolls über das Netzwerk übertragen. Dieses Protokoll wird verwendet, weil das verwendete Trackingsystem eine entsprechende Server-Schnittstelle anbietet, weil das Protokoll zur Übertragung der Thermografiedaten verwendet werden kann und weil 3D Slicer dafür eine steuerbare Client-Schnittstelle zur Verfügung stellt. Die 3D Slicer-Erweiterung OpenIGTLinkIF verwaltet die Verbindungen zu den Serverschnittstellen für die Thermografiekamera und für das Neuronavigationssystem. Die Erweiterung OpenIGTLinkRemote dient zur manuellen Steuerung der zu übertragenden Daten in einer Sitzung, da das OpenIGTLink-Protokoll dafür keine automatischen Mechanismen vorsieht. Die in dieser Arbeit vorgestellten Funktionalitäten wurden in Form einer neuen Erweiterung implementiert. Diese Erweiterung besteht aus drei Modulen, die verschiedene notwendige Teilaufgaben lösen.
        
    \section{Verwendete Hardware}

In der Arbeit wird das Neuronavigationssystem \textit{BrainLab VectorVision 2} verwendet, welches alle nötigen Trackinginformationen liefert. Es übermittelt kontinuierlich die Position und Orientierung von diversen Adaptern und Instrumenten im Anatomiekoordinatensystem. Die nötige Kalibrierung der Adapter und Instrumente wurde bereits für den intraoperativen Einsatz durchgeführt und die Kalibrierungsdaten sind gegeben. Bei allen präoperativen Schritten zur Parameterbestimmung wird die Thermografiekamera und das notwendige Kalibrationsmuster benötigt. Bei der präoperativen Kameralagebestimmung, wird zusätzlich das Neuronavigationssystem benötigt. Dabei trackt es einen Referenzadapter an einem Phantom, einen Referenzadapter an der Kamera und ein beliebig positionierbares Zeigerinstrument. Die präoperativen Kamerakalibrierung und -lagebestimmung muss für ein statisches Hardwaresetup nur ein einziges Mal durchgeführt werden. Intraoperativ kommt die Thermografiekamera mitsamt dem gleich befestigten Referenzadapter und dem Neuronavigationssystem zum Einsatz.

        \subsection{Neuronavigationssystem}

Neuronavigationssysteme werden für bildgestützte, chirurgische Eingriffe verwendet und sie bestehen aus mindestens zwei Komponenten. Eine Stereokamera wird verwendet, um intraoperativ die Lage der zu behandelnden Person und die Lage der verwendeten Instrumente in der Szene zu bestimmen. Außerdem besitzt ein Neuronavigationssystem ein Display, auf dem die präoperative Aufnahmen der zu behandelnden Person angezeigt werden. Zum Beginn jeder Operation wird das Szenenkoordinatensystem mit der zu behandelnde Person auf das Anatomiekoordinatensystem registriert. Nachdem diese Registrierung durchgeführt wurde, können verschiedene nützliche Informationen aus den präoperativen Aufnahmen bestimmt werden. Ein mögliches Anwendungsbeispiel ist die Anzeige der relativen Position eines Zeigerinstruments bezüglich präoperativen MRT-Daten.  Somit lassen sich reale Orte direkt auf die MRT-Daten beziehen, um z.B. die Abgrenzung von verschiedenen Gewebearten zu erleichtern. Im Rahmen der Arbeit kommt das Neuronavigationssystem \textit{BrainLab VectorVision 2} zum Einsatz, welches über eine Netzwerkschnittstelle zur Übertragung der Trackingdaten verfügt. Da dieses Neuronavigationssystem bereits für allen relevanten Eingriffe verwendet wird, stellt es keine neue Hardwareanforderung dar, die während einer Operation potentiell Platz wegnimmt. Das verwendete Neuronavigationssystem und dessen Komponenten sind in der Abbildung \ref{neuronvagiation_components_fgr} zu sehen.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.51\textwidth}
        \includegraphics[width=\textwidth]{images/neuronavigation_system}
        \caption{Stereokamera \& Display des Neuronavigationssystems}
    \end{subfigure} 
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pointer_instrument}
        \caption{Zeigerinstrument}
        
        \vspace{1.3em}
        
        \includegraphics[width=.9\textwidth]{images/reference_adapter}
        \caption{Referenzadapter}
    \end{subfigure}
    \caption{\label{neuronvagiation_components_fgr} Für die Registrierung kommt das Neuronavigationssystem \textit{BrainLab VectorVision 2} zum Einsatz. Es trackt das Zeigerinstrument und den Referenzadapter an der Kamera.}
\end{figure}

Da Neuronavigationssysteme für den intraoperativen Einsatz konzipiert sind, setzen sie, vor dem Beginn der Operation, die Registrierung der zu behandelnen Person auf eine entsprechende MRT-Aufnahme des zu operierenden Bereichs voraus. Für den präoperativen Einsatz wird als Ersatz für eine Person ein Phantom verwendet, welches in einer Vorarbeit von Weidner konstruiert wurde \cite{Weidner2014}. Von diesem Phantom liegen MRT-Daten vor. Auf diesem Phantom wurden, genau wie bei der Registrierung mit einer realen Person, äußerlich Marker angebracht, die in der MRT-Bildgebung erkennbar sind. Die Markerpositionen im Anatomiekoordinatensystem wurden manuell mithilfe der MRT-Aufnahme definiert.

\begin{figure}[h]
    \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/phantom_vis}
        \caption{Phantom mit angebrachten Fiducials (weiß, kreisförmig)}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/phantom_mri}
        \caption{$T_2$-MRT-Bild des Phantoms (Volumengrafik)}
    \end{subfigure}
    \caption{\label{phantom_fgr}Für die präoperative Verwendung des Neuronavigationssystems wird ein Phantom verwendet, das von Weidner \textit{et al.} entwickelt wurde \cite{Weidner2014}. Auf der Oberfläche des Phantoms befinden sich Marker (Fiducials), die in den MRT-Aufnahmen erkennbar sind.}
\end{figure}

Außerdem wird am Kopfbereich der zu behandelnden Person oder an dem Phantom statisch ein Referenzadapter angebracht. Ein weiterer Referenzadapter wird statisch an der Kamera angebracht. Die Referenzadapter besitzen unterschiedliche bekannte Geometrien, mit denen jeweils ein lokales Referenzadapterkoordinatensystem definiert ist. An jedem Referenzadapter sind auf drei festen Punkten Markerkügelchen angebracht, deren Positionen im Szenenkoordinatensystem von der Stereokamera des Neuronavigationssystems erfasst werden. Der am Kopfberereich/Phantom angebrachte Referenzadapter dient, ähnlich wie bei dem vorgestellten intraoperativen Bildregistierungsansatz, dazu, die notwendige Transformation vom dem Szenenkoordinatensystem in das Anatomiekoordinatensystem aktuell zu halten. Das Neuronavigationssystem liefert kontinuierlich die Lage des Kamerareferenzadapters. Diese wird als Transformationsmatrix von dem Referenzadapterkoordinatensystem in das Anatomiekoordinatensystem übertragen.

Neben den Referenzadaptern gehört außerdem noch ein Zeigerinstrument zum Repertoire des Neuronavigationssystems.
Dieses Zeiger hat genau wie die Adapter eine bekannte Geometrie und auf dem Instrument sind an zwei festen Punkten ebenfalls Markerkügelchen angebracht, deren Position von der Stereokamera im Szenenkoordinatensystem erfasst werden. Das Neuronavigationssystem übermittelt kontinuierlich die Positionen der Zeigerspitze im Anatomiekoordinatensystem.

Die Registrierung der präoperativen MRT-Daten auf die Szene, in der sich die zu behandelnde Person befindet, erfolgt mit einer vorgegebenen Prozedur. Der Registrierungsprozess des verwendeten Neuronavigationssystems wird in dem Buch \textit{Neurosurgery}, das in der Reihe \textit{European Manual of Medicine} erschienen ist, erläutert \cite{Lumenta2010}. Dazu werden auf der Haut der zu behandelnen Person leicht erkennbare Marker befestigt, die als Fiducials bezeichnet werden. Die Fiducials sind bei der präoperativen MRT-Aufnahme und intraoperativ an den gleichen Stellen und in einer nicht koplanaren Anordnung angebracht. Kurz vor der Operation werden nach der Anbringung des Referenzadapters, die Szenenposition der Fiducials ermittelt. Dazu wird mit dem Zeigerinstrument und einer Pivotierbewegung auf die Fiducials gezeigt. Damit berechnet das Neuronavigationssystem eine lineare Registrierungstransformation zwischen den Szenenpunkten und den Anatomiepunkten der Fiducials in den MRT-Daten.  Die Fiducialpositionen im Anatomiekoordinatensystem werden mithilfe der MRT-Aufnahmen manuell von dem medizinischen Fachpersonal definiert. Mit dem an dem Kopfbereich angebrachten Referenzadapter wird so für den Verlauf der Operation die Transformation vom Szenenkoordinatensystem zum Anatomiekoordinatensystem definiert. Das Anatomiekoordinatensystem wird dabei anhand der MRT-Daten definiert. Der Ursprung dieses Koordinatensystems liegt im Mittelpunkt der MRT-Daten (in der Kopfmitte). Gemäß der anatomischen Lagebeschreibungen des Menschens, verläuft die x-Achse vom linken zum rechten Ohr, die y-Achse von posterior (hinten) nach anterior (vorn) und die z-Achse verläuft von inferior (unten) nach superior (oben). Diese Kombination von Achsenorientierungen wird als RAS (Rechts, Anterior, Superior) abgekürzt. Die Distanzen werden in \si{mm} angegeben, deswegen muss die Auflösung der MRT-Daten bekannt sein.

        \subsection{Thermografiekamera}

In der Arbeit kommt die Thermografiekamera \textit{InfraTec 2 VarioCAM HD head 680 S} zum Einsatz. Zur Temperaturbestimmung verwendet diese Kamera ein nicht gekühltes Mikrobolometerarray. Dabei erreicht die langwellige Infrarotstrahlung (LWIR) mit Wellenlänge zwischen \SIrange{7,5}{14}{\micro\meter} (FIR) das Detektormaterial. Das Material erhitzt sich und die Veränderung des elektrischen Widerstands wird gemessen und in eine Temperatur umgerechnet.

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{images/thermography_camera}
    \caption{\label{thermography_camera_fgr}Die Thermografiekamera \textit{InfraTec 2 VarioCAM HD head 680 S} wird zur Verwendung auf ein Stativ montiert, an dem gleichzeitig der Referenzadapter angebracht wird.}
\end{figure}

Das Sensorarray der Kamera besteht aus $\num{640} \times \num{480}$ Messpunkten/Pixeln und es ist \SI{16}{mm} breit und \SI{12}{mm} hoch. Die Thermografiekamera kann bis zu \num{60} Bilder pro Sekunde aufnehmen. Die Temperaturauflösung liegt bei \SI{0,03}{K} bei Messungen um die \SI{30}{\celsius}. Die Fokusebene der Kamera lässt sich mit einem Schrittmotor verändern, der Sichtwinkel bleibt davon unbeeinflusst.

Die Thermografiekamera wird mit einem Teleobjektiv verwendet, das eine Brennweite von \SI{60}{mm} besitzt. Daraus resultiert ein horizontaler Sichtwinkel von \SI{15.2}{\degree} und ein vertikaler Sichtwinkel von \SI{11,4}{\degree}. Allerdings wird zusätzlich ein Fokusring eingesetzt, der den Fokuspunkt um eine feste Distanz verschiebt. Das ist nötig, um den Einsatz mit weiteren Kameras, die einen festen Fokuspunkt besitzen, zu ermöglichen. Die daraus resultierende Veränderung an dem Sichtwinkel wird nicht vom Hersteller angegeben. Aufgrund der vergleichsweise hohen Brennweite, ist die Tiefenunschärfe hoch, sodass die Tiefe des scharf dargestellten Bereichs sehr gering ausfällt. Die Thermografiekamera ist mit dem verwendetem Objektiv und einem angebrachten Referenzadapter in der Abbildung \ref{thermography_camera_fgr} dargestellt.

        \subsection{Kalibrationsmuster}


Für die präoperative Kamerakalibrierung und -lagebestimmung wird ein selbst entwickeltes Kalibrationsmuster eingesetzt. Damit die Merkmale dieses Musters in Thermografieaufnahmen erkennbar sind, müssen diese Merkmale einen Temperaturgradienten erzeugen, der größer als das Wärmegrundrauschen der Umgebung ist. Deswegen ist das Kalibrationsmuster eine Platte mit einem Lochmuster, hinter dem sich eine beliebig starke Wärmequelle positionieren lässt. Die Wärmequelle strahlt durch die Löcher, aber nicht durch das restliche Material, sodass an den Löchern die benötigten Temperaturgradienten erzeugt werden. Das Muster wurde in eine \SI{5}{mm} dicke, schwarz gefärbte, mitteldichte Holzfasserplatte gebohrt. Durch das verwendete Material erwärmt sich die Platte nur sehr langsam. Reflektionen der emittierten Wärme von anderen Wärmequellen werden durch die dunkle Farbe minimiert. Das Muster besteht aus $\num{6} \times \num{7}$ regelmäßig angeordneten, runden Löchern. Die Lochbreite beträgt \SI{5}{mm} und der Lochabstand beträgt \SI{10}{mm}. Außerdem sind auf dem Kalibrationsmuster in fest definierten Abständen drei kleine Einkerbungen für das Zeigerinstrument der Neuronavigation aufgebracht. Die co-planaren Lochmittelpunkte lassen sich mit den Positionen dieser Einkerbungen berechnen.

\begin{figure}[h]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/calibration_pattern_vis}
        \caption{Kalibrationsmuster und Thermopad}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/calibration_pattern_thermo}
        \caption{Thermografieaufnahme des Kalibrationsmusters}
    \end{subfigure}
    \caption{\label{calibration_pattern_fgr}Für die präoperative Bestimmung der Kameraparamter wird ein Kalibrationsmuster eingesetzt, das sichtbare Merkmale für die Thermografie besetzt.}
\end{figure}

Da elektrische Geräte im Operationsbereichen bestimmten Sicherheitsvorschriften unterliegen, wurde auf einen elektrischen betriebene Wärmeplatte als Wärmequelle verzichtet. Stattdessen wird ein Thermopad verwendet, welches sich leicht erhitzen lässt und welches die Temperatur für die Dauer der präoperativen Schritte ausreichend lang hält. Das Kalibrationsmuster ist in der Abbildung \ref{calibration_pattern_fgr} mit dem verwendeten Thermopad dargestellt.

    \section{Verwendete Software}

Um die Implementierung zu erleichtern, wurde für die Arbeit frei verfügbare Software genutzt. Das Tool zur präoperativen Parameterbestimmung nutzt die Algorithmen zur Kamerakalibrierung und -lagebestimmung aus der Bibliothek OpenCV. Die intraoperative Visualisierung wird als eine Erweiterung der Software 3D Slicer implementiert. 3D Slicer und die implementierte Erweiterung nutzen dabei intensiv das objektorientierte Visualisierungstoolkit VTK. Außerdem nutzt die Erweiterung die Bildverarbeitungsalgorithmen aus OpenCV, um die Bildfusion zu implementieren. Das OpenIGTLink-Protokoll wird genutzt um die Thermografiedaten und die Trackinginformationen über das Netzwerk zum dem Rechner zu übertragen, auf dem die Visualisierung läuft.

        \subsection{OpenCV}

OpenCV\footnote{\url{https://opencv.org/}} ist eine freie und quelloffene Bibliothek, die verschiedene Algorithmen aus dem Bereich der Computer Vision implementiert. Die Bibliothek ist in C(++) geschrieben und sie legt ihren Fokus auf Effizienz und Echtzeitanwendungen. Bspw. lassen sich mit den enthalten Bibliotheksmodulen eine Objektidentifikation, das Tracking von Kameras und Objekten, die Erstellung von 3D Modellen und beliebige Bildbearbeitungschritte sehr leicht realisieren.

Die präoperative Software zur Parameterbestimmung benutzt zum Großteil das Modul \textit{calib3d}, welches mehrere Algorithmen für die Kamerakalibrierung und -lagebestimmung bietet. Desweiteren wird das Modul \textit{features2d} verwendet, um die Positionen der Löcher des Kalibrationsmusters in den 2D-Bildern zu ermitteln. Die dafür notwendige grafische Oberfläche lies sich ohne viel Aufwand mit dem Modul \textit{highui} umsetzen, welches die Verwaltung von Bildfenstern auf einem hohen Abstraktionslevel zur Verfügung stellt. Zur effizienten CPU-basierten Umsetzung der Bildfusion wird das Modul \textit{imgproc} verwendet, welches diverse Filter- und Bildverarbeitungsoperationen implementiert.

        \subsection{OpenIGTLink}

OpenIGTLink\footnote{\url{http://openigtlink.org/}} stellt mithilfe eines Protokolls und einer Bibliothek eine freie und quelloffene Netzwerkschnittstelle zu Verfügung, die für Bild-gestützte Eingriffe konzipiert ist. Die initiale Version des Protokolls wurde 2009 von Tokuda \textit{et al.} veröffentlich \cite{Tokuda2009} und entsprechende Schnittstellen sind seitdem für viele Geräte der Bild-gestützen Chirurgie verfügbar. Das Protokoll definiert Nachrichten zur Übertragung von 2D- \& 3D-Bildern, Transformationsmatrizen, Statusnachrichten, Trajektorien, Sensordaten, Polygonnetzen und Videos. Die Übertragung basiert auf dem TCP-Netzwerkprotokoll. Mechanismen zur Sitzungsverwaltung sind vom Protokoll nicht vorgesehen. Eine in C(++) geschriebene Bibliothek ermöglicht das Interpretieren und Erstellen solcher Nachrichten, mithilfe von TCP-Sockets und mit einem Client/Server-Modell.

In dieser Arbeit wurde ein OpenIGTLink-Server eingesetzt, der die Bilder der Thermografiekamera über eine proprietäre Bibliothek des Kameraherstellers lokal abgreift und der sie als Bildnachrichten (vom Typ IMAGE) an beliebige OpenIGTLink-Clients versendet. So eine Bildnachricht beinhaltet den zugrunde liegenden Datentyp, das verwendete Koordinatensystem, die Pixeldimensionen, die Auflösung der einzelnen Dimensionen, die Lage des Bildes im Raum, den Bereich des übertragenen Subvolumens und die Bilddaten selbst. Weiterhin läuft auf dem verwendeten Neuronavigationssystem ein OpenIGTLink-Server der auf Nachfrage Trackinginformationsnachrichten (vom Typ TDATA) an beliebige OpenIGTLink-Clients versendet. Eine einzelne Nachricht mit Trackinginformationen beinhaltet den Namen, den Typ und eine Transformationsmatrix für jedes aktuell getrackte Instrument. Für den Empfang der Nachrichten wurde für die Software zur präoperativen Parameterbestimmung ein OpenIGTLink-Client implementiert. In 3D Slicer wurde eine bestehende OpenIGTLink-Erweiterung verwendet, die ebenfalls einen konfigurierbaren OpenIGTLink-Client zur Verfügung stellt.

        \subsection{VTK}

VTK\footnote{\url{https://www.vtk.org/}} ist eine freie und quelloffene C++-Bibliothek, welche die Erzeugung von 3D-Computergrafiken, die Umsetzung von Bildverarbeitungsschritten und die Verwendung verschiedener Visualisierungsmethoden mit einem sehr flexiblen, objekt-orientierten Interface ermöglicht. Das Kernkonzept von VTK ist eine bedarfsorientierte Verarbeitungspipeline. Dazu werden die Ein- und Ausgaben von verschiedenen Algorithmen miteinander verknüpft und nur dann ausgeführt, wenn sich die Eingaben verändert haben. Am Ende der Pipeline können die erzeugten Daten in grafische Primitive umgewandelt werden, die über eine grafische Oberfläche dargestellt werden können. Mit der Bibliothek lassen sich Visualisierungen wie Farbabbildungen, Teppichdiagramme, Isoflächen und Thresholding erzeugen. Der starke Einsatz des objekt-orientierten Paradigmas ermöglicht das nahtlose Erweitern der Rendering Pipeline um beliebige Algorithmen. Die Bibliothek ist ereignisorientiert implementiert, sodass jegliche Mensch-Computer-Interaktion leicht angepasst werden kann.

Da VTK vor allem für die Erstellung von Visualisierungen geeignet ist, wurde es für die Implementierung der 3D Slicer Erweiterung verwendet. VTK bietet für die Entwicklung eigener Visualisierungen anpassbare OpenGL-Shadertemplates zur Darstellung von Polygonnetzen, mithilfe derer die Visualisierung der Thermografiedaten erfolgt. Der Bildfusionsalgorithmus wurde als beliebig verwendbarer Teil der Rendering Pipeline implementiert. 3D Slicer verwendet VTK ebenfalls in vielen Teilen des Programms.

       \subsection{3D Slicer}

3D Slicer\footnote{\url{https://www.slicer.org/}} ist eine freie und quelloffene Softwareplattform für medizinische Bildgebung, Bildverarbeitung und 3D-Visualisierung. Die Softwareentwicklung begann 1998 als ein Projekt in einer Masterarbeit und seitdem wurde die Software kontinuierlich mit neuen Funktionalitäten erweitert, sodass inzwischen von einer Softwareplattform gesprochen werden kann \cite{Gering1999, Gering2001, Pieper2004, Pieper2006, Fedorov2012, Kikinis2013}. Die Funktionalitäten werden mithilfe einer Reihe von Bibliotheken (wie z.B. VTK, CTK, ITK, IGSTK) implementiert und mittels einer grafischen Benutzeroberfläche (die auf Qt basiert) zur direkten Nutzung bereitgestellt. Dazu wird das komplett serialisierbare Datenmodel MRML (Medical Reality Modeling Language) zur Beschreibung der Szene verwendet. Das Datenmodell und die GUI sind zur Laufzeit mit einem eingebauten Python-Interpreter veränderbar. Die Software gliedert sich in eine große Anzahl von Modulen, die jeweils eigene Funktionalitäten in einem dedizierten GUI-Bereich bereitstellen. Weitere Module können über fremd entwickelte Erweiterungen hinzugefügt werden, die über einen Erweiterungsmanager verwaltbar sind. Da die Module Zugriff auf das komplette Datenmodell und die GUI haben, sind umfangreiche und tiefgreifende Änderungen mit den Erweiterungen umsetzbar.

Alle selber verwendbaren Daten werden in 3D Slicer als \textit{MRML-Nodes} repräsentiert. Ein solcher \textit{MRML-Node} kann z.B. Gitterbilddaten oder eine lineare Transformationen beschreiben. Jeder dieser \textit{MRML-Nodes} lässt sich in einem XML-basiertes Dateiformat speichern und daraus wieder einlesen. Zu jedem \textit{MRML-Node} der visualisierbare Daten repräsentiert, gibt es potentiell mehrere \textit{MRML-DisplayNodes} die beschreiben wie diese Daten visualisiert werden. Zu jedem \textit{MRML-Node} der serialisierbare Daten repräsentiert, gibt es potentiell mehrere MRML-StorageNodes die beschreiben wie diese Daten in einem bestimmten Dateiformat gespeichert werden. Die \textit{MRML-Nodes} können Referenzen aufeinander aufweisen und andere \textit{MRML-Nodes} nach dem \textit{Observer} Pattern beobachten um bei Änderungen entsprechende Aktionen auszuführen. Alle \textit{MRML-Nodes} sind Teil der \textit{MRML-Szene} die eine zentrale Datenstruktur darstellt, die für alle Module zur Verfügung steht.

Jede Erweiterung für 3D Slicer ist eine Sammlungen von Modulen. Jedes Modul implementiert dabei voneinander unabhängige Funktionalitäten. CLI Module bestehen aus einem alleinstehenden, ausführbaren Programm und einer automatisch generierten GUI. Geskriptete Module werden in Python geschrieben und ermöglichen vollen Zugriff auf die API vieler verwendeter Bibliotheken, da für die meisten Bibliotheken Wrapper zur Verfügung stehen. Ladbare Module werden in C++ geschrieben und direkt gegen denn Quellcode von 3D Slicer gebaut, damit haben sie die Möglichkeit fast alle Programmteile zu verändern. Eigene Visualisierungen lassen sich nur mit ladbaren Module implementieren, daher sind alle in dieser Arbeit implementierten Module, ladbare Module. Diese Modul bestehen i.d.R. aus mindestens einer Logikklasse, welche für die Datenmanipulation zuständig ist und aus einer GUI-Klasse (basierend auf Qt Widgets), welche die grafische Ein- und Ausgaben von Daten übernimmt. Außerdem können ladbare Module die \textit{MRML-Szene} um eigene \textit{MRML-Node} Typen, wie z.B. einem neuen \textit{MRML-DisplayNode}, erweitern. Zur Erstellung von eigenen Visualisierungen muss ein eigener \textit{MRML-DisplayableManager} implementiert werden, der die \textit{MRML-Szene} und die entsprechende \textit{MRML-DisplayNodes} beobachtet. Dieser \textit{DisplayableManger} muss sich dann darum kümmern, dass zu den \textit{MRML-DisplayNodes} entsprechende Visualisierungsobjekte mit VTK erzeugen und aktualisiert werden. Zum Programmstart werden alle ladbaren Module als dynamische Bibliotheken geladen und es werden entsprechende Instanzen der Logikklassen, der GUI-Repräsentationen und der \textit{DisplayableManager} erzeugt.

Zur Erzeugung der in der Arbeit vorgestellten Visualisierung sind eine Reihe von Modulen notwendig. Dabei gibt es interne Module, die direkt Teil von Slicer sind, es gibt externe Module die Teil einer fremden Erweiterungen sind und es gibt die externen Module, die im Rahmen der Arbeit implementiert wurden. Für die präoperative Extraktion der Oberflächengeometrie wird das externe Modul \textit{ROBEXBrainExtraction\footnote{\url{https://www.slicer.org/wiki/Documentation/Nightly/Modules/ROBEXBrainExtraction}}} und das interne Modul \textit{Grayscale Model Maker} genutzt. Zur Eingabe der Transformation von der Kamera zum Referenzadapter wird das interne Modul \textit{Transformations} genutzt. Die Datenübertragung über OpenIGTLink erfolgt mit den externen Modulen \textit{OpenIGTLinkIF\footnote{\url{https://www.slicer.org/wiki/Documentation/Nightly/Developers/OpenIGTLinkIF}}} und \textit{OpenIGTLinkRemote\footnote{\url{https://www.slicer.org/wiki/Documentation/Nightly/Modules/OpenIGTLinkRemote}}}. Zur Visualisierung der MRT-Daten wird das interne Modul \textit{Volume Rendering} genutzt. Für die Erstellung und Synchronisation der Thermografiekameraansicht wurde das externe Modul \textit{Camera Creator} implementiert. Die Visualisierung der Thermografie-Daten wurde in dem externen Modul \textit{Projective Texture Mapping} implementiert. Die Fusion der MRT- und Thermografievisualisierungen wurde in dem externe Modul \textit{Image Fusion} implementiert.

    \section{Software zur Bestimmung der Modellparameter}

Zur Bestimmung der Parameter des Kameraabbildungsmodells wurde eine Software für den präoperativen Einsatz entwickelt. Diese Software benötigt einige Eingabendaten, wie z.B. die Thermografiedaten des Kalibrationsmusters in einer bestimmten Pose, um diese Parameter zu bestimmen. Die einzelnen Teilprozesse der Software generieren dabei verschiedene Ausgabedaten, wie z.B. die ermittelten intrinsischen Kameraparameter der Thermografiekamera, die in den Teilschritten weiterverwendet werden und am Ende als Ergebnis in eine XML-Datei geschrieben werden. Die Bestimmung der Kameraabbildungsparameter erfolgt in zwei aufeinander folgenden Teilprozessen. Der erste Teilprozess ist die Kamerakalibrierung zur Bestimmung der intrinsischen Kameraparameter. Während diesem Prozess befindet sich die Thermografiekamera in einer gleichbleibenden Pose. Dazu wird mit der Kamera eine Datenreihe des Kalibrationsmusters aus unterschiedlichen Posen aufgenommen. Die Thermografiedaten werden dazu in geeignete Bilder umgewandelt. Weil alle Lochmerkmale in den Bildern ähnlich Größen, Helligkeiten und Orientierungen besitzen, werden ihre zugehörigen Bildpunkte pro Thermografiedatensatz mithilfe eines einfachen und effizienten Bloberkennungsalgorithmus von OpenCV automatisch detektiert. Am Ende dieses Teilprozesses berechnet der Kamerakalibrierungsalgorithmus von OpenCV, aus den Bildpunkten und aus zusätzlichen Informationen über die Mustergeometrie, die gesuchten intrinsischen Parameter (Brennweite, Hauptpunkt, Verzerrungskoeffizienten). Der darauf folgende Teilprozess ist die Bestimmung der Transformation von dem Referenzadapterkoordinatensystem in das Kamerakoordinatensystem (und deren Inverse). In diesem Prozess hat das Kalibrationsmuster und die Kamera jeweils eine gleichbleibende Pose. Diesmal wird nur einmal die Thermografiedaten des Kalibrationsmusters in einer beliebigen Pose aufgenommen. Die Bildpunkte des Lochmusters werden genau wie im ersten Teilprozess detektiert. Danach werden, mit dem getrackten Zeigerinstrument des Neuronavigationssystems, drei vordefinierte Punkte auf dem Muster angetippt. Der Kameralagebestimmungsalgorithmus von OpenCV berechnet, aus den Bildpunkten und aus zusätzlichen Informationen über die angetippten Anatomiepunkte, einmalig die extrinsischen Parameter der Kamera. Dann wird die Position und die Orientierung des getrackten Referenzadapters des Neuronavigationssystems verwendet, um die gesuchte statische Transformation zu berechnen.

Während des gesamten Prozess werden die Daten der Thermografiekamera benötigt. Diese werden von einem OpenIGTLink-Server übertragen. Deswegen läuft ein eigens implementierter OpenIGTLink-Client in einem separaten Thread und empfängt die ankommenden Thermografiedaten asynchron. Alle Temperaturwerte werden unkomprimiert als ein 32-Bit Gleitkommazahlen übertragen. Damit lassen sich über eine Verbindung, die eine verfügbare Datenrate von \SI{100}{Mbit/s} besitzt, ca. \num{10} Bilder pro Sekunde übertragen. Die Temperaturwerte sind bei der Übertragung in \si{\milli\kelvin} angegeben. Da die verwendeten Algorithmen von OpenCV aber auf RGB- oder Grauwertbildern, werden die Daten anhand eines selbst definierten Schemas in Bilder konvertiert. Dazu wird die Skala der Temperaturwerte von dem minimal und maximal Temperaturwert linear auf eine Skala von Grauwerten, die im Bereich \numrange{0}{255} liegen, abgebildet. Eine Farbtiefe von \SI{8}{Bit} ist dabei ausreichend. Die absolute Temperaturwerte sind für die Merkmalserkennung irrelevant. Die so erzeugten Bilder werden je nach Bedarf, in einer Eventschleife threadsicher kopiert, verwendet und angezeigt.

Für den zweiten Teilprozess wird für einen längeren Zeitraum die Position des Zeigerinstruments und die Position und die Orientierung des Referenzadapters im Anatomiekoordinatensystem benötigt. Diese Daten werden auf Nachfrage vom OpenIGTLink-Server des eingesetzten Neuronavigationssystems versendet. Der asynchrone Datenempfang erfolgt, wie bei den Thermografiedaten, mithilfe eines eigens implementierten OpenIGTLink-Clients, der in einem separaten Thread läuft. Da das OpenIGTLink-Protokoll keine Sitzungsverwaltung spezifiziert, wird die Übertragung der Trackinginformationen durch Versenden einer Nachricht mit der gewünschten Aktualisierungsrate an den OpenIGTLink-Server gestartet. Der Client erhält dann die Nachrichten, welche die Position und Orientierung aller aktuell sichtbaren Instrumente und Referenzadapter in Form von Transformationsmatrizen enthalten. Die Transformationsmatrizen konvertieren dabei das jeweilige lokale Referenzkoordinatensystem in das Anatomiekoordinatensystem. Der Client speichert und aktualisiert diese Matrizen. Da die Instrumente und die Adapter vom Neuronavigationssystem temporär nicht erkannt werden können (z.B. wenn Markerkügelchen verdeckt sind), wird zusätzlich ein Zeitstempel vom Empfangsdatum der jeweiligen Trackinginformationsnachricht gespeichert. Die Transformationsmatrizen während dem zweiten Teilprozess über kurze Abstände wiederholt threadsicher kopiert und weiterverwendet.

Für beide Teilprozess müssen jeweils die Bildkoordinaten der Merkmale (Löcher) bestimmt werden. Da die zu detektierenden Löcher die Wärmestrahlung durchlassen, zeigen sich die Löcher in den Bildern als helle Kreise/Ellipsen. Zur Detektion dieser Merkmale kommt ein einfacher Blobdetektor aus OpenCV zum Einsatz \cite{OCVBlobDetector2017}. Ein komplexer Detektionsalgorithmus wird nicht benötigt, da alle Merkmale ähnliche Charakteristiken besitzen. Der Detektor wandelt das Bild in mehrere Binärbilder um, in dem Pixel mit verschiedenen Schwellwerten (aus einem festen Bereich) verglichen werden. Der mittleren Schwellwert wird automatisch bestimmt, weil sich die Umgebungstemperatur verändern kann und weil das verwendete Thermopad mit der Zeit weniger Wärme abstrahlt. Der Bereich der verwendeten Schwellwerte liegt um \num{\pm20} an dem mittleren Schwellwert. Zur Bestimmung des mittleren Schwellwerts wurde der von Ridler und Calvard vorgestellte Algorithmus implementiert, der iterativ einen optimalen Schwellwert  bestimmt \cite{Ridler1978}. Der aktuelle Schwellwert segmentiert das Bild in zwei Klassen. Der neue Schwellwert ergibt sich als Mittelwert der mittleren Bildintensitäten beider Klassen. Für die so erzeugten Binärbilder werden die Zusammenhangskomponenten und ihre Zentren ermittelt. Dann werden die Zusammenhangskomponenten z.B. nach ihrer Farbe, anhand ihrerer Zirkularität und/oder anhand ihrerer Konvexität gefiltert. Die Zentren unterschiedlicher Binärbilder, deren Koordinaten in einem minimalen Abstand voneinander liegen, werden einer Gruppe zugeordnet und gemittelt. Diese so ermittelten Gruppenzentren entsprechen den Merkmalsbildpunkten der gefundenen Blobs. Da nicht nur einzelne Merkmalespunkte gefunden werden muss, sondern ein ganzes Muster, wird von den Algorithmen in OpenCV zusätzlich eine feste Ordnung der Punkte ermittelt. Diese Ordnung ergibt sich aus den Bildkoordinaten und den Dimensionen des Musters. Ist Dimension der Musterpunkte nicht quadratisch, ist diese Ordnung für einen Rotation des Kalibrationsmusters um die Kameraachse bis \SI{90}{\degree} eindeutig, da es nur eine mögliche Anordnung gibt.

Die Bestimmung der intrinsischen Parameter erfolgt mit dem Kamerakalibrierungsalgorithmus aus OpenCV. Dessen Implementierung basiert auf dem von Zhang vorgestellten Algorithmus \cite{Zhang2000} und einem MATLAB-Toolkit von Bouguet \cite{Bouguet2004}. Die Parameter werden initial geschätzt und durch die Minimierung der Summe der quadratischen Rückprojektionsfehler, mit dem Levenberg-Marquardt Optimierungsalgorithmus gesucht. Die dafür nötigen extrinsischen Parameter werden dafür aus den initial geschätzten Parameterwerten für jedes Bild mit dem Kameralagebestimmungsalgorithmus bestimmt. Der Kamerakalibrierungsalgorithmus benötigt die zugehörigen Merkmalsobjektpunkte für $n$ Bilder und die Merkmalsbildpunkte aus $n$ Bildern. Die Reihenfolge mit der die Bild- und Objektpunkten übergeben werden, bestimmt die Korrespondenzen, sie muss daher gleichbleiben. Die Bildpunkte werden pro Bild mit dem beschriebenen Schema zur Mustererkennung ermittelt. 
\begin{align}
    P_{i,j} =
    \begin{pmatrix}
        i * s_x\\
        j * s_y\\
        0
    \end{pmatrix} \label{object_points_eq}
\end{align}

Die Objektpunkte sind für alle Bilder gleich, da immer alle Merkmalspunkte sichtbar seien sollen. Sie werden mit der Gleichung \ref{object_points_eq} anhand der Geometrie des Kalibrationsmusters definiert. $P_{i,j}$ ist dabei der Objektpunkt für das Merkmal an der Gitterstelle $(i, j)$. Die Indexierung bezieht sich auf ein regelmäßiges Merkmalsgitter mit $m$ Spalten und $n$ Zeilen, wobei das Merkmal an der Stelle $(0, 0)$ oben links im Bild und das Merkmal mit der Stelle $(m - 1, n - 1)$ unten rechts im Bild ist. $s_x$ und $s_y$ sind die achsenspezifischen Ausdehnungen einer Gitterzelle in der Einheit des Objektkoordinatensystems (\si{mm}, wie im Anatomiekoordinatensystem). Damit die Merkmalspunkte ausreichend über das Bild verteilt sind, wurde eine Unterteilung des Bildraums in ein regelmäßiges Gitter implementiert. Für jede Gitterzelle muss eine feste Anzahl von Bildern vorliegen, in denen das Kalibrationsmuster in der entsprechenden Zelle enthalten ist. Ein Bild von dem Kalibrationsmuster wird dabei einer Gitterzelle zugeordnet, wenn der zugehörige mittlere Merkmalspunkt in dem rechteckigen Bereich der Zelle liegt. Für eine ausreichende Variabilität in der Orientierung und dem Abstand des Musters zur Kamera, muss selbst gesorgt werden.

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{images/calibration_pattern_ref_points}
    \caption{\label{calibration_pattern_ref_points_fgr}Auf dem Kalibrationsmuster befinden sich Referenzpunkte ($P_{OR}$, $P_{OL}$, $P_{UL}$). Mit den Anatomiekoordinaten der Referenzpunkte lassen sich alle Merkmalspunkte berechnen.}
\end{figure}

Die Bestimmung der extrinsischen Parameter erfolgt mit dem iterativen Kameralagebestimmungsalgorithmus aus OpenCV \cite{OCVPoseEstimation2017}. Die anderen in OpenCV verfügbaren Kameralagebestimmungsalgorithmen erzielten z.T. sehr unzuverlässige Ergebnisse. Ähnlich wie bei der Kamerakalibrierung werden iterativ die Parameter verbessert, indem die Summe des quadratischen Rückprojektionsfehler minimiert wird. Dazu wird ebenfalls der Levenberg-Marquardt Optimierungsalgorithmus genutzt. Es werden die Merkmalsobjektpunkte im Anatomiekoordinatensystem und die Merkmalsbildpunkte für ein Bild benötigt. Die vorher bestimmten intrinsischen Kameraparameter werden zusätzlich benötigt. Da sich alle benötigten Punkte auf einen Szenenzustand beziehen, darf sich das Kalibrationsmuster und die Kamera beim Ermitteln dieser Punkte nicht bewegen. Zu Beginn wird ein Bild aufgenommen, aus dem die Merkmalsbildpunkte, nach dem beschreiben Musterkennungsschema, extrahiert werden. Danach werden mit dem Zeigeinstrument drei co-planare Referenzpunkte auf dem Kalibrationsmuster angetippt. Diese Referenzpunkte sind in der Abbildung \ref{calibration_pattern_ref_points_fgr} dargestellt. Würde das Gitter des Kalibrationsmuster links um eine Spalte und oben um eine Zeile erweitert werden, würde der Referenzpunkt $P_{OL}$ oben links (Gitterstelle $(-1, -1)$), der Referenzpunkt $P_{OR}$ oben rechts (Gitterstelle $(m - 1, -1)$) und der Referenzpunkt $P_{UL}$ unten links (Gitterstelle $(-1, n -1)$) im Bild liegen. Anhand der Koordinaten von $P_{OL}$, $P_{OR}$ und $P_{UL}$ lassen sich die Merkmalspunkte im Anatomiekoordinatensystem berechnen.
\begin{align}
    \vec{v}_x &= \left(P_{OR} - P_{OL}\right) \frac{1}{s_x * (m + 1)} \label{pose_feature_x_coord_eq}\\
    \vec{v}_y &= \left(P_{UR} - P_{OL}\right) \frac{1}{s_y * (n + 1)} \label{pose_feature_y_coord_eq}\\
    P_{i,j} &= P_{OL} + \vec{v_x} * (i + 1) + \vec{v_y} * (j + 1) \label{pose_feature_position_eq}
\end{align}

$\vec{v}_x$ ist ein Vektor, der im Anatomiekoordinatensystem vom Merkmalsobjektpunkt an der Gitterstelle $(i,j)$ zum Merkmalsobjektpunkt an der Gitterstelle $(i + 1, j)$ zeigt. Analog ist $\vec{v}_y$ der Vektor, der im Anatomiekoordinatensystem vom Merkmalsobjektpunkt an der Gitterstelle $(i,j)$ zum Merkmalsobjektpunkt an der Gitterstelle $(i, j + 1)$ zeigt. Der Objektpunkt $P_{i,j}$ des Merkmals an der Gitterstelle $(i, j)$ lässt sich damit nach der Gleichung \ref{pose_feature_position_eq} berechnen. Zur genauen Ermittlung der angetippten Punkte, wird die Zeigerposition über einen kurzen Zeitraum gemittelt, da das Neuronavigationssystem ein zufälligen Fehler einführt. Die Mittelung der Zeigerposition wird begonnen, wenn die Zeigerspitze eine Weile still gehalten wird. Sie wird zurückgesetzt, falls zwischenzeitlich keine Positionswerte vorhanden waren oder wenn die Zeigerspitze über eine minimale Distanz bewegt wurde.

Bevor der Szenenzustand wieder verändert werden kann, wird außerdem noch die Transformationsmatrix des Kamerareferenzadapters bestimmt. Somit liegen für einen gleichen Szenenzustand eine Transformationsmatrix vom Anatomiekoordinatensystem zum Kamerakoordinatensystem und eine Transformationsmatrix vom Referenzadapterkoordinatensystem zum Anatomiekoordinatensystem vor.  Die gesuchte Transformation von Referenzadapterkoordinatensystem zum Kamerakoordinatensystem wird nach Gleichung \ref{reference_transform_eq} berechnet. Anschließend wird zur Validierung aller bestimmten Parameter, neben der Ausgabe des finalen Rückprojektionsfehlers, die projizierten Zeigerspitze im Bild dargestellt. Da die frei bewegbare Zeigerspitze und ihre ermittelte Position in den Bildern erkennbar ist, lässt sich so manuell der Fehler für verschiedene Kameraposen einschätzen.

Einige Einstellungen der Software zur Bestimmung der Kameraabbildungsmodellparameter lassen sich über eine Konfigurationsdatei verändern, um z.B. andere Kalibrationsmuster oder andere Konfigurationen zu ermöglichen. Dabei sind folgende Werte konfigurierbar: die IP und Port des Thermografie-OpenIGTLink-Servers, die IP und der Port des OpenIGTLink-Servers des Neuronavigationssystems, die gesuchten Namen des Zeigeinstruments und der Referenzadapters in den Trackinginformationsnachrichten, die Anzahl der Zeilen und Spalten des symmetrischen Merkmalsmusters, der Abstand der Merkmalspunkte in \si{mm}, der Zeitraum in dem sich das Kalibrationsmuster nicht bewegen darf, damit es als still betrachtet wird, der Zeitraum in dem nach der Verwendung eines Bilds kein weiteres Bild verwendet werden darf (Abklingzeit), die Anzahl der Zeilen und Spalten des Hilfsgitters für die Kamerakalibrierung, die Anzahl der benötigten Bilder pro Hilfsgitterzelle, die Skalierung der Abstände zwischen den Referenzpunkten, der Offset des oberen linken Zeigerpunkts vom Nullpunkt des Merkmalsgitters, die minimale Anzahl von Punkten die für den gleitenden Mittelwertsfilter benötigt werden, bis eine Zeigerposition akzeptiert wird und der Abstand im Anatomiekoordinatensystem der zur Bewertung herangezogen wird, ob sich die Zeigerspitze von ihrer ursprünglichen gemittelten Position bewegt hat.

    \section{3D Slicer Modul Camera Creator}

Aus den intrinsischen Kameraparametern, aus der konstanten Transformation von der Kamera zum Referenzadapter und aus der veränderlichen Transformation vom Referenzadapter zur Anatomie lässt sich eine virtuelle Kameraansicht definieren, die der Ansicht der Thermografiekamera entspricht. Zu diesem Zweck wurde das 3D Slicer Modul \textit{Camera Creator} implementiert. Dieses Modul  erzeugt und aktualisiert dafür einen \textit{MRML-CameraNode}, der die virtuelle Kameraansicht repräsentiert. Dazu müssen mehrere Informationen auf der grafischen Oberfläche des Moduls eingegeben werden. Der statische \textit{MRML-LinearTransformNode}, welcher die lineare Transformation vom Kamerakoordinatensystem zum Referenzadapterkoordinatensystem beinhaltet, muss ausgewählt werden. Genauso muss der \textit{MRML-LinearTransformNode}, der die kontinuierlich aktualisierte lineare Transformation von dem Referenzadapterkoordinatensystem in das Ananatomiekoordinatensystem beinhaltet, ausgewählt werden. Das externe Modul \textit{OpenIGTLinkIF} erzeugt und aktualisiert diesen \textit{MRML-LinearTransformNode} beim Erhalt entsprechender Trackinginformationsnachrichten vom OpenIGTLink-Server des Neuronavigationssystems. Außerdem müssen die Vektoren im Kamerakoordinatensystem angegeben werden, die von der Kamera nach oben und nach vorn zeigen. Zuletzt muss noch der vertikaler Sichtwinkel der Kamera angegeben werden. All diese Informationen werden verwendet, um einen auswählbaren \textit{MRML-CameraNode} entsprechend zu verändern.

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{images/slicer_camera_creator}
    \caption{Das implementierte 3D Slicer Modul \textit{Camera Creator} (rot umrandet) erzeugt eine virtuelle Kameraansicht, die der Thermografiekamera gleicht. Dazu müssen Kameraparameter, die statische Transformation von der Kamera zum Referenzadapter und die variierende Transformation vom Referenzadapter zur Anatomie eingegeben werden.}
\end{figure}

Die Spezifikation der nach oben und rechts zeigenden Vektoren von der Kamera sind dafür da, um die Kameralagebestimmung mit einem beliebig orientierten Kamerakoordinatensystem zu erlauben. OpenCV definiert das Kamerakoordinatensystem z.B. so, dass der normalisierte, nach oben zeigende Vektor die Koordinaten $(0, -1, 0)$ hat und dass der normalisierte, nach vorn zeigende Vektor die Koordinaten $(0, 0, 1)$ hat. Aus den beiden Eingabematrizen wird eine Transformationsmatrix von dem Kamerakoordinatensystem in das Anatomiekoordinatensystem berechnet. Damit werden die von der Kamera nach oben und vorn zeigenden Vektoren und die Kameraposition $(0, 0, 0)$ in das Anatomiekoordinatensystem überführt. Die so errechnete Kameraposition, die beiden Kameravektoren und der vertikale Kamerasichtwinkel wird in den aktuell ausgewählten \textit{MRML-CameraNode} übernommen. Diese Informationen werden deshalb benötigt, weil die von 3D Slicer verwendete \textit{vtkCamera} Klasse, eine Kamera anhand genau dieser Informationen spezifiziert. Wenn alle Daten vorhanden sind, wird der \textit{MRML-CameraNode} immer dann aktualisert, wenn entweder ein entsprechender Button gedrückt wird oder wenn sich eine der Transformationen aus den MRML-LinearTransformNodes geändert hat. Der vertikaler Kamerasichtwinkel $FOV_h$ lässt sich aus der y-achsenspezifischen Brennweite $\alpha_y$, der y-Koordinate des Hauptpunkts $\underline{c}_y$ und der Bildhöhe $h$ berechnen.
\begin{align}
    FOV_v = arctan2 \left(\underline{c}_y, \alpha_y\right) + arctan2 \left(h - \underline{c}_y, \alpha_y\right) \label{vertical_fov_eq}
\end{align}

Die dafür benötigten Parameter werden alle von der Software zur präoperativen Bestimmung der Abbildungsmodellparameter geliefert.

    \section{3D Slicer Modul Projective Texture Mapping}

Die Erzeugung der projektiven Thermografievisualisierung wird in dem 3D Slicer Modul \textit{Projective Texture Mapping} implementiert. Zur Erzeugung einer neuen Visualisierung, welche bisher nicht in 3D Slicer und VTK enthalten ist, musste unter anderem ein neuer Typ von \textit{MRML-DisplayNodes} implementiert werden. Dieser \textit{MRML-DisplayNode} enthält alle nötigen Informationen/Referenzen, welche für eine solche Visualisierung nötig sind. All diese Informationen können in der grafischen Oberfläche des Moduls eingeben werden. So muss ein \textit{MRML-ModelNode} ausgwählt werden, der die bemessene Oberfläche als Dreiecksnetz beinhaltet. Weiterhin muss ein \textit{MRML-VolumeNode} ausgewählt werden, der die kontinuierlich aktualisierte Daten der Thermografiekamera beinhaltet. Ein \textit{MRML-ColorNode}, der eine Farbtabelle für die Abbildung der Temperaturen auf Farbwerte beschreibt, muss außerdem ausgewählt werden. Der vom Modul \textit{Camera Creator} erstellte \textit{MRML-CameraNode}, der die Ansicht der Thermografiekamera in der Szene beschreibt, muss ebenfalls ausgewählt werden. Der z-Toleranzwert $\varepsilon$, der im Algorithmus \ref{thermo_vis_fragment_code} beschrieben wurde, kann ebenfalls eingegeben und verändert werden. Zuletzt kann die Sichtbarkeit der Visualisierung an- oder ausgestaltet werden. All diese Informationen werden in einem \textit{MRML-ProjectiveTextureMappingDisplayNode} gespeichert. Auf der grafischen Oberfläche des Moduls kann entweder ein solcher \textit{MRML-Node} erstellt werden oder ein solcher bestehender \textit{MRML-Node} kann zur Modifikation ausgewählt werden.

Die Visualisierung werden mit VTK in Form von \textit{Mappern} realisiert, welche die Eingangsdaten, wie z.B. regelmäßige Gitternetze, auf grafische Primitive abbilden, die in ein beliebigen Fenster gerendert werden können. Da der Visualisierungsansatz für das projektive Texture Mapping noch nicht in VTK vorhanden war, musste eine neue \textit{Mapper} Klasse implementiert werden. Dieser \textit{Mapper} basiert auf der bestehenden OpenGL-basierten \textit{Mapper} Klasse für polygonale Netze, die Polygone auf grafische Primitive abbildet \cite{VTK2017}. Dies Klasse erzeugt je nach Einstellung und Eingangsdaten passenden Shadercode und setzt beim Rendern die entsprechenden globalen Uniformvariablen. Die Erzeugung des OpenGL-Shadercodes basiert in VTK auf einem Templatesystem. Der initial Shadercode besteht fast nur aus Templatenamen, die von der \textit{Mapper} Klasse in mehreren Durchläufen ersetzt wird. Der selbst implementiert \textit{Mapper} erbt von der bestehenden \textit{Mapper} Klasse und verändert nur einige dieser Templates. Dadurch bleiben die Teile des Shadercodes erhalten, die für den projektiven Texture Mapping-Ansatz nicht verändert werden müssen. Die neue \textit{Mapper} Klasse führt hauptsächlich Änderungen an den Templates des Fragmentshaders durch, welche die Texturkoordinaten berechnen. Des weiteren wurden neue globale Uniformvariablen für die Parameter des Kameraabbildungsmodell und für die Tiefentextur, die aus der Sicht der Thermografiekamera gerendert wird, zur Berechnung der Texturkoordinaten hinzugefügt. Außerdem wurde ein neuer \textit{DisplayableManager} implementiert, der die \textit{MRML-Szene} und alle darin enthaltenen \textit{MRML-ProjectiveTextureMappingDisplayNodes} beobachtet. Bei Veränderungen an einem solchen \textit{MRML-Node}, werden entsprechende VTK-Objekte erzeugt/gelöscht/aktualisiert, welche die passende Visualisierung erzeugen. So wird nicht nur der neue \textit{Mapper} erzeugt, sondern auch noch eine untergeordnete Rendering Pipeline, welche die Tiefentextur rendert, wenn es nötig ist. Das neue \textit{Projective Texture Mapping} Modul lässt sich für die Rückprojektion arbiträrer zweidimensionaler Bilder auf beliebige Oberflächenmodelle einsetzen, das war mit 3D Slicer bisher nicht möglich.

    \section{3D Slicer Modul Image Fusion}

Die kombinierte MRT- und Thermografievisualisierung wird mithilfe eines Bildfusionsalgorithmus in dem 3D Slicer Modul \textit{Image Fusion} implementiert. Da auch diese Visualisierung weder in 3D Slicer noch VTK vorhanden war, musste unter anderem ein weiterer neuer Typ von \textit{MRML-DisplayNodes} implementiert werden. Genau wie der \textit{MRML-ProjectiveTextureMappingDisplayNode}, muss dieser neue \textit{MRML-DisplayNode} alle nötigen Informationen/Referenzen enthalten, die für eine solche kombinierte Visualisierung nötig sind. Die notwendigen Informationen können auf der grafischen Oberfläche des Moduls eingeben werden. Da die Visualisierung auf den Ergebnissen zweier anderer Visualisierung beruht, müssen dafür passende \textit{MRML-DisplayNodes} ausgewählt werden. Das betrifft einen \textit{MRML-VolumeRenderingDisplayNode}, der die Informationen über die MRT-Visualisierung als Volumengrafik beschreibt. Und es betrifft den \textit{MRML-ProjectiveTextureMappingDisplayNode}, der mit dem Modul \textit{Projective Texture Mapping} erzeugt wurde und der die Informationen über die projizierte Thermografievisualisierung beschreibt. Außerdem kann die Art des verwendeten Bildfusionsalgorithmus ausgewählt werden. Der primär verwendete Bildfusionsalgorithmus der in dem Modul implementiert wird, ist, der von Li \textit{et al.} vorgestellte, der auf geführter Bildweichzeichnung basiert \cite{Li2013}. Zum Vergleich wurde desweiteren ein Bildfusionsalgorithmus implementiert,  der auf einer einfachen, gewichteten Mittelwertsbildung basiert. Diese Informationen werden mit einem \textit{MRML-ImageFusionDisplayNode} beschrieben. Auf der grafischen Oberfläche des Moduls kann entweder ein \textit{MRML-ImageFusionNode} erstellt werden oder ein bereits bestehender \textit{MRML-ImageFusionNode} kann zur Modifikation ausgewählt werden.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=\textwidth]{images/MRI_slicer}
        \caption{MRT-Volumengrafikbild}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=\textwidth]{images/Thermography_slicer}
        \caption{Projiziertes Thermografiebild}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=\textwidth]{images/Thermography_and_MRI_Slicer}
        \caption{Fusioniertes Bild}
    \end{subfigure}
    \caption{\label{slicer_image_fusion_fgr}Das implementierte 3D Slicer Modul \textit{Image Fusion} erzeugt fusionierte MRT- und Thermografiebilder aus Bildern der MRT-Volumengrafik und der Thermografievisualisierung. Die Thermografiedaten wurden künstlich erzeugt, um den Effekt zu verdeutlichen.}
\end{figure}

Zunächst wurde die Bildfusionsalgorithmen mit VTK als eine in die Verarbeitungspipeline einbindbare Algorithmenklasse implementiert. Diese Klasse nimmt zwei RGBA-Bilder als Eingabe und berechnet ein fusioniertes RGB-Ausgabebild. Dies geschieht nur, wenn das Ausgabebild benötigt wird und wenn das vorher berechnete Ausgabebild älter als eines der Eingabebilder ist. Damit die Bildstellen, an denen nur ein Eingabebild nicht-transparente Farbwerte aufweist, nicht das Bildfusionsergebnis beeinflussen, werden die Eingabebilder vor der Anwendung des eigentlichen Bildfusionsalgorithmus manipuliert. Dies ist z.B. der Fall wenn die MRT-Visualisierung sichtbare Teile des Gehirns darstellt, die nicht in der projizierten Thermografievisualisierung dargestellt werden. Zur Manipulation der Eingabebilder wird dafür \textit{Alpha Blending} eingesetzt. Dazu wird zuerst ein Hintergrund mit einem nicht transparenten Farbwert mit einem der beiden Eingabebilder überlagert. Danach wird dieses erzeugte Bild mit dem anderen Eingabebild überlagert. Die so erzeugten neuen Eingabebilder sehen so aus, als ob der Bildgegenstand des jeweils andere ursprüngliche Eingabebilds im Hintergrund liegt. Daher zeigen die manipulierten Eingabebilder für den eingesetzten Anwendungsfall, einmal die MRT-Volumengrafik vor der projizierten Thermografieoberfläche und einmal die projizierte Thermografieoberfläche vor der MRT-Volumengrafik. Damit die damit verbundene Vorder- und Hintergrundrelationen valide sind, muss ein dargestelltes Objekt in allen Bildstellen vor dem jeweils anderen dargestellten Objekt liegen. Für die dargestellte Gehirnoberfläche ist das zumindest aus den meisten Ansichten der Fall. Weil die so erzeugten Bilder wegen dem undurchsichtigen Hintergrund keine Transparenz mehr enthalten, wird ihr Alphakanal verworfen. Die manipulierte RGB-Eingabebilder werden mit der in den Gleichungen \ref{rgb_to_h_eq}, \ref{rgb_to_s_eq} und \ref{rgb_to_v_eq} beschrieben Farbraumtransformation in den HSV-Farbraum überführt. Die Helligkeitskanäle der beiden Bilder (V-Kanal) werden mit dem eingestellten Bildfusionsalgorithmus fusioniert. Die dafür notwendigen Bildverarbeitungsschritte wurden mithilfe von Funktionen aus OpenCV implementiert. Dann wird der fusionierter Luminazkanal und der Farbton- und Sättigungskanal des manipulierten Thermografieeingabebilds in ein neues HSV-Bild kombiniert. Mit der in der Gleichung \ref{hsv_to_rgb_eq} beschrieben Farbraumtransformation wird das finale, fusionierte Bild im RGB-Farbraum erzeugt. Eine Beispiel der Ergebnisse der Bildfusion ist in der Abbildung \ref{slicer_image_fusion_fgr} dargestellt.

Zusätzlich wurde ein neuer \textit{DisplayableManager} implementiert, der die \textit{MRML-Szene} und alle darin enhalteten \textit{MRML-ImageFusionDisplayNodes} beobachtet. Bei Veränderungen an solchen \textit{MRML-Nodes}, werden entsprechende VTK-Objekte erzeugt/gelöscht/aktualisiert, welche die jeweils passende kombinierte Visualisierung erzeugen. Für jeden \textit{MRML-ImageFusionDisplayNode} werden passende \textit{Mapper} der zu fusionierenden Visualisierungen aus dem gemeinsamen Zustand aller \textit{DisplayableManager} gesucht. Die Ausgabebilder dieser \textit{Mapper} werden jeweils als Eingabe in den Bildfusionsalgorithmus umgeleitet. Das fusionierte Ausgabebild  wird auf einer 2D-Ebene im Vordergrund der Szene angezeigt, die das komplette Fenster der 3D-Ansicht von 3D Slicer ausfüllt. Die Steuerung der Kamera mit der Maus bleibt dadurch unbeeinflusst. Das implementierte \textit{Projective Texture Mapping} Modul lässt sich potentiell für die Fusion von beliebigen zweidimensionalen Bildern erweitern. Ebenso wäre es möglich, das Modul dahingehend abzuändern, dass es die Ausgaben beliebiger anderer Visualisierungsmethoden miteinander fusioniert. Eine kombinierte Darstellung von projektiven 2D-Daten und 3D-Gitterdaten war in dem vorgestellten Umfang bisher in 3D Slicer noch nicht möglich.

% --------------------------------------------------------------------------- %
\chapter{Evaluation}

Ein Hauptkriterium der wissenschaftlichen Visualisierungen ist die Ausdrucksfähigkeit der erzeugten Bilder. Demnach muss gewährleistet sein, dass die Bilder keine falsche Aussagen über die dargestellten Daten suggerieren. Für die vorgeschlagene, fusionierte Visualisierung von MRT- und Thermografiedaten, hängt die Erfüllung dieses Kriteriums von der Übereinstimmung der beiden Beobachtungsräume und von dem Wahrheitsgehalt der vermittelten Informationen über die dargestellten Merkmalsräume ab. Ob und wie stark die erste Bedingung erfüllt ist, wird von dem Registrierungfehler und von dem Unterschied zwischen der extrahierten und der realen Oberfläche bestimmt. Die zweite Bedingung ist gemäß dem Verwendungszweck der Daten erfüllt, wenn die farbkodierten Temperaturwerte immer eindeutig bestimmbar sind und wenn alle strukturellen Merkmale korrekt dargestellt werden. Ein weiteres Kriterium einer wissenschaftlichen Visualisierung ist die Effektivität der Bilder in Bezug auf die menschliche Wahrnehmung. Die erzeugten Bilder müssen daher alle relevanten Informationen leicht interpretierbar darstellen. Dieses Kriterium ist dann erfüllt, wenn diese Informationen in allen Anwendungsfällen schnell erkennbar sind. Das letzte Kriterium einer wissenschaftlicher Visualisierung bildet die Effizienz der Bildererstellung, die für den vorgegeben Anwendungsfall angemessen sein muss. Dieses Kriterium ist erfüllt, wenn die Bildwiederholrate der Visualisierung für die intraoperative Nutzung geeignet ist. Ob die vorgestellte Visualisierung den Anforderungen einer wissenschaftlichen Visualisierung genügt, ist nur ein wichtiger Aspekt. Eine der mögliche Anwendungen für die Visualisierung ist die Verwendung als eine intraoperative Entscheidungshilfe zur Klassifikation von Gehirngewebe. Werden die daraus resultierenden Entscheidungen negativ beeinflusst, könnte das lang anhaltende Folgen für die zu behandelnde Person haben, wenn z.B. statt Tumorgewebe, funktionstragendes Gehirngewebe entfernt wird. Daher ist es von enormer Bedeutung das vorgestellte Visualisierungsframework hinsichtlich der genannten Kriterien zu untersuchen, damit die Sicherheit über die vermittelnden Informationen quantifizierbar und bewertbar ist.

Um mögliche Einflüsse auf die Übereinstimmung der Beobachtungsräume zu finden, wurden zunächst verschiedene präoperative Teilschritte untersucht. Dazu wurde ermittelt unter welchen Bedingungen und wie korrekt die Kamerakalibrierung die intrinsischen Kameraparameter bestimmt. Außerdem wurde analysiert unter welchen Bedingungen und wie zuverlässig die Kameralagebestimmung die initialen extrinsischen Kameraparameter bestimmt. Die gegebene quantifiziert Genauigkeit des verwendeten Neuronavigationssystems wurde angegeben. Unter den daraus ermittelten optimalen Bedingungen, wurde ein finaler Registrierungsfehler bestimmt. Zuletzt wurde der Fehler zwischen den aus den MRT-Daten extrahierten Oberflächenpunkten und den realen, von der Thermografiekamera abgebildeten Oberflächenpunkten approximiert.

Damit geklärt ist, ob die vorgestellte Visualisierung angemessen für die intraoperative Nutzung ist, wurde die Bilderstellungsdauer der gesamten Visualisierung und ihrer Teilschritte gemessen. Für die Bewertung des Wahrheitsgrades der dargestellten Informationen, wurde diskutiert ob die Temperaturen und die strukturellen Merkmale korrekt, eindeutig und vollständig dargestellt werden. Zur Feststellung potentieller Wahrnehmungsprobleme der Visualisierung, wurden mögliche Vor- und Nachteile der Visualisierung für unterschiedliche Anwendungsaufgaben erörtert.

    \section{Kamerakalibrierung}

Vor der Evaluation der Kamerakalibrierung wurde zunächst betrachtet, ob der Hauptpunkt der Kamera in der Bildmitte liegt und es wurden minimale und maximale Werte für die Brennweite des eingesetzten Objektivs mit dem verwendeten Fokusring ermittelt. Dazu wurde die Thermografiekamera senkrecht über eine ebene Oberfläche angebracht, sodass der sichtbare Bildauschnitt komplett von der Oberfläche gefüllt war. Dann wurden mit einem Gliedermaßstab die Längen verschiedener Strecken gemessen. Die Höhe des Bildausschnitt $h_i$ war demnach \SI{6,5}{cm} groß und die Breite des Bildausschnitts $w_i$ war \SI{8,2}{cm} groß. Der Abstand von der Oberfläche bis zur Objektivmitte $d$ betrug \SI{40}{cm}. Die senkrechte Verlängerung durch die Objektivmitte zeigte dabei auf die Mitte des betrachteten Oberflächenausschnitts. Demnach muss der Hauptpunkt sich auf/nahe der Bildmitte (\SI{320}{px}, \SI{240}{px}) befinden. Diese Beobachtung deckt sich auch mit den Sichtwinkeln und der Brennweite des initial verwendeten Objektivs, die vom Hersteller angegeben werden. Würde der Hauptpunkt außerhalb der Bildmitte liegen, würden die Sichtwinkel nach Gleichung \ref{vertical_fov_eq} kleiner ausfallen.
\begin{align}
    \frac{f}{w_s} = \frac{d}{w_i} & \text{ und } \frac{f}{h_s} = \frac{d}{h_i}\label{focal_length_aprox_eq}
\end{align}

Über die Verhältnisgleichungen \ref{focal_length_aprox_eq} lässt sich die Brennweite $f$ approximieren. $w_s$ ist dabei die Sensorbreite und $h_s$ die Sensorhöhe. Je nach dem, ob die gemessene Breite oder Höhe des Bildausschnitts verwendet wird, beträgt die approximierte Brennweite entweder \SI{74}{mm} oder \SI{78}{mm}. Wird für die Höhe und Breite des Bildauschnitt \SI{1}{cm} und für den Abstand zum Objektiv \SI{2}{cm} als Messfehler angenommen, kann die Brennweite minimal \SI{61}{mm} und maximal \SI{93}{mm} lang sein. Das deckt sich mit dem Fakt, dass der eingesetzte Fokusring die normale Brennweite des verwendeten Objektivs von \SI{60}{mm} um eine feste Distanz erhöht.

Bei der Kamerakalibrierung soll der Einfluss von den verwendeten Kalibrationsmusterposen auf die ermittelte Brennweite und den ermittelten Hauptpunkt bestimmt werden. Weil die tatsächlichen Merkmalspositionen, aufgrund der einfachen Fertigung des Kalibrationsmusters, um bis zu \SI{1}mm von ihren angenommen Positionen abweichen, wurden die ermittelten Koeffizienten des Verzerrungsmodells nicht untersucht. Mögliche Aussagen über diese Koeffizienten könnten so nicht zuverlässig auf den Kamerakalibrierungsprozess zurückgeführt werden. Außerdem liegt keine Grundwahrheit über die Verzerrungskoeffizienten vor, sodass sie nicht einfach hinsichtlich ihrer Korrektheit bewertet werden können und das verwendete Objektiv weißt auch keine unmittelbar sichtbaren Verzerrungseffekte, wie z.B. eine tonnenförmige Verzerrung, auf. Die Brennweite und die Hauptpunktkoordinaten lassen sich aber mit dem zuvor ermittelten Vorwissen sinnvoll einordnen. Anhand von zwei unabhängigen Faktoren wurden die in den Experimenten verwendeten Posen des Kalibrationsmusters festgelegt. Der erste Faktor bestimmt ob das Kalibrationsmuster mit mehreren unterschiedlichen Winkeln (\SIrange{0}{30}{\degree}, Fall $W$) oder mit einem gleichbleibendem Winkel (\SI{0}{\degree}, Fall $\lnot W$) zur Kamera ausgerichtet wird. Der zweite Faktor bestimmt, ob das Kalibrationsmuster mit variierender Distanz (\SIrange{40}{60}{cm}, Fall $D$) oder mit gleichbleibender Distanz (\SI{60}{cm}, Fall $\lnot D$) zur Kamera positioniert wird. Die Kamerakalibrierung wurde experimentell für jede Faktorenkombination mehrfach durchgeführt ($n = 10$). Pro Durchgang wurden jeweils 12 Thermografiebilder des Kalibrationsmusters in den jeweiligen Posen und an unterschiedlichen Bildpositionen erzeugt.

\begin{table}[h]
    \centering
    \caption{\label{calib_free_pp_tbl}\textbf{Ergebnisse der Kamerakalibrierung} in Abhängigkeit, ob die Distanz des Kalibrationsmusters variiert wurde (ja $D$, nein $\lnot D$) und ob die Winkel des Kalibrationsmusters variiert wurden (ja $W$, nein $\lnot W$). Aufgelistet sind die statistischen Kenngrößen (Mittelwert $\pm$ Standardabweichung) der Brennweite $f$, der Hauptpunktkoordinaten $(\underline{c}_x, \underline{c}_y)$ und des mittleren quadratischen Rückprojektionsfehlers $e_{rms}$.}
    \begin{tabular}{r|l|l}
    	         \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\fm {\lnot D}}     & \multicolumn{1}{c}{\fm D}                \\ \hline
    	                               & $f = \SI{109,8\pm16,4}{mm}$            & $f = \SI{96,2\pm77,1}{mm}$               \\
    	                               & $\underline{c}_x = \SI{437\pm61}{px}$  & $\underline{c}_x = \SI{387\pm29}{px}$    \\
    	                               & $\underline{c}_y = \SI{227\pm44}{px}$  & $\underline{c}_y = \SI{96,2\pm77,1}{px}$ \\
    	\multirow{-4}{*}{\fm{\lnot W}} & $e_{rms} = \SI{0,82\pm0,01}{px}$       & $e_{rms} = \SI{0,92\pm0,02}{px}$         \\ \hline
    	                               & $f = \SI{78,2\pm7,8}{mm}$              & $f = \SI{77,3\pm4,8}{mm}$                \\
    	                               & $\underline{c}_x = \SI{325\pm101}{px}$ & $\underline{c}_x = \SI{346\pm72}{px}$    \\
    	                               & $\underline{c}_y = \SI{227\pm44}{px}$  & $\underline{c}_y = \SI{279\pm115}{px}$   \\
    	       \multirow{-4}{*}{\fm W} & $e_{rms} = \SI{0,90\pm0,01}{px}$       & $e_{rms} = \SI{0,92\pm0,01}{px}$
    \end{tabular}
\end{table}

Die statistischen Kenngrößen der Brennweite, der x-/y-Koordinate des Hauptpunkts und des finalen mittleren quadratische Rückprojektionsfehlers sind als Ergebnis der Versuchsreihe in Tabelle \ref{calib_free_pp_tbl} aufgelistet. Die Daten zeigen, dass die Kamerakalibrierung ohne Winkel- und Distanzvariation nutzlos ist, da sie immer falsche Ergebnisse für die Brennweite liefert. Für diese Fallkombination liegt die untere Grenze des 95\%-Konfidenzintervalls (\SI{99,6}{mm}) der Brennweite über dem maximal möglichen Wert von \SI{93}{mm}. Eine Kamerakalibrierung ohne Winkelvariation und mit einer Distanzvariation liefert sehr unzuverlässige Ergebnisse für die Brennweite, denn die Standardabweichung der ermittelten Brennweiten beträgt \SI{77,1}{mm}. Diese Art der Kalibrationsmusterposen ist daher eben so ungeeignet. Die beiden anderen Fallkombination liefern plausible und zuverlässige Werte für die Brennweite. Die Mittelwerte der Hauptpunktkoordinaten erscheinen ebenso plausibel (nahe bei (\SI{320}{px}, \SI{240}{px})), allerdings ist die Standardabweichung der Koordinatenwerte noch sehr groß.
\begin{table}[h]
    \centering
    \caption[]{\label{calib_fixed_pp_tbl}\textbf{Ergebnisse der Kamerakalibrierung}, wenn der Hauptpunkt auf die Bildmitte fixiert wird. Aufgelistet sind die statistischen Kenngrößen (Mittelwert $\pm$ Standardabweichung) der Brennweite $f$ und des mittleren quadratischen Rückprojektionsfehlers $e_{rms}$ für die selbe Messreihe wie in Tabelle \ref{calib_free_pp_tbl}}
    \begin{tabular}{r|c|c}
    	                      &  $f$ (in \si{mm})   & $e_{rms}$ (in \si{px}) \\ \hline
    	\fm{\lnot D \wedge W} & $\num{92,6\pm11,4}$ &  $\num{0,93\pm0,02}$   \\ \hline
    	      \fm{D \wedge W} & $\num{82,9\pm4,2}$  &  $\num{0,93\pm0,01}$
    \end{tabular}

\end{table}

Deswegen wurde die Berechnung der Kameraparameter für die verbleibenden Fallkombinationen mit den selben Daten und einem festgesetzten Hauptpunkt bei (\SI{320}{px}, \SI{240}{px}) wiederholt, die resultierenden Kenngrößen der Brennweiten und der mittleren quadratischen Rückprojektionsfehler sind in der Tabelle \ref{calib_fixed_pp_tbl} aufgeführt. Die Kamerakalibrierung mit der Winkelvariation und ohne die Distanzvariation erzeugte dabei deutlich weniger plausible Werte für die Brennweite, denn im Mittel liegt die ermittelte Brennweite nahe an der oberen Grenze von \SI{93}{mm}. Daher ist diese Art der verwendeten Kalibrationsmusterposen unter Verwendung eines fixen Hauptpunkts nicht verwendbar. Die Kamerakalibrierung mit der Winkel- und Distanzvariation liefert bei einem fixen Hauptpunkt plausible Werte für die Brennweite. Die resultierende Standardabweichung sinkt sogar im Vergleich mit der freien Wahl der Hauptpunktkoordinaten. Die mittleren quadratische Rückprojektionsfehler weichen, bis auf einen Fall, nicht bedeutend voneinander ab. Der Fall bei dem keine Winkel- und Distanzvariation angewandt hat im Vergleich mit den anderen Fällen, einen deutlich niedrigeren mittleren quadratischen Rückprojektionsfehler. Das liegt vermutlich daran, dass sich die zugehörigen Merkmalsbildpunkte einfacher mit beliebigen intrinsischen Kameraparametern beschreiben lassen. So lässt sich für diesen Fall z.B. eine parallel zur Bildebene verschobenes Muster entweder durch einen verschoben Hauptpunkt oder durch eine Verschiebung der Kamera erklären. Dies verdeutlicht, dass der Rückprojektionsfehler der verwendeten Merkmalspunkte kein Garant über die Korrektheit der Kamerakalibrierung gibt. Stattdessen fungiert er nur als ein Maß der Anpassungsgüte und er beschreibt wie gut sich die beobachteten Merkmalspunkte mit den ermittelten Parametern erklären lassen.

Für eine korrekte Kamerakalibrierung mit zuverlässigen Ergebnissen muss daher zwischen mehreren Aufnahmen der Winkel des Kalibrationsmuster zur Kamera und die Distanz zwischen dem Muster und der Kamera verändert werden. Eine Verbesserung der Zuverlässigkeit der Kamerakalibrierung ist bis zu einem gewissen Grad vorstellbar, wenn die Anzahl der verwendeten Bilder erhöht wird oder wenn die Fertigungsgenauigkeit des Kalibrationsmusters verbessert wird. Dies wurde aber aufgrund des dafür notwendigen, höheren Zeitaufwands und dem zur Verfügung stehenden Fertigungsgerät (einfache, elektrische Lochbohrmaschine) nicht experimentell bestätigt. Die intrinsischen Kameraparameter die in den weiteren Versuchsreihen werden, wurde mit aus 36 Bildern ermittelt. Der Hauptpunkt liegt dabei fix in der Bildmitte und es wurde die vorgeschlagenen Variation der Kalibrationsmusterposen eingesetzt.

    \section{Kameralagebestimmung}

Bei der Kameralagebestimmung soll der Einfluss von der verwendeten Merkmalsobjektpunktbestimmung auf die ermittelten Rotationswinkel und die ermittelten Translationskoordinaten bestimmt werden. Weil die Objektpunkte der Merkmale mit verschieden Zeigerpositionen des Zeigerinstruments berechnet werden, wurde dabei untersucht wie das Zeigerinstrument zum Einsatz kommt. Dazu wird unterschieden, ob das Zeigerinstrument während der Mittelung der Zeigerpositionen mit einer Schwenkbewegung um den Zeigerpunkt rotiert wird (Pivotierbewegung, Fall $P$) oder ob es dabei still gehalten wird (Fall $\lnot P$). Das Kalibrationsmuster lag dabei auf einem Tisch, der die Bewegung des Musters an zwei Rändern einschränkt. Die Kameralagebestimmung wurde bei einer gleichbleibenden Kamerapose für jeden Fall mehrfach durchgeführt ($n = 10$).
\begin{table}[h]
    \centering
     \caption{\label{pose_tbl}\textbf{Ergebnisse der Kameralagebestimmung} in Abhängigkeit, von der Bewegung des Zeigerinstruments bei der Bestimmung der Referenzpunktkoordinaten des Kalibrationsmusters (Pivotbewegung $P$, Stillhalten $\lnot P$). Aufgelistet sind die Standardabweichungen der eulerschen Rotationswinkel in x-, y-, z-Achse $\beta_x$, $\beta_y$, $\beta_z$ und der Translationskoordinaten $\vec{t}_x$, $\vec{t}_y$, $\vec{t}_z$}
    \begin{tabular}{r|c|c|c|c|c|c}
    	             & $\beta_x$ (in \si{\degree}) & $\beta_y$ (in \si{\degree}) & $\beta_z$ (in \si{\degree}) & $\vec{t}_x$ (in \si{mm}) & $\vec{t}_y$ (in \si{mm}) & $\vec{t}_z$ (in \si{mm}) \\ \hline
    	\fm{\lnot P} &         \num{0,97}          &         \num{1,37}          &         \num{0,17}          &     \num{8,11}     &     \num{2,50}     &     \num{9,37}     \\ \hline
    	       \fm P &         \num{1,20}          &         \num{0,96}          &         \num{0,09}          &     \num{10,2}     &     \num{1,34}     &     \num{5,37}
    \end{tabular}
\end{table}

Die statistischen Kenngrößen der eulerschen Rotationswinkel um die x-, y- und z-Achse ($\beta_x$, $\beta_y$, $\beta_z$), der x-, y- und z-Koordinaten des Translationsvektors ($t_x$, $t_y$, $t_z$) und der mittleren quadratischen Rückprojektionsfehler sind als Ergebnis der Versuchsreihe in Tabelle \ref{pose_tbl} aufgelistet. Die Mittelwerte sind dabei irrelevant, da die extrinsischen Kameraparameter für unterschiedliche Kameraposen variieren und keine Grundwahrheit über die verwendete Kamerapose bekannt ist. Das Pivotieren des Zeigerinstruments anstatt des Stillhaltens verbessert die Standardabweichung der Rotationswinkel um die y- und z-Achse relativ stark und verschlechtert die Standardabweichung des Rotionswinkels um die x-Achse relativ leicht. Diese achsenspezifische Asymmetrie tritt auch bei dem Translationsvektor auf. Beim Vergleich der Pivotbewegung mit keiner Bewegung verbessert sich die Standardabweichung der y- und z-Koordinate des Translationsvektors relativ stark und die Standardabweichung der x-Koordinate des Translationsvektors verschlechtert sich relativ leicht. Die x-Achse des Anatomiekoordinatensystems lag bei der Versuchsreihe parallel zu den Objektiven der Stereokamera und die y-Achse zeigte leicht angewinkelt in die Richtung der Stereokameraachse. Die Pivotbewegung reduziert vermutlich den Einfluss von einem systematischen Fehler der Zeigerposition. Der Teil des systematischen Fehlers der in der Rotationsebene der Bewegung liegt, würde dabei durch die Mittelung der Zeigerposition kompensiert werden. Wenn das Zeigerinstrument still gehalten wurde, lag der mittlere quadratische Rückprojektionsfehler $e_{rms}$ bei \SI{1,34\pm0,03}{px}. Beim Pivotieren der Zeigerspitze betrug der Fehler \SI{1,32\pm0,02}{px}. Beide Fälle unterscheiden sich nicht bedeutend in der Anpassungsgüte der ermittelten extrinsischen Kameraparameter. Allerdings ist der mittlere quadratische Rückprojektionsfehler nach der Kameralagebestimmung bemerkenswert größer (um ca. \SI{0,4}{px}), als bei der Kamerakalibrierung. Absolut betrachtet erscheint die Streuung aller Werte relativ hoch, anhand der Versuchsreihe wäre ein Fehler von mehr als \SI{1}{cm} für die Distanz zwischen dem geschätzten und dem echten Translationsvektor möglich.

Um die Zuverlässigkeit der Kameralagebestimmung zu verbessern, empfiehlt es sich während der Mittelung der Zeigerpositionen das Zeigerinstrument zu pivotieren. Aufgrund der absoluten Streuung der Werte kann das Verwenden von nur einem Ergebnis der Kameralagebestimmung, potentiell zu einer vergleichsweise unpräzisen Registrierung führen. Daher ist es wichtig, dass der Registrierungsfehler nach jeder Kameralagebestimmung ausgewertet wird. Fällt der Fehler dabei zu groß aus, muss die Kameralagebestimmung wiederholt werden. Es ist vorstellbar, dass die Zuverlässigkeit der Kameralagebestimmung weiter verbessert werden kann, wenn das Kalibrationsmuster fest eingespannt werden kann. Für die Versuchsrahe gab es allerdings keine Möglichkeit für eine statische Befestigung einer entsprechenden Einspannungsvorrichtung/Verankerung. Eine genauere Fertigung des Kalibrationsmusters könnte die Zuverlässigkeit der Kameralagebestimmung ebenso erhöhen. Da für die Kameralagebestimmung das Neuronavigationssystem genutzt wird, spielt außerdem die Genauigkeit mit der das System das Zeigerinstrument trackt eine Rolle. Weicht dadurch z.B. die Geometrie der ermittelten Objektpunkte von der tatsächlichen ab, würde das z.T. die Vergrößerung des mittleren quadratischen Rückprojektionsfehlers gegenüber der Kamerakalibrierung erklären.

    \section{Trackinggenauigkeit des Neuronavigationssystems}

In der Vorarbeit von Hoffmann \textit{et al.} \cite{Hoffmann2017} wurde der Fehler zwischen der übermittelten und der tatsächlichen Zeigerposition des Neuronavigationssystems untersucht. Dazu wurde zunächst eine Registrierung mit dem Phantom durchgeführt. Dann wurde mit dem Zeigerinstrument auf die Positionen der angebrachten Fiducials gezeigt. Die übermittelten Koordinaten der getrackten Zeigerspitze wurden dabei mit den manuell definierten Koordinaten der Fiducials in den MRT-Aufnahmen verglichen. Da die Fiducialpositionen anhand der MRT-Aufnahmen definiert werden, hängt die Genauigkeiten ihrer Positionen von der Genauigkeit des verwendeten Kernspintomographen ab. Laut Herstellerangaben beträgt der Fehler der MRT-Daten maximal \SI{2}{mm} auf der x-Achse und maximal \SI{0.98}{mm} auf der y- und z-Achse. Bei der eigentlichen Versuchsreihe wurden ebenfalls achsenspezifische Fehler ermittelt, die jeweils durch den Abstand zwischen der übermittelten Zeigerposition und der bekannten Fiducialposition beschrieben werden. Der Fehler betrug für die x-Koordinate \SI{0.37\pm0.31}{mm}, für die y-Koordinate \SI{0.43\pm0.31}{mm} und für die z-Koordinate \SI{0.99\pm0.79}{mm}

Die Größe dieses so ermittelten Fehlers hängt dabei, neben der Genauigkeit der MRT-Aufnahme, von zwei Faktoren ab, die durch das Neuronavigationssystem gegeben sind. Zum einen wird der Fehler von der Genauigkeit der Registrierung von dem Szenenkoordinatensystems auf das Anatomiekoordinatensystem beeinflusst. Außerdem hängt der Fehler davon ab, wie zuverlässig das Neuronavigationssystem die Position der Markerkügelchen bestimmen kann. In dieser Arbeit wird das Neuronavigationssystem unter anderem verwendet, um die Merkmalspositionen des Kalibrationsmusters im Anatomiekoordinatensystem zu bestimmen. Nach den Gleichungen \ref{pose_feature_x_coord_eq} und \ref{pose_feature_y_coord_eq} verteilt sich der beschriebene Zeigerpositionsfehler auf alle Merkmalspunkte.
    
    \section{Finale Registrierungsgenauigkeit}

Zur Bestimmung der intraoperativen Registrierungsgenauigkeit wurde das Projektionsverhalten von bekannten Punkten des Phantoms ausgewertet. Dazu wurde die Kamera mit einem Abstand von \SI{40}{cm} über dem Phantom angebracht. Die Kamerachse stand dabei zunächst senkrecht zu der sichtbaren Seite des Phantoms. Der Abstand ist der selbe, der verwendet wird, wenn Thermografiedaten von einer zu behandelnden Person aufgenommen werden sollen. Auf dem Phantom befinden sich im Sichtbereich der Kamera drei Fiducials ($F_1$, $F_2$, $F_3$), deren Objektpunkte im Anatomiekoordinatensystem bekannt sind. Diese Objektpunkte wurden in der Vorarbeit von Weidner \textit{et al.} \cite{Weidner2014} manuell anhand von den MRT-Daten definiert. In die lochförmigen Mitten der Fiducials wurde kaltes Wasser eingefüllt, um eine sichtbaren Temperaturgradienten zu erzeugen. Als Resultat erscheinen die so befüllten Fiducials als dunkle Kreise in den Thermografiebildern. Mit einer semi-automatischen Merkmalsdetektion wurden die Bildpositionen der Fiducialmitten ermittelt. Dazu wurden manuell die initiale Bildpositionen der Fiducials ausgewählt. Mithilfe eines simplen Floodfillalgorithmus wurden Zusammenhangskomponenten für jeden Fiducial bestimmt, die jeweils das runde Fiducialloch beschreiben. Der Floodfillalgorithmus geht von der initialen Bildposition aus und fügt wiederholt Pixel zu der Zusammhangskomponente hinzu. Dabei wird jeweils die umliegenden Nachbarschaft eines noch nicht untersuchten Pixels betrachtet (8-Konnektivität). Beträgt der Unterschied der Intensitäten von dem betrachteten Pixel und von dem initialen Pixel weniger als ein maximaler Wert, wird der betrachtete Pixel zur Zusammenhangkomponente hinzugefügt. Die Mittelpunkte dieser Zusammenhangskomponenten wurden als Grundwahrheit der Fiducialbildpunkte verwendet. Die Genauigkeit der intraoperativen Registrierung wurde in diesem Experiment durch den Rückprojektionsfehler $e$ zwischen den detektierten Fiducialbildpunkten und den Projektionen der bekannten Fiducialobjektpunkte quantifiziert. Für die Projektion der Fiducialobjektpunkte wurden die gleichen intrinsischen Kameraparameter verwendet, die auch für die Evaluation der Kameralagebestimmung genutzt wurden. Weil die Kameralagebestimmung zur Ermittlung der Transformation von der Kamera zum Referenzadapter relativ unzuverlässig ist, wurden mehrere Transformationen bestimmt ($n = 6$). Vor jeder Bestimmung einer solchen Transformation wurde der Registrierungsprozess des Neuronvagationssystems neu durchgeführt. Aus der Menge der Transformationen wurde diejenige verwendet, die den geringsten mittleren Rückprojektionsfehler für dieses Experiment aufwies. Wie bei der intraoperativen Nutzung wurde die getrackte Transformation vom Referenzadapter in das Anatomiekoordinatensystem verwendet, um die aktuellen extrinsischen Kameraparameter zu berechnen. Während des Experiments befanden sich das Phantom und die Kamera in stabilen unveränderlichen Posen.
Der Fehler wurde für mehrere übermittelte Trackinginformationen bestimmt ($n = 20$). Jegliche Varianz in den Rückprojektionsfehlern ist auf das Tracking des Kamerareferenzadapters zurückzuführen, da die verbleibenden Parameter konstant waren. Es wurden mehrere Messreihen durchgeführt, bei denen jeweils der Kameraneigungswinkel $\theta$ an der Kameraquerachse um \SIlist{+15;+30}{\degree} verändert wurde. Die Messung wurde von der senkrechten Positionierung begonnen ($\theta = \SI{0}{\degree}$). Dadurch konnten die hier ermittelten Ergebnisse mit den experimentellen Ergebnissen aus der Vorarbeit von Hoffmann \textit{et al.} \cite{Hoffmann2017} verglichen werden.
\begin{table}[h]
    \centering
    \caption{\label{reprojection_tbl}\textbf{Ergebnisse der finalen Registrierung} in Abhängigkeit von dem Kameraneigungswinkel $\theta$ um die Kameraquerachse. Aufgelistet sind die statistischen Kenngrößen (Mittelwert $\pm$ Standardabweichung) des Rückprojektionsfehler $e$ von unterschiedlichen Fiducials $F_1$, $F_2$ und $F_3$ deren Bild- und Anatomiekoordinaten bekannt sind.}
    \begin{tabular}{r|c|c|c|c|c|c}
    	                                & $e_{F_1}$ (in \si{px}) & $e_{F_2}$ (in \si{px}) & $e_{F_3}$ (in \si{px}) & $e_{F_1}$ (in \si{mm}) & $e_{F_2}$ (in \si{mm}) & $e_{F_3}$ (in \si{mm}) \\ \hline
    	 \fm{\theta\!: \SI{0}{\degree}} &   \num{26,60\pm2,27}   &   \num{9,75\pm2,15}    &   \num{31,62\pm2,12}   &   \num{3,33\pm0,28}    &   \num{1,22\pm0,27}    &   \num{3,95\pm0,27}    \\ \hline
    	\fm{\theta\!: \SI{15}{\degree}} &   \num{27,73\pm2,65}   &   \num{13,98\pm2,62}   &   \num{32,90\pm2,43}   &   \num{3,47\pm0,33}    &   \num{1,75\pm0,33}    &   \num{4,11\pm0,30}    \\ \hline
    	\fm{\theta\!: \SI{30}{\degree}} &   \num{8,00\pm2,32}    &   \num{14,75\pm1,41}   &   \num{28,08\pm1,92}   &   \num{1,00\pm0,29}    &   \num{1,84\pm0,18}    &   \num{3,51\pm0,24}
    \end{tabular}
\end{table}

Die statistischen Kenngrößen der einzelnen Rückprojektionsfehler sind für die unterschiedlichen Kameraneigungswinkel als Resultat der Messreihen in der Tabelle \ref{reprojection_tbl} aufgeführt. Der Fehler von bis zu \SI{33}{px} erscheinen auf den ersten Anblick recht groß. Weil ein Kameraabstand von \SI{40}{cm} verwendet wurde, haben die Thermografiebilder allerdings nur eine Auflösung von ca. \SI{0,125}{mm/px}. Deswegen ist der gezeigte Bildausschnitt sehr klein und die Pixeldistanzen entsprechen nur einigen \si{mm}.
Die Kenngrößen der Rückprojektionsfehler wurden anhand der Auflösung von \si{px} in \si{mm} konvertiert und ebenfalls in der Tabelle \ref{reprojection_tbl} angegeben. Dass das Verhalten der Fehler in Bezug auf den Kameraneigungswinkel für die einzelnen Fiducialpositionen z.T. nicht linear abhängig ist, liegt wahrscheinlich an einem systematischer Fehler bei den Kameraparametern. Das ist durchaus plausibel, weil z.B. bei der Kameralagebestimmung die Varianzen der extrinsischen Kameraparameter recht groß sind. Außerdem ist eine maximale Standardabweichung von \SI{0,33}{mm} zu beobachten, die auf die Ungenauigkeit des getrackten Referenzadapters zurückzuführen ist.
\begin{table}[h]
    \centering
    \caption{\label{overall_reprojection_tbl}\textbf{Ergebnisse der finalen Registrierung} in Abhängigkeit von dem Kameraneigungswinkel $\theta$. Aufgelistet sind die Mittelwerte des mittleren Rückprojektionsfehlers $\overline{e}$ über alle Fiducials und der gesamte Mittelwert des mittleren Rückprojektionsfehler über alle Fiducials und über alle Kameraneigungswinkel.}
    \begin{tabular}{r|c|c|c|c}
    	\fm{\theta} (in \si{\degree}) &  \num{0}   &  \num{15}  &  \num{30}  &  \textbf{Mittelwert}   \\ \hline
    	  $\overline{e}$ (in \si{mm}) & \num{2,83} & \num{3,11} & \num{2,11} & \underline{\num{2,69}}
    \end{tabular}
\end{table}
Die gesamten mittleren Rückprojektionsfehler von allen Fiducialpunkten $\overline{e}$ ist für unterschiedlichen Kameraneigungswinkel und als Mittelwert über alle Kameraneigungswinkel in Tabelle \ref{overall_reprojection_tbl} aufgeführt. Der gesamte mittlere Fehler ist dabei nicht linear von dem Neigungswinkel der Kamera abhängig.

Der gesamte mittlere Registrierungsfehler über alle Kameraneigungswinkel von \SI{2,69}{mm} ist vergleichbar mit dem gesamten mittleren Fehler des Registrierungsansatzes von Hoffmann \textit{et al.} \cite{Hoffmann2017}, der in der Arbeit \SI{2,46}{mm} betrug. Der Fehler des hier vorgestellten Registrierungsansatzes ist aufgrund der verwendeten perspektivischen Abbildung, bei einer perfekten Registrierung, nicht mehr von dem Winkel der Kamera zum Bildgegenstand abhängig. Es können aber weiterhin Fehler in Abhängigkeit von der Kamerapose auftreten, wenn z.B. die extrinsischen Kameraparameter falsch bestimmt wurden sind. Mit dem vorgestellten Ansatz können automatisch Kameraparameter bestimmt werden, deren Registrierungsgenauigkeit der einer manuellen Registrierung ähnelt. Die Genauigkeit der Registrierung variiert außerdem intraoperativ in Abhängigkeit von der Genauigkeit des getrackten Referenzadapters. Diese ungenaue Transformation von dem Referenzadapter zur Anatomie wird auch verwendet, um präoperativ die Transformation von der Kamera zu dem Referenzadapter zu bestimmen. Daher könnte der systematische Registrierungsfehler weiter verringert werden, wenn die übermittelte Referenzadaptertransformation bei der präoperativen Transformationsbestimmung über einen kurzen Zeitraum gemittelt wird.

    \section{Fehler der Oberflächenextraktion}
Neben dem Registrierungsfehler bestimmt die Genauigkeit der Oberflächenextraktion wie gut die Beobachtungsräume der MRT- und Thermografiedaten übereinstimmen. Intraoperativ führt der sogenannte Brain Shift zu einer Deformierung des Gewebes, welche im präoperativen MRT-Datensatz nicht enthalten ist und von dem Neuronavigationssystem nicht korrigiert wird. Zu diesem Zweck wird ein Fehler bestimmt, der den Unterschied zwischen der extrahierten und der real bemessenen Oberfläche näherungsweise quantifiziert. In manchen Fällen, bei denen eine Trepanation durchgeführt werden muss, werden während der Operation mit dem Zeigerinstrument die Gehirnoberflächepunkte am Trepanationsrand berührt. Für mehrere dieser Fälle liegen die Daten, die bei einem Bild-gestützten Eingriff zur Verwendung kamen, in anonymisierter Form vor. Diese Daten beinhalten die MRT-Daten der behandelten Person, eine Gehirnsegmentierung des Neuronvagiationssystems und die angetippten Gehirnoberflächenpunkte im Anatomiekoordinatensystem. Da die Oberflächenpunkte mithilfe des Zeigerinstruments ermittelt wurden, werden sie potentiell durch den Registrierungsfehler des Neuronavigationssystems verfälscht. Aus den MRT-Daten wurde mit dem 3D Slicer Modul \textit{ROBEXBrainExtraction} eine weitere Gehirnsegmentierung erzeugt. Mit dem 3D Slicer Modul \textit{Grayscale Model Maker} wurden zwei Gehirnoberflächen aus den beiden Gehirnsegmentierungen erzeugt. Der Schwellwert wurde auf 1 gesetzt, da beide Segmentierungsalgorithmen die Gitterpunkte auf 0 setzen, die nicht als Gehirn klassifiziert wurden. Das Fehlermaß wurde dann für jede Gehirnsegmentierung ($d_{\text{Neuro}}$, $d_{\text{ROBEX}}$) als der minimale Abstand von den ``wahren'' Gehirnoberflächenpunkten zu der extrahierten Oberfläche berechnet. Für jeden Fall lagen \numrange{5}{9} Gehirnoberflächenpunkte vor.
\begin{table}[]
    \centering
    \caption[]{\label{reconstruction_error_tbl}\textbf{Ergebnisse der Oberflächenextraktion} für verschiedene Falldaten. Aufgelistet sind die statistischen Kenngrößen (Mittelwert $\pm$ Standardabweichung) des minimalen Abstands $d$ von manuell definierten Oberflächenpunkten zu der extrahierten Oberfläche aus der Gehirnsegmentierung der Neuronavigation und aus der ROBEX-basierten Gehirnsegmentierung \cite{Iglesias2011}.}
    \begin{tabular}{r|c|c}
    	     \textbf{Pathologie} & $d_{\text{Neuro}}$ (in \si{mm}) & $d_{\text{ROBEX}}$ (in \si{mm}) \\ \hline
    	  \textit{Epilepsie \#1} &        \num{4,51\pm2,36}        &        \num{3,04\pm1,99}        \\ \hline
    	      \textit{Gliom \#1} &        \num{4,08\pm0,94}        &        \num{3,99\pm2,61}        \\ \hline
    	      \textit{Gliom \#2} &        \num{6,71\pm2,84}        &        \num{4,20\pm2,32}        \\ \hline
    	 \textit{Astrozytom \#1} &        \num{8,56\pm2,54}        &        \num{4,84\pm2,67}        \\ \hline
    	\textit{Glioblastom \#1} &        \num{1,48\pm0,98}        &        \num{1,80\pm1,79}        \\ \hline
    	\textit{Glioblastom \#2} &        \num{8,93\pm3,14}        &        \num{6,24\pm4,25}        \\ \hline
    	\textit{Glioblastom \#3} &        \num{2,31\pm1,27}        &        \num{1,51\pm1,02}        \\ \hline
    	\textit{Glioblastom \#4} &       \num{11,04\pm2,61}        &        \num{6,71\pm2,45}        \\ \hline
    	\textit{Glioblastom \#5} &        \num{6,87\pm1,84}        &        \num{3,77\pm1,55}        \\ \hline
    	  (\textit{Melanom \#1}) &       (\num{9,22\pm1,87})       &      (\num{11,65\pm3,37})       \\ \hline\hline
    	     \textbf{Mittelwert} &     \underline{\num{6,05}}      &     \underline{\num{4,00}}
    \end{tabular}
\end{table}

Die Ergebnisse sind für die einzelnen Fälle in Tabelle \ref{reconstruction_error_tbl} aufgeführt. Für jeden Fall ist zusätzlich die vorliegende Pathologie angegeben. Die ROBEX-basierten Oberfläche liegen dabei in den meisten Fällen am nächsten an den tatsächlichen Gehirnoberflächenpunkten. Das liegt zum Großteil an der Funktionsweise der vorgenommen Segmentierung. Der Gehirnsegmentierungsalgorithmus des Neuronavigationssystems zählt die Bereiche zwischen den Gehirnwindungen und das Tumorgewebe nicht zum Gehirn. Liegt ein Gehirnoberflächenpunkt z.B. auf dem Bereich zwischen den Gehirnwindungen, liegt der nächstliegende Punkt erst auf der Oberfläche der entfernteren Gehirnwindungen. Der ROBEX-Gehirnsegementierungsalgorithmus zählt diese Bereiche und das Tumorgewebe i.d.R. zum Gehirn mit. Bei dem Fall in dem mehrere Melanome vorlagen, lagen die Gehirnoberflächenpunkten direkt auf dem Tumorgewebe. Für die Segmentierung des Neuronavigationssystems, die dieses Gewebe nicht beinhaltete, lagen der nächste Punkte daher deutlich weiter weg, als in den anderen Fällen. Die ROBEX-Gehirnsegmentierung war für diesen Fall quantifizierbar falsch, das segmentierte Gehirn war sichtbar in die kaudale Richtung (>\SI{1}{cm}) verschoben. Dieser Fall wurde daher nicht in die Berechnung des mittleren Abstands für alle Fälle einbezogen. Im Mittel war der Fehler bei allen Fällen bei der ROBEX-Segmentierung ca. 50\% besser, als der Fehler der Segmentierung des Neuronavigationssystems.

Die Ergebnisse legen nahe, dass der ROBEX-Gehirnsegmentierungsalgorithmus besser für die Erzeugung der Gehirnoberfläche geeignet ist. Der Fall in dem mehrere Melanome vorlagen, zeigt allerdings, dass dieser Algorithmus nicht unfehlbar ist. Daher sollten die Segmentierungsergebnisse immer manuell eingeschätzt werden, bevor sie für die Oberflächenextraktion verwendet werden. Mit der vorliegenden Anzahl von Fällen lässt sich keine verlässliche Aussage über den Einfluss der vorliegenden Phatologie treffen. Ein Einfluss ist aber durchaus vorstellbar, da Tumore möglicherweise die Verformung des Gehirns bei der Trepanation beeinflussen. Der gesamte mittlere Oberflächenextraktionsfehler von \SI{4}{mm} ist im Vergleich mit dem gesamten mittleren Registrierungsfehler recht groß. Eine Verbesserung der Oberflächenextraktion ist daher genauso wichtig, wie die Verbesserung der Registrierungsgenauigkeit. Das verwendete Fehlermaß der Oberflächenextraktion bildet allerdings nur eine Approximation des realen Fehlers. Das Problem ist, dass keine wirkliche Grundwahrheit über die real bemessene Oberfläche vorliegt. In der Zukunft könnte Stereoskopie zur Ermittelung einer verlässlichere Oberflächenbeschreibung eingesetzt werden, um z.B. ein aussagekräftigeres Fehlermaß zu definieren.

    \section{Finale Visualisierung}
Bei der finalen Visualisierung soll zunächst die Bilderzeugungseffizienz der vorgestellten Visualisierungsmethoden ermittelt werden. Dazu wurden die Renderingzeiten der verschiedenen Teilvisualisierungen und der finalen kombinierten Visualisierung für die implementierten Bildfusionsalgorithmen gemessen. Dafür wurde ein \SI{4,0}{GHz} \textit{Core i7-4790K} Vierkern-Prozessor von Intel und eine \textit{GeForce GTX 980 Ti} Grafikkarte von Nvidia verwendet. Die 3D-Ansicht von 3D Slicer wurde in zwei unterschiedlichen Größen verwendet. Zum einen wurde die Standardgröße der 3D-Ansicht verwendet, die Auflösung betrug dabei $\num{807}\times\num{327}$ Pixel. Außerdem wurde eine größere 3D-Ansicht ($\sim$ maximale Größe auf einem FullHD-Monitor) mit einer Auflösung von $\num{1632}\times\num{895}$ Pixeln verwendet. Für die Volumengrafikdarstellung wurde 1 GB Grafikspeicher zur Verfügung gestellt und die adaptive Renderingqualität wurde abgestellt, damit die Bilderstellungsdauern nicht durch eine maximale Bildwiederholrate limitiert werden. 3D Slicer wurde in einer selbst gebaute Nightly-Version vom 15.12.2017 verwendet, die in ihrer Funktionalität der 3D Slicer Version 4.8.1 entspricht.
\begin{table}[h]
    \centering
    \caption{\label{frame_time_tbl}\textbf{Mittlere Bilderstellungszeiten} für eine MRT-Volumengrafik (MRT), ein projiziertes Thermografiebild (TRM), ein fusioniertes MRT- und Thermografiebild mit dem gewichteten Mittelwert (AVG) und mit der geführten Weichzeichnung (GFF) \cite{Li2013}}
    \begin{tabular}{r|c|c|c|c}
    	         \textbf{Auflösung} & $\overline{t}_{\text{MRT}}$ (in \si{ms}) & $\overline{t}_{\text{TRM}}$ (in \si{ms}) & $\overline{t}_{\text{AVG}}$ (in \si{ms}) & $\overline{t}_{\text{GFF}}$ (in \si{ms}) \\ \hline
    	 $\num{807}\times\num{327}$ &                \num{6,31}                &                \num{0,84}                &               \num{36,14}                &               \num{53,72}                \\ \hline
    	$\num{1632}\times\num{895}$ &               \num{34,03}                &                \num{0,90}                &               \num{172,00}               &               \num{307,04}
    \end{tabular}
\end{table}

Die mittleren Bilderstellungszeiten sind für die MRT-Volumengrafikdarstellung ($\overline{t}_{\text{MRT}}$), für die projizierte Thermografievisualisierung ($\overline{t}_{\text{TRM}}$), für die fusionierte MRT- und Thermografievisualisierung, die mit dem gewichteten Mittelwert erzeugt wurde ($\overline{t}_{\text{AVG}}$) und für die fusionierte MRT- und Thermografievisualisierung, die mit der gewichteten Weichzeichnung erzeugt wurde ($\overline{t}_{\text{GFF}}$) in der Tabelle \ref{frame_time_tbl} aufgeführt. Die mittlere Bilderstellungsdauer der projizierten Thermografievisualisierung ist für beide Auflösungen sehr kurz, demnach hat diese Visualisierung einen vernachlässigbaren Einfluss auf die finale Visualisierung. Die mittlere Bilderstellungsdauer der MRT-Volumengrafikvisualisierung ist um eine Größenordnung kleiner als die fusionierte MRT- und Thermografievisualisierung, die mit der geführten Weichzeichnung erstellt wurde. Für die kleinere Standardauflösung laufen beide fusionierte Visualisierungen schnell genug, um die Bildübertragungsrate der Thermografiebilder darzustellen. Für die große Auflösung ist das nicht mehr der Fall. Dies liegt vermutlich daran, dass beide Bildfusionsalgorithmen komplett CPU-basiert implementiert wurden. Eine effizientere GPU-basierte Implementierung der Bildfusion anhand von geführter Weichzeichnung wäre möglich, da dieser Algorithmus eine lineare Zeitkomplexität $O(n)$ besitzt \cite{Li2013}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/fused_big}
    \caption{\label{fused_image_big_fgr}Das vorgestellte Bildfusionsschema erhält gleichzeitig die Helligkeitsvariationen beider Eingabebilder und den Farbton und die Sättigung des Thermografiebilds (hellblau hervorgehoben). Allerdings tritt mit den verwendeten Bildfusionsparametern ein Halo-Effekt am Rande des Bildgegenstands auf (gelb hervorgehoben).}
\end{figure}

In dem vorgestellten Bildfusionsschema sind die farbkodierten Temperaturwerte unabhängig von den MRT-Daten korrekt ablesbar, wenn eine Farbkodierung verwendet wird, welche jeden Temperaturwert eindeutig auf einen Farbton abbildet. Außerdem muss diese Farbkodierung den Temperaturwert monoton steigend auf die Helligkeit abbilden, damit die räumlichen Merkmale der Thermografievisualisierung ebenfalls in den finalen Visualisierung erhalten sind. Die projizierte Thermografievisualisierung bildet die Temperaturen aufgrund der Nearest neighbour Interpolation getreu der Farbkodierung ab. Bei den standardmäßig verwendeten Auflösungen sind dabei auch keine Blockartefakte erkennbar. Da die Farbton- und Sättigungsinformation komplett aus der Thermografievisualisierung in die fusionierte Visualisierung übernommen wird, sind die dargestellten Farben wieder eindeutig auf die originalen Temperaturwerten zuzuordnen. Eine erweiterte Farbkodierungslegende könnte neben der eigentlichen Visualisierung dargestellt, um die Ablesbarkeit der Temperaturwerte zu erleichtern. Diese Legende wäre nicht wie normalerweise üblich eindimensional, sondern sie wäre zweidimensional und würde für jeden Farbwert alle möglichen Helligkeitsvariationen und den kodierten Temperaturwert anzeigen. Die zweite Informationen die korrekt dargestellt werden sollen, bilden die räumlichen Hinweisreize. Diese sind zunächst nicht formal definiert, sie spiegeln sich aber überwiegend in den Helligkeitsvariationen wieder. Die Helligkeitsinformationen der farbkodierten Temperaturen und der MRT-Kontrastwerte werden mit dem Bildfusionsalgorithmus von Li \textit{et al.} fusioniert. Dieser berechnet für jedes Pixel ein Salienzmaß, dass genau diese Helligkeitsvariationen quantifiziert. In dem fusionierten Bild erscheinen dabei die Pixelintensitäten am sichtbarsten, deren Salienz am größten ist. Demnach wird an jeder Bildstelle die Helligkeitsvariation dargestellt, die aus beiden Bildern am größten ist. Die Konsistenzherstellung bei den Gewichtungseffizienten mit der geführten Weichzeichnung trägt dazu bei, dass Pixel mit ähnlichen Salienzen und unterschiedlichen Helligkeiten nicht gleich gewichtet gemittelt werden. Somit sind in allen Bildbereichen des fusionierten Bildes eine maximal große Menge an Helligkeitsvariationen sichtbar. Aufgrund der geteilten Repräsentation der Bilder als Detail- und Grundbild, entsteht in den fusionierten Bildern an den scharfen Kanten der Bildgegenstände jeweils ein falscher Halo-Effekt. Dieser ist in der Abbildung \ref{fused_image_big_fgr} hervorgehoben. Das liegt vermutlich an der geführten Bildfilterung. Sie sorgt zwar dafür, dass die Koeffizienten einer Ebene konsistent sind, aber nicht dafür, dass die Koeffizienten über verschiedene Ebenen konsistent sind. Da dieser Effekt aber bekannt ist und da er nur an den scharfen Kanten der Bildgegenstände auftaucht, lassen sich dadurch keine falsche räumlichen Informationen ableiten. Demnach werden die salientesten räumlichen Hinweisreize aus beiden Bildern weitestgehend unverfälscht dargestellt. Alle relevanten Informationen aus den Merkmalsräumen werden, sofern die Beobachtungsräume übereinstimmen, daher so dargestellt, dass keine falsche Aussagen über die dargestellten Daten suggeriert werden.


Die fusionierte MRT- und Thermografievisualisierung stellt die relevante Informationen dar, um etwaige thermische Anomalien räumlich abzugrenzen. Ein Vorteil bildet dabei die integrierte Darstellung von den gemessen Temperaturen und den räumlichen Hinweisreizen in einem Bild. So muss der Fokus nicht zwischen mehreren Bildern gewechselt werden. Ein weiterer Vorteil dieser Darstellung ist es, dass räumliche Strukturen wie z.B. die Gehirnwindungen, auch an den Stellen erkennbar sind, an denen Thermografiedaten vorliegen. Das ist aber potentiell auch ein Nachteil, wenn auf solchen räumlichen Merkmalen Temperaturwerte dargestellt werden. Konkrete Temperaturwerte lassen sich aufgrund der veränderten Helligkeitsinformationen vermutlich schwerer ablesen als von einer einfachen Pseudofarbdarstellung. Die Visualisierungsmethode erfordert keine weitere menschliche Interaktion, um beliebige Bereiche in den Thermografiedaten auf zugehörige Gehirnareale zuzuordnen. Insgesamt erscheinen die erzeugten fusionierte Bilder deutlich geeigneter für den intraoperative Einsatz, als eigenständige Bilder, welche die MRT- und Thermografiedaten separat darstellen. Weitere Vor- und Nachteile zeigen sich voraussichtlich erst, wenn das vorgestellte Visualisierungsmethode in einem realen Eingriff genutzt wird oder wenn sie von medizinischem Fachpersonal eingeschätzt wird.


% --------------------------------------------------------------------------- %
\chapter{Fazit}

Das Ziel dieser Arbeit war es, eine kombinierte Visualisierung von präoperativen MRT-Daten und intraoperativen Thermografiedaten zu erstellen. Dazu wurde eine Software entwickelt, die intrinsische und extrinsische Kameraparameter unter der Verwendung eines Neuronavigationssystems bestimmt. Diese Software berechnet außerdem eine Transformation von der Kamera zu einem statisch angebrachten Referenzadapter. Diese Informationen werden mit dem Neuronavigationssystem und dem gleich angebrachten Referenzadapter verwendet, um intraoperativ die Thermografie- und MRT-Daten aufeinander zu registrieren. Weil herkömmliche Kalibrationsmuster für die Thermografie ungeeignet sind, wurde als Ersatz ein einfaches planares Lochmuster angefertigt, das für die präoperativen Parameterbestimmung verwendet werden kann. Bei der selbst konzipierten Prozedur zur präoperativen Bestimmung der Kameraparameter und der Transformation von der Kamera zu dem Referenzadpater werden alle für die Registrierung benötigten Informationen automatisch berechnet. Die intraoperative Visualisierung wurde als eine Erweiterung für die Software 3D Slicer implementiert. Die Thermografiedaten werden zunächst mit einem projektiven Texture Mapping auf ihrer dargestellte Oberfläche projiziert und aus beliebigen Ansichten visualisiert. Die dafür notwendige Oberfläche wurde mit einem vorhanden Gehirnsegmentierungsalgorithmus und mithilfe von Marching Cubes extrahiert. Die MRT-Daten lassen sich bereits mit 3D Slicer als Volumengrafikdarstellung visualisieren. Die Bilder beider Visualisierungen werden in Echtzeit, mit einem selbst konzipierten Bildfusionsschema, das die Ablesbarkeit der farbkodierten Temperaturen gewährleistet, in ein finale fusionierte Visualisierung kombiniert. Dabei kommt ein vorhandener Bildfusionsalgorithmus zur Fusion der Helligkeitsinformation zum Einsatz. In der Arbeit wurden experimentell mögliche Einflussfaktoren auf die Genauigkeit und die Effizienz des gesamten Visualisierungssystems untersucht.

Die Bestimmung der intrinsischen Kameraparameter läuft demnach optimal ab, wenn die Posen des Kalibrationsmusters in Winkeln und Distanzen zur Kamera variiert werden. Bei der Bestimmung der extrinsischen Kameraparameter wurden die zuverlässigsten Ergebnisse erzielt, wenn das verwendete Zeigerinstrument bei der Bestimmung der Musterreferenzpunkte pivotiert wird. Der finale intraoperative Registrierungsgenauigkeit wurde anhand des Rückprojektionsfehlers von bekannten Objektkoordinaten eines Phantoms quantifiziert. Der Fehler der Registrierung betrug dabei im Mittel \SI{2,69}{mm}. Außerdem wurde der Unterschied der extrahierten und der tatsächlich bemessenen Oberfläche näherungsweise ermittelt. Er betrug bei verschiedenen Fällen durchschnittlich \SI{4,00}{mm}. Zum Schluss wurde die finale Visualisierung hinsichtlich ihrer Aussagekräftigkeit, Effizienz und Effektivität bewertet. Neben kleineren Problemen, erfüllt die Visualisierung alle notwendigen Kriterien für eine intraoperative Nutzung.

Das vorgestellte Visualisierungsframework kommt aufgrund seiner Komplexität mit einigen Vor- und Nachteilen einher. So lassen sich z.B. alle Parameter der verwendeten Kameraabbildung präoperativ automatisch bestimmen und sie lassen sich intraoperativ nur in Abhängigkeit von den Trackinginformationen aktualisiert berechnen. Allerdings ist der dafür nötige Aufbau für die präoperative Parameterbestimmung recht komplex und er kann somit potentiell viele Fehlerquellen beinhalten. Das speziell für die Thermografie gefertigte Kalibrationsmuster ermöglicht dabei allerdings die Nutzung von etablierten Algorithmen für die Kamerakalibrierung und Kameralagebestimmung. Die verwendete Kameraabbildung wird häufig in der Computergrafik verwendet, weswegen sich darauf basierende Visualisierungen effizient rendern lassen. Die intraoperative Visualisierung benötigt außerdem, sofern es nicht bereits vorhanden ist, ein Neuronavigationssystem im Operationssaal. Auch wenn das verwendetes Kameramodell effizient auswertbar ist, stellt es eine starke Vereinfachung der Realität dar, sodass z.B. Unschärfeeffekte bei der Bestimmung der Temperatur an den Oberflächenpunkten vernachlässigt werden. Die Abwesenheit von Unschärfeeffekten gewährleistet allerdings die einfache Ablesbarkeit der Temperaturwerte.     Der verwendete Ansatz zur Oberflächenextraktion kann präoperativ erfolgen und erfordert damit intraoperativ keine weitere Rechenzeit. Allerdings ist die so extrahierte Oberfläche ist nur eine Approximation der tatsächlich sichtbaren Oberfläche und sie ist unabhängig von den Bilddaten der intraoperativen Szene. Außerdem basiert die Oberflächenextraktion auf einem Segmentierungsverfahren, dass nur die für MRT-Daten des Gehirns geeignet ist. Die verwendete Thermografievisualisierung ermöglicht es mit echtzeitfähigen Bildwiederholraten die Temperaturwerte auf allen bemessenen Oberflächenpunkten darzustellen. Dafür muss die Oberfläche allerdings als ein Dreiecksgitter vorliegen. Der verwendete Marching Cubes Algorithmus approximiert die Isoflächen dabei als ebene Dreieckselemente, das Interpolationsschema der Gitterdaten ist allerdings trilinear. Deswegen wird bei der Verwendung von diesem Algorithmus mitunter die Gitterstruktur sichtbar.     Die fusionierte Darstellung ermöglicht eine gleichzeitige, interaktionsfreie und integrierte Auswertung von Thermografie- und MRT-Daten. Allerdings bietet die finale Darstellung bis auf die Ansichtsänderung wenig Interaktionsmöglichkeiten, sodass ein exploratives Untersuchen der Daten weniger unterstützt wird. Dies ist bei dem intraoperativen Einsatz der Visualisierung allerdings nicht unbedingt notwendig. In der Zukunft kann das vorgestellte perspektivische Registrierungsmodell verwendet werden, um eine initiale Registrierung der MRT- und Thermografiedaten zu generieren, die für einen verformungsbasierte Registrierungsprozess verwendet werden kann. Der größte Vorteil der intraoperative Registrierungsmethode ist die Datenunabhängikeit. Dadurch basiert die Registrierungsgenauigkeit nur auf externen Parametern die sich mehr oder weniger kontrollieren lassen. Daher lässt sich dieser Ansatz theoretisch für beliebige weitere Modalitäten, wie z.B. für optische oder hyperspektrale Bildgebungsverfahren, verwenden.

Verschiedene Aspekte der untersuchten Themen, die während der Arbeit kein direkter Forschungsgegenstand waren, liefern einen Ausblick für zukünftige Forschungsthemen. So ist z.B. die Tiefenunschärfe des verwendeten Objektivs der Thermografiekamera sehr groß. Es gibt verschiedene Bildfusionsalgorithmen die mehrere Bilder mit unterschiedlichen Fokuspunkten in ein komplett scharfes Bild fusionieren. Ist die zeitlichen Auflösung der Thermografiedaten weniger relevant, könnten mit diesen Bildfusionsalgorithmen künstlich geschärfte Bilder erzeugt werden. Das in der Arbeit entwickelte Kalibrationsmuster bietet nur eine geringe Fertigungsgenauigkeit. Daher könnte industrielle Fertigungsmethoden eingesetzt werden, um ein optimales Kalibrationsmuster für die Thermografie zu entwickeln. So ließe sich z.B. das Material des Musters, die Musterdicke, die Bohrungsgenauigkeit, die Anzahl der Merkmalslochpunkte oder die verwendete Punktanordnung verbessern. Dadurch würde der Registrierungsfehler weiter verringert werden. Die Oberflächenbestimmung könnte auch mit komplett anderen Verfahren erfolgen. So ist die Verwendung von stereoskopischen Bildern zur Bestimmung einer exakteren Oberfläche vorstellbar. Dadurch ließe sich der Fehler für die Visualisierung verringern. Das vorgestellte Bildfusionsschema könnte für die Fusion beliebiger Modalitäten erweitert werden. So könnten, genau wie für die Thermografiedaten, rückprojizierte optische Bilder generiert werden. Diese Bilder könnten eine fusionierte Visualisierung um zusätzliche Texturinformationen anreichern.

\appendix
\makeatletter

\chapter{Selbstständigkeitserklärung}

Hiermit erkläre ich, dass ich die von mir am heutigen Tag dem Prüfungsausschuss der Fakultät \@faculty~eingereichte Arbeit zum Thema:
\begin{center}
    \textit{\@title} 
\end{center}

vollkommen selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt sowie Zitate kenntlich gemacht habe.

Dresden, den \@date\\[3cm]
\@author
\makeatother
\end{document}